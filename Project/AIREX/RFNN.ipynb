{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxpY--vDV-rr"
   },
   "source": [
    "#### Implementing FNN from Scratch for a Regression task on sklearn dataset.\n",
    "        No of Hidden layers = 4\n",
    "        No of Hidden Units = 20\n",
    "        No of epochs = 1000\n",
    "        Mini-batch size = 16\n",
    "        Learning Algorithm = Gradient descent with backpropagation\n",
    "        Loss function = sum of squares fxn (Output Activation = Linear)\n",
    "        Loss function = cross-entropy (Output Actiavation = softmax)\n",
    "        Using Sigmoid(logistic function) for activation in between layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m89K1lJzWKLs"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1737153185836,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "9kysm2uzWODH"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1qoiU7DWZDZ"
   },
   "source": [
    "Defining Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1737153186899,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "ZzTnlbvFWdxT"
   },
   "outputs": [],
   "source": [
    "# there are other activation fxns tooo ReLu , tanh and so on we prefer sigmoid because output is real and is between 0 and 1 so can intrepet as probability\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "# if we are using softmax function to activate the outermost layer then we need to use cross entropy as the loss function else in the case of using linear then use MSE as the loss function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "# not needed for regression\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -np.sum(targets * np.log(predictions + 1e-9)) / targets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ew9Msb5XUT-"
   },
   "source": [
    "#### Nueral Network with 4 hidden layers\n",
    "Algorithm Using : Gradient Descent with backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1737153188166,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "jB4xyQIDXSl7"
   },
   "outputs": [],
   "source": [
    "class FeedforwardNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size): #assumming all hidden layers are having same no of nuerons\n",
    "    # step 1 : initializing wts and biases\n",
    "        self.weights_0 = np.random.randn(input_size, hidden_size) # weights from input layer to first hidden layer\n",
    "        self.weights_1 = np.random.randn(hidden_size, hidden_size) # weights from first hidden layer to second hidden layer\n",
    "        self.weights_2 = np.random.randn(hidden_size, hidden_size) # and so on \n",
    "        self.weights_3 = np.random.randn(hidden_size, hidden_size)\n",
    "        self.weights_4 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "        self.bias_0 = np.zeros((1, hidden_size))\n",
    "        self.bias_1 = np.zeros((1, hidden_size))\n",
    "        self.bias_2 = np.zeros((1, hidden_size))\n",
    "        self.bias_3 = np.zeros((1, hidden_size))\n",
    "        self.bias_4 = np.zeros((1, output_size))\n",
    "    # step 2: forward prop -> computes activation for hidden and output layers\n",
    "    def forward(self, X):\n",
    "        self.z0 = sigmoid(np.dot(X, self.weights_0) + self.bias_0)\n",
    "        self.z1 = sigmoid(np.dot(self.z0, self.weights_1) + self.bias_1)\n",
    "        self.z2 = sigmoid(np.dot(self.z1, self.weights_2) + self.bias_2)\n",
    "        self.z3 = sigmoid(np.dot(self.z2, self.weights_3) + self.bias_3)\n",
    "        self.output = (np.dot(self.z3, self.weights_4) + self.bias_4) # linear activation function\n",
    "        return self.output\n",
    "    # step 3 : backward prop -> adjusting the weights based on the error between predicted and auctual outputs\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # Output layer error\n",
    "        output_error = self.output - y\n",
    "\n",
    "        # Hidden layer 3 error\n",
    "        hidden_3_error = np.dot(output_error, self.weights_4.T) * sigmoid_derivative(self.z3)\n",
    "\n",
    "        # Hidden layer 2 error\n",
    "        hidden_2_error = np.dot(hidden_3_error, self.weights_3.T) * sigmoid_derivative(self.z2)\n",
    "\n",
    "        # Hidden layer 1 error\n",
    "        hidden_1_error = np.dot(hidden_2_error, self.weights_2.T) * sigmoid_derivative(self.z1)\n",
    "\n",
    "        # Hidden layer 0 error\n",
    "        hidden_0_error = np.dot(hidden_1_error, self.weights_1.T) * sigmoid_derivative(self.z0)\n",
    "\n",
    "        # Update weights and biases (why we are using - bcz of GD algorithm proof can be done by using cos(angle) is ---)\n",
    "        self.weights_4 -= learning_rate * np.dot(self.z3.T, output_error) / m\n",
    "        self.bias_4 -= learning_rate * np.sum(output_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.weights_3 -= learning_rate * np.dot(self.z2.T, hidden_3_error) / m\n",
    "        self.bias_3 -= learning_rate * np.sum(hidden_3_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.weights_2 -= learning_rate * np.dot(self.z1.T, hidden_2_error) / m\n",
    "        self.bias_2 -= learning_rate * np.sum(hidden_2_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.weights_1 -= learning_rate * np.dot(self.z0.T, hidden_1_error) / m\n",
    "        self.bias_1 -= learning_rate * np.sum(hidden_1_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.weights_0 -= learning_rate * np.dot(X.T, hidden_0_error) / m\n",
    "        self.bias_0 -= learning_rate * np.sum(hidden_0_error, axis=0, keepdims=True) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZxGiTm4YfpB"
   },
   "source": [
    "Load and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1737153190197,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "k1zYj8HFYery"
   },
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNfqHlLJY2jv"
   },
   "source": [
    "**Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1737153669457,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "SgFWZuR-Y_XM"
   },
   "outputs": [],
   "source": [
    "\"\"\"Note: Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1],\n",
    "or standardize it to have mean 0 and variance 1. Note that you must apply the same scaling to the test set for meaningful results. You can use StandardScaler for standardization.\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovIcVSqzZMth"
   },
   "source": [
    "Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "executionInfo": {
     "elapsed": 181883,
     "status": "ok",
     "timestamp": 1737154142532,
     "user": {
      "displayName": "Sai Sandesh Kanthala",
      "userId": "12437310925255289088"
     },
     "user_tz": -330
    },
    "id": "Pr8FNJRlZIXu",
    "outputId": "155d1a0e-e200-4795-ae96-1912aff1a3ba"
   },
   "outputs": [],
   "source": [
    "# # Convert target to categorical (binned regression)\n",
    "# y_binned = pd.qcut(y, q=10, labels=False)  # Divide into 10 bins\n",
    "# y_one_hot = np.eye(10)[y_binned]  # One-hot encoding\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and Train Neural Network\n",
    "nn = FeedforwardNN(input_size=X_train.shape[1], hidden_size=20, output_size=1) # setting the no of hidden layers to be equal to 20\n",
    "num_epochs = 400\n",
    "learning_rate = 0.01 #Determines how quickly the model updates weights\n",
    "\n",
    "losses = []\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    predictions = nn.forward(X_train)\n",
    "    loss = mean_squared_error(predictions, y_train)\n",
    "    losses.append(loss)\n",
    "    nn.backward(X_train, y_train, learning_rate)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "end = time.time()\n",
    "print(\"Time per 100 epochs: \", end - start)\n",
    "\n",
    "# if using mini batch gradient descent \n",
    "# num_epochs = 1000\n",
    "# learning_rate = 0.01\n",
    "# batch_size = 32  # Mini-batch size\n",
    "\n",
    "# losses = []\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Shuffle the training data\n",
    "#     permutation = np.random.permutation(X_train.shape[0])\n",
    "#     X_train_shuffled = X_train[permutation]\n",
    "#     y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "#     for i in range(0, X_train.shape[0], batch_size):\n",
    "#         X_batch = X_train_shuffled[i:i + batch_size]\n",
    "#         y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        \n",
    "#         predictions = nn.forward(X_batch)\n",
    "#         loss = mean_squared_error(predictions, y_batch)\n",
    "#         losses.append(loss)\n",
    "#         nn.backward(X_batch, y_batch, learning_rate)\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "# Plot Loss Curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "# Reminder : 80 percent train data , 20 test \n",
    "# # Evaluate on Train Data \n",
    "# train_predictions = nn.forward(X_train)\n",
    "# train_loss = mean_squared_error(train_predictions,y_train)\n",
    "# print(f\"Train loss: {train_loss}\")\n",
    "# Evaluate on Test Data\n",
    "test_predictions = nn.forward(X_test)\n",
    "test_loss = mean_squared_error(test_predictions, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMWEeBzkRXd5gtiSF7z/HCh",
   "mount_file_id": "10u6LzdNC-e6Oup7jX2C0up-mDsI8zRBA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
