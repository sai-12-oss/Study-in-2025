{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Linear Regression - Ordinary Least Squares(OLS) and Ridge Regression(RR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-1 : Query the oracle to obtain D_train1 , D_train2 , D_test1 , and D_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Oracle_Assignment_2\n",
    "import numpy as np\n",
    "data_1 = Oracle_Assignment_2.q3_linear_1(23627)\n",
    "data_2 = Oracle_Assignment_2.q3_linear_2(23627)\n",
    "(X_train_1,y_train_1,X_test_1,y_test_1) = data_1\n",
    "(X_train_2,y_train_2,X_test_2,y_test_2) = data_2\n",
    "X_train_1,y_train_1 = np.array(X_train_1),np.array(y_train_1)\n",
    "X_test_1,y_test_1 = np.array(X_test_1),np.array(y_test_1)\n",
    "X_train_2,y_train_2 = np.array(X_train_2),np.array(y_train_2)\n",
    "X_test_2,y_test_2 = np.array(X_test_2),np.array(y_test_2)\n",
    "D_1_train = (X_train_1, y_train_1)\n",
    "D_2_train = (X_train_2, y_train_2)\n",
    "D_1_test = (X_test_1, y_test_1)\n",
    "D_2_test = (X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-2 : Compute weights using OLS and RR for D_train1 and D_train2 (use Lambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of weight vectors:\n",
      "w1_ols: (5, 1)\n",
      "w1_rr: (5, 1)\n",
      "w2_ols: (100, 1)\n",
      "w2_rr: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "def Solve_OLS(X,y):\n",
    "    X_T = X.T  # Transpose of X\n",
    "    X_T_X = np.matmul(X_T, X)  # X^T X using matmul\n",
    "    X_T_y = np.dot(X_T, y)  # X^T y using dot\n",
    "    w = np.dot(np.linalg.inv(X_T_X), X_T_y)  # (X^T X)^(-1) (X^T y) using inverse and dot\n",
    "    return w\n",
    "def Solve_RR(X, y, lambda_=1.0):\n",
    "    n, d = X.shape  # Number of samples and features\n",
    "    X_T = X.T  # Transpose of X\n",
    "    X_T_X = np.matmul(X_T, X)  # X^T X\n",
    "    I = np.eye(d)  # d × d identity matrix\n",
    "    reg_term = n * lambda_ * I  # Regularization term\n",
    "    X_T_X_reg = X_T_X + reg_term  # X^T X + nλI\n",
    "    X_T_y = np.dot(X_T, y)  # X^T y\n",
    "    w = np.dot(np.linalg.inv(X_T_X_reg), X_T_y)  # (X^T X + nλI)^(-1) (X^T y)\n",
    "    return w\n",
    "def Compute_Weights(D1_train, D2_train):\n",
    "    # Unpack datasets\n",
    "    X1, y1 = D1_train\n",
    "    X2, y2 = D2_train\n",
    "\n",
    "    # Compute weights for D1_train\n",
    "    w1_ols = Solve_OLS(X1, y1)\n",
    "    w1_rr = Solve_RR(X1, y1, lambda_=1.0)\n",
    "\n",
    "    # Compute weights for D2_train\n",
    "    w2_ols = Solve_OLS(X2, y2)\n",
    "    w2_rr = Solve_RR(X2, y2, lambda_=1.0)\n",
    "\n",
    "    return w1_ols, w1_rr, w2_ols, w2_rr\n",
    "w1_ols, w1_rr, w2_ols, w2_rr = Compute_Weights(D_1_train, D_2_train)\n",
    "print(\"Shapes of weight vectors:\")\n",
    "print(f\"w1_ols: {w1_ols.shape}\")\n",
    "print(f\"w1_rr: {w1_rr.shape}\")\n",
    "print(f\"w2_ols: {w2_ols.shape}\")\n",
    "print(f\"w2_rr: {w2_rr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-3 : Calculate MSE for w1_ols and w1_rr using D_1_train and MSE for w2_ols and w2_rr using D_2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for w1_ols on D_1_train: [0.03340458]\n",
      "MSE for w1_rr on D_1_train: [0.11333904]\n",
      "MSE for w2_ols on D_2_train: [7115.79566132]\n",
      "MSE for w2_rr on D_2_train: [2.90381334]\n"
     ]
    }
   ],
   "source": [
    "def Compute_MSE(X, y, w):\n",
    "    n = len(y)\n",
    "    y_pred = np.dot(X, w)  # Predictions: X w\n",
    "    residuals = y - y_pred  # y - ŷ\n",
    "    squared_residuals = residuals ** 2  # (y - ŷ)^2\n",
    "    mse = sum(squared_residuals) / n  # Mean of squared residuals\n",
    "    return mse\n",
    "\n",
    "X1, y1 = D_1_train\n",
    "X2, y2 = D_2_train\n",
    "\n",
    "# Calculate MSE for each weight vector\n",
    "mse_w1_ols = Compute_MSE(X1, y1, w1_ols)\n",
    "mse_w1_rr = Compute_MSE(X1, y1, w1_rr)\n",
    "mse_w2_ols = Compute_MSE(X2, y2, w2_ols)\n",
    "mse_w2_rr = Compute_MSE(X2, y2, w2_rr)\n",
    "\n",
    "# Print the results\n",
    "print(\"MSE for w1_ols on D_1_train:\", mse_w1_ols)\n",
    "print(\"MSE for w1_rr on D_1_train:\", mse_w1_rr)\n",
    "print(\"MSE for w2_ols on D_2_train:\", mse_w2_ols)\n",
    "print(\"MSE for w2_rr on D_2_train:\", mse_w2_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delivearbles : \n",
    "\n",
    "1. Can you do OLS if X does not have full column rank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code not needed mostly \n",
    "# import numpy as np\n",
    "\n",
    "# # Create a rank-deficient X (3 samples, 3 features, but rank 2)\n",
    "# X = np.array([[1, 2, 2],\n",
    "#               [2, 4, 4],\n",
    "#               [3, 6, 6]])\n",
    "# y = np.array([1, 2, 3])\n",
    "\n",
    "# # Attempt standard OLS\n",
    "# try:\n",
    "#     X_T_X = np.dot(X.T, X)\n",
    "#     X_T_X_inv = np.linalg.inv(X_T_X)  # This will fail\n",
    "#     w_ols = np.dot(X_T_X_inv, np.dot(X.T, y))\n",
    "# except np.linalg.LinAlgError as e:\n",
    "#     print(\"Error: X^T X is singular, cannot compute standard OLS solution.\")\n",
    "\n",
    "# # Use pseudo-inverse (via lstsq)\n",
    "# w_ols_pseudo = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "# print(\"OLS solution using pseudo-inverse:\", w_ols_pseudo)\n",
    "\n",
    "# # Ridge Regression (will work)\n",
    "# lambda_ = 1.0\n",
    "# n, d = X.shape\n",
    "# X_T_X_reg = X_T_X + n * lambda_ * np.eye(d)\n",
    "# w_rr = np.dot(np.linalg.inv(X_T_X_reg), np.dot(X.T, y))\n",
    "# print(\"Ridge Regression solution:\", w_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverabeles:\n",
    "\n",
    "2. Report MSE on D_1_train. Report w1_ols and w1_rr in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for w1_ols on D_1_train: [0.03340458]\n",
      "MSE for w1_rr on D_1_train: [0.11333904]\n",
      "[[0.29247212]\n",
      " [0.22938018]\n",
      " [0.25741656]\n",
      " [0.37920109]\n",
      " [0.0631003 ]] [[0.10001808]\n",
      " [0.06957697]\n",
      " [0.08057619]\n",
      " [0.22592413]\n",
      " [0.01404171]]\n"
     ]
    }
   ],
   "source": [
    "w1_ols, w1_rr, w2_ols, w2_rr = Compute_Weights(D_1_train, D_2_train)\n",
    "X = D_1_train[0]\n",
    "y = D_1_train[1]\n",
    "w1_ols = Solve_OLS(X,y)\n",
    "w1_rr = Solve_RR(X,y,lambda_=1.0)\n",
    "mse_w1_ols = Compute_MSE(X,y,w1_ols)\n",
    "mse_w1_rr = Compute_MSE(X,y,w1_rr)\n",
    "print(\"MSE for w1_ols on D_1_train:\", mse_w1_ols)\n",
    "print(\"MSE for w1_rr on D_1_train:\", mse_w1_rr)\n",
    "print(w1_ols,w1_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables: \n",
    "\n",
    "3. Report MSE on Dtrain2 . Attach w2_ols and w2_rr as csv files named w_ols_[five-digit-srnumber].csv and w_rr_[five-digit-srnumber].csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for w2_ols on D_2_train: [7115.79566132]\n",
      "MSE for w2_rr on D_2_train: [2.90381334]\n",
      "Saved weights to w_ols_23627.csv and w_rr_23627.csv\n"
     ]
    }
   ],
   "source": [
    "X2, y2 = D_2_train\n",
    "mse_w2_ols = Compute_MSE(X2, y2, w2_ols)\n",
    "mse_w2_rr = Compute_MSE(X2, y2, w2_rr)\n",
    "\n",
    "# Print MSE for reporting in PDF\n",
    "print(\"MSE for w2_ols on D_2_train:\", mse_w2_ols)\n",
    "print(\"MSE for w2_rr on D_2_train:\", mse_w2_rr)\n",
    "\n",
    "# Save weights to CSV files\n",
    "srn = 23627\n",
    "np.savetxt(f\"w_ols_{srn}.csv\", w2_ols, delimiter=\",\")\n",
    "np.savetxt(f\"w_rr_{srn}.csv\", w2_rr, delimiter=\",\")\n",
    "print(f\"Saved weights to w_ols_{srn}.csv and w_rr_{srn}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned stock ticker: MRK\n",
      "Total number of days: 1258\n",
      "\n",
      "Data split for t = 7:\n",
      "  X_train shape: (625, 7)\n",
      "  y_train shape: (625,)\n",
      "  X_test shape: (626, 7)\n",
      "  y_test shape: (626,)\n",
      "\n",
      "Data split for t = 30:\n",
      "  X_train shape: (614, 30)\n",
      "  y_train shape: (614,)\n",
      "  X_test shape: (614, 30)\n",
      "  y_test shape: (614,)\n",
      "\n",
      "Data split for t = 90:\n",
      "  X_train shape: (584, 90)\n",
      "  y_train shape: (584,)\n",
      "  X_test shape: (584, 90)\n",
      "  y_test shape: (584,)\n"
     ]
    }
   ],
   "source": [
    "import Oracle_Assignment_2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Query the oracle to get the stock ticker\n",
    "Stock_Name = Oracle_Assignment_2.q3_stocknet(23627)\n",
    "print(f\"Assigned stock ticker: {Stock_Name}\")  # Returns MRK\n",
    "\n",
    "# Step 2: Load the stock data from the CSV file\n",
    "# Assuming the stocknet-dataset has been cloned to the current directory\n",
    "csv_path = f\"stocknet-dataset/price/raw/{Stock_Name}.csv\"\n",
    "stock_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 3: Extract closing prices and normalize\n",
    "# Extract the 'Close' column\n",
    "closing_prices = stock_data['Close'].values  # Shape: (N,)\n",
    "\n",
    "# Normalize the closing prices using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "closing_prices_normalized = scaler.fit_transform(closing_prices.reshape(-1, 1)).flatten()\n",
    "N = len(closing_prices_normalized)  # Total number of days\n",
    "print(f\"Total number of days: {N}\")\n",
    "\n",
    "# Step 4 & 5: Create feature matrix X and labels y for each t in {7, 30, 90}\n",
    "data_splits = {}\n",
    "for t in [7, 30, 90]:\n",
    "    # Create X: Each row contains t consecutive days\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(N - t):\n",
    "        # Row i: closing prices from day i to day i+t-1\n",
    "        X.append(closing_prices_normalized[i:i+t])\n",
    "        # Label for row i: closing price on day i+t\n",
    "        y.append(closing_prices_normalized[i+t])\n",
    "    \n",
    "    X = np.array(X)  # Shape: (N-t, t)\n",
    "    y = np.array(y)  # Shape: (N-t,)\n",
    "    \n",
    "    # Step 6: Split into train and test sets (first half for training, second half for testing)\n",
    "    split_idx = (N - t) // 2\n",
    "    X_train = X[:split_idx]  # Shape: (split_idx, t)\n",
    "    y_train = y[:split_idx]  # Shape: (split_idx,)\n",
    "    X_test = X[split_idx:]   # Shape: (N-t-split_idx, t)\n",
    "    y_test = y[split_idx:]   # Shape: (N-t-split_idx,)\n",
    "    \n",
    "    # Store the data split\n",
    "    data_splits[t] = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler  # Save the scaler for denormalization later\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nData split for t = {t}:\")\n",
    "    print(f\"  X_train shape: {X_train.shape}\")\n",
    "    print(f\"  y_train shape: {y_train.shape}\")\n",
    "    print(f\"  X_test shape: {X_test.shape}\")\n",
    "    print(f\"  y_test shape: {y_test.shape}\")\n",
    "\n",
    "# The data_splits dictionary now contains the preprocessed data for each t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the following SVRs using the train set for t ∈{7,30,90}:\n",
    "\n",
    "1. Solve the dual of the slack linear support vector regression using cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SVR for t = 7:\n",
      "  w shape: (7,), b: 0.0004\n",
      "  Number of support vectors: 121\n",
      "\n",
      "Trained SVR for t = 30:\n",
      "  w shape: (30,), b: 0.0075\n",
      "  Number of support vectors: 137\n",
      "\n",
      "Trained SVR for t = 90:\n",
      "  w shape: (90,), b: 0.0144\n",
      "  Number of support vectors: 169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import cvxopt\n",
    "from cvxopt import matrix, solvers\n",
    "def train_linear_svr(X_train, y_train, epsilon=0.1, C=1.0):\n",
    "    \"\"\"\n",
    "    Train a linear Support Vector Regression (SVR) model using the dual formulation with cvxopt.\n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): Feature matrix of shape (N, t), where N is the number of samples\n",
    "                           and t is the number of features (past days).\n",
    "        y_train (ndarray): Target vector of shape (N,), the values to predict.\n",
    "        epsilon (float): Epsilon parameter for the epsilon-insensitive loss (default: 0.1).\n",
    "        C (float): Regularization parameter controlling the trade-off between margin and error (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "        w (ndarray): Weight vector of shape (t,), the coefficients of the linear model.\n",
    "        b (float): Bias term of the linear model.\n",
    "        alphas (ndarray): Dual variables alpha of shape (N,), Lagrange multipliers for positive deviations.\n",
    "        alphas_star (ndarray): Dual variables alpha* of shape (N,), Lagrange multipliers for negative deviations.\n",
    "    \"\"\"\n",
    "    # Number of samples (N) and features (t)\n",
    "    N, t = X_train.shape\n",
    "\n",
    "    # Compute the linear kernel matrix K = X @ X.T\n",
    "    # For linear SVR, the kernel is the dot product of feature vectors\n",
    "    K = X_train @ X_train.T\n",
    "\n",
    "    # --- Dual Formulation Setup ---\n",
    "    # The dual problem for linear SVR is:\n",
    "    # Maximize: -1/2 * sum_i sum_j (alpha_i - alpha_i*)(alpha_j - alpha_j*) * (x_i^T x_j)\n",
    "    #           - epsilon * sum_i (alpha_i + alpha_i*) + sum_i y_i (alpha_i - alpha_i*)\n",
    "    # Subject to:\n",
    "    #   1. sum_i (alpha_i - alpha_i*) = 0 (equality constraint)\n",
    "    #   2. 0 <= alpha_i <= C, 0 <= alpha_i* <= C (box constraints)\n",
    "    # where:\n",
    "    #   - alpha_i, alpha_i*: Dual variables for each sample (Lagrange multipliers)\n",
    "    #   - x_i: Feature vector for sample i\n",
    "    #   - y_i: Target value for sample i\n",
    "    #   - epsilon: Width of the epsilon-insensitive tube\n",
    "    #   - C: Upper bound on dual variables, controls regularization\n",
    "\n",
    "    # cvxopt solves: min 1/2 z^T P z + q^T z\n",
    "    # subject to G z <= h, A z = b\n",
    "    # We convert the maximization problem to minimization by negating the objective\n",
    "\n",
    "    # Define z = [alpha_1, ..., alpha_N, alpha_1*, ..., alpha_N*], size 2N\n",
    "    # Quadratic term P = [[K, -K], [-K, K]], where K is the kernel matrix\n",
    "    P = np.block([[K, -K], [-K, K]])\n",
    "    P = matrix(P)\n",
    "\n",
    "    # Linear term q = epsilon * 1_{2N} - [y; -y]\n",
    "    q = epsilon * np.ones(2 * N)\n",
    "    q[:N] -= y_train  # First N elements: epsilon - y_i\n",
    "    q[N:] += y_train  # Last N elements: epsilon + y_i\n",
    "    q = matrix(q)\n",
    "\n",
    "    # Equality constraint: sum(alpha_i) - sum(alpha_i*) = 0\n",
    "    A = np.hstack([np.ones(N), -np.ones(N)])  # [1, 1, ..., 1, -1, -1, ..., -1]\n",
    "    A = matrix(A, (1, 2 * N))\n",
    "    b = matrix(0.0)\n",
    "\n",
    "    # Inequality constraints: 0 <= z <= C\n",
    "    # G z <= h includes:\n",
    "    #   -z <= 0 (i.e., z >= 0)\n",
    "    #   z <= C\n",
    "    G = np.vstack([-np.eye(2 * N), np.eye(2 * N)])\n",
    "    h = np.hstack([np.zeros(2 * N), C * np.ones(2 * N)])\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "\n",
    "    # Solve the quadratic program\n",
    "    solvers.options['show_progress'] = False  # Suppress solver output\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    z = np.array(solution['x']).flatten()\n",
    "\n",
    "    # Extract alpha and alpha* from the solution\n",
    "    alphas = z[:N]\n",
    "    alphas_star = z[N:]\n",
    "\n",
    "    # Compute the weight vector w = sum_i (alpha_i - alpha_i*) x_i\n",
    "    w = np.sum((alphas - alphas_star)[:, np.newaxis] * X_train, axis=0)\n",
    "\n",
    "    # Compute the bias b using support vectors (where 0 < alpha_i < C or 0 < alpha_i* < C)\n",
    "    # For a support vector:\n",
    "    #   If alpha_i > 0: y_i = w^T x_i + b + epsilon => b = y_i - w^T x_i - epsilon\n",
    "    #   If alpha_i* > 0: y_i = w^T x_i + b - epsilon => b = y_i - w^T x_i + epsilon\n",
    "    support_indices = np.where((alphas > 1e-5) & (alphas < C - 1e-5))[0]\n",
    "    if len(support_indices) == 0:\n",
    "        support_indices = np.where((alphas_star > 1e-5) & (alphas_star < C - 1e-5))[0]\n",
    "    \n",
    "    b_values = []\n",
    "    for i in support_indices:\n",
    "        if alphas[i] > 0:\n",
    "            b_i = y_train[i] - np.dot(w, X_train[i]) - epsilon\n",
    "        else:\n",
    "            b_i = y_train[i] - np.dot(w, X_train[i]) + epsilon\n",
    "        b_values.append(b_i)\n",
    "    b = np.mean(b_values) if b_values else 0.0  # Average b over support vectors\n",
    "\n",
    "    return w, b, alphas, alphas_star\n",
    "for t in [7, 30, 90]:\n",
    "    # Example: N = 100 samples, t features\n",
    "    X_train_t = data_splits[t][\"X_train\"]\n",
    "    y_train_t = data_splits[t][\"y_train\"]\n",
    "    \n",
    "    # Train the SVR model\n",
    "    w, b, alphas, alphas_star = train_linear_svr(X_train_t, y_train_t, epsilon=0.1, C=1.0)\n",
    "    \n",
    "    print(f\"Trained SVR for t = {t}:\")\n",
    "    print(f\"  w shape: {w.shape}, b: {b:.4f}\")\n",
    "    print(f\"  Number of support vectors: {np.sum((alphas > 1e-5) | (alphas_star > 1e-5))}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the following SVRs using the train set for t ∈{7,30,90}:\n",
    "\n",
    "2. Solve the dual of the kernelized support vector regression using the RBF kernel for γ = [1,0.1,0.01,0.001]\n",
    "using cvxopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RBF SVR for t = 7, gamma = 1\n",
      "  Number of support vectors: 158\n",
      "  Bias term b: -0.3682\n",
      "\n",
      "Training RBF SVR for t = 7, gamma = 0.1\n",
      "  Number of support vectors: 142\n",
      "  Bias term b: -0.4617\n",
      "\n",
      "Training RBF SVR for t = 7, gamma = 0.01\n",
      "  Number of support vectors: 178\n",
      "  Bias term b: -0.6180\n",
      "\n",
      "Training RBF SVR for t = 7, gamma = 0.001\n",
      "  Number of support vectors: 246\n",
      "  Bias term b: 0.0000\n",
      "\n",
      "Training RBF SVR for t = 30, gamma = 1\n",
      "  Number of support vectors: 259\n",
      "  Bias term b: -0.1084\n",
      "\n",
      "Training RBF SVR for t = 30, gamma = 0.1\n",
      "  Number of support vectors: 165\n",
      "  Bias term b: -0.1637\n",
      "\n",
      "Training RBF SVR for t = 30, gamma = 0.01\n",
      "  Number of support vectors: 191\n",
      "  Bias term b: -0.3396\n",
      "\n",
      "Training RBF SVR for t = 30, gamma = 0.001\n",
      "  Number of support vectors: 288\n",
      "  Bias term b: -0.8986\n",
      "\n",
      "Training RBF SVR for t = 90, gamma = 1\n",
      "  Number of support vectors: 515\n",
      "  Bias term b: -0.0675\n",
      "\n",
      "Training RBF SVR for t = 90, gamma = 0.1\n",
      "  Number of support vectors: 151\n",
      "  Bias term b: -0.1156\n",
      "\n",
      "Training RBF SVR for t = 90, gamma = 0.01\n",
      "  Number of support vectors: 206\n",
      "  Bias term b: -0.1599\n",
      "\n",
      "Training RBF SVR for t = 90, gamma = 0.001\n",
      "  Number of support vectors: 280\n",
      "  Bias term b: -0.5314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from cvxopt import matrix, solvers\n",
    "def train_rbf_svr(X_train,y_train,gamma,epsilon=0.1,C=1.0):\n",
    "    def rbf_kernel(X1, X2, gamma):\n",
    "        N1, t = X1.shape\n",
    "        N2, _ = X2.shape\n",
    "        K = np.zeros((N1, N2))\n",
    "        for i in range(N1):\n",
    "            for j in range(N2):\n",
    "                # Compute K(x_i, x_j) = exp(-gamma * ||x_i - x_j||^2)\n",
    "                diff = X1[i] - X2[j]\n",
    "                K[i, j] = np.exp(-gamma * np.sum(diff ** 2))\n",
    "        return K\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    # Compute the RBF kernel matrix K = K(X_train, X_train)\n",
    "    K = rbf_kernel(X_train, X_train, gamma)\n",
    "    P = np.block([[K, -K], [-K, K]])\n",
    "    P = matrix(P)\n",
    "\n",
    "    # Linear term q = epsilon * 1_{2N} - [y; -y]\n",
    "    q = epsilon * np.ones(2 * N)\n",
    "    q[:N] -= y_train  # First N elements: epsilon - y_i\n",
    "    q[N:] += y_train  # Last N elements: epsilon + y_i\n",
    "    q = matrix(q)\n",
    "\n",
    "    # Equality constraint: sum(alpha_i) - sum(alpha_i*) = 0\n",
    "    A = np.hstack([np.ones(N), -np.ones(N)])  # [1, 1, ..., 1, -1, -1, ..., -1]\n",
    "    A = matrix(A, (1, 2 * N))\n",
    "    b = matrix(0.0)\n",
    "\n",
    "    # Inequality constraints: 0 <= z <= C\n",
    "    G = np.vstack([-np.eye(2 * N), np.eye(2 * N)])\n",
    "    h = np.hstack([np.zeros(2 * N), C * np.ones(2 * N)])\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "\n",
    "    # Solve the quadratic program\n",
    "    solvers.options['show_progress'] = False  # Suppress solver output\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    z = np.array(solution['x']).flatten()\n",
    "\n",
    "    # Extract alpha and alpha* from the solution\n",
    "    alphas = z[:N]\n",
    "    alphas_star = z[N:]\n",
    "\n",
    "    # Compute the bias b using support vectors (where 0 < alpha_i < C or 0 < alpha_i* < C)\n",
    "    # For a support vector:\n",
    "    #   If alpha_i > 0: y_i = sum_j (alpha_j - alpha_j*) K(x_j, x_i) + b + epsilon\n",
    "    #   If alpha_i* > 0: y_i = sum_j (alpha_j - alpha_j*) K(x_j, x_i) + b - epsilon\n",
    "    support_indices = np.where((alphas > 1e-5) & (alphas < C - 1e-5))[0]\n",
    "    if len(support_indices) == 0:\n",
    "        support_indices = np.where((alphas_star > 1e-5) & (alphas_star < C - 1e-5))[0]\n",
    "    \n",
    "    b_values = []\n",
    "    for i in support_indices:\n",
    "        # Compute sum_j (alpha_j - alpha_j*) K(x_j, x_i)\n",
    "        kernel_row = K[i]\n",
    "        kernel_sum = np.sum((alphas - alphas_star) * kernel_row)\n",
    "        if alphas[i] > 0:\n",
    "            b_i = y_train[i] - kernel_sum - epsilon\n",
    "        else:\n",
    "            b_i = y_train[i] - kernel_sum + epsilon\n",
    "        b_values.append(b_i)\n",
    "    b = np.mean(b_values) if b_values else 0.0  # Average b over support vectors\n",
    "\n",
    "    return alphas, alphas_star, b\n",
    "gamma_values = [1,0.1,0.01,0.001]\n",
    "models = {}\n",
    "for t in [7, 30, 90]:\n",
    "    models[t] = {}\n",
    "    X_train = data_splits[t]['X_train']\n",
    "    y_train = data_splits[t]['y_train']\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        print(f\"\\nTraining RBF SVR for t = {t}, gamma = {gamma}\")\n",
    "        alphas, alphas_star, b = train_rbf_svr(X_train, y_train, gamma, epsilon=0.1, C=1.0)\n",
    "        \n",
    "        # Store the model parameters\n",
    "        models[t][gamma] = {\n",
    "            'alphas': alphas,\n",
    "            'alphas_star': alphas_star,\n",
    "            'b': b,\n",
    "            'X_train': X_train,  # Needed for predictions\n",
    "            'y_train': y_train\n",
    "        }\n",
    "        \n",
    "        # Report the number of support vectors\n",
    "        num_support_vectors = np.sum((alphas > 1e-5) | (alphas_star > 1e-5))\n",
    "        print(f\"  Number of support vectors: {num_support_vectors}\")\n",
    "        print(f\"  Bias term b: {b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class SVR:\n",
    "    \"\"\"A class to train Support Vector Regression (SVR) models using the dual formulation with cvxopt.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=0.1, C=1.0):\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.C = C\n",
    "        self.models = {}  # Store trained models for each t and gamma\n",
    "\n",
    "    def rbf_kernel(self, X1, X2, gamma):\n",
    "\n",
    "        N1 = len(X1)\n",
    "        N2 = len(X2)\n",
    "        K = np.zeros((N1, N2))\n",
    "        for i in range(N1):\n",
    "            for j in range(N2):\n",
    "                diff = X1[i] - X2[j]\n",
    "                K[i, j] = np.exp(-gamma * np.sum(diff ** 2))\n",
    "        return K\n",
    "\n",
    "    def train_linear_svr(self, X_train, y_train):\n",
    "\n",
    "        N, t = X_train.shape\n",
    "        K = X_train @ X_train.T  # Linear kernel: K = X @ X.T\n",
    "\n",
    "        # Dual Formulation Setup\n",
    "        # Maximize: -1/2 * sum_i sum_j (alpha_i - alpha_i*)(alpha_j - alpha_j*) * (x_i^T x_j)\n",
    "        #           - epsilon * sum_i (alpha_i + alpha_i*) + sum_i y_i (alpha_i - alpha_i*)\n",
    "        # Subject to: sum_i (alpha_i - alpha_i*) = 0, 0 <= alpha_i, alpha_i* <= C\n",
    "\n",
    "        P = np.block([[K, -K], [-K, K]])\n",
    "        P = matrix(P)\n",
    "\n",
    "        q = self.epsilon * np.ones(2 * N)\n",
    "        q[:N] -= y_train\n",
    "        q[N:] += y_train\n",
    "        q = matrix(q)\n",
    "\n",
    "        A = np.hstack([np.ones(N), -np.ones(N)])\n",
    "        A = matrix(A, (1, 2 * N))\n",
    "        b = matrix(0.0)\n",
    "\n",
    "        G = np.vstack([-np.eye(2 * N), np.eye(2 * N)])\n",
    "        h = np.hstack([np.zeros(2 * N), self.C * np.ones(2 * N)])\n",
    "        G = matrix(G)\n",
    "        h = matrix(h)\n",
    "\n",
    "        solvers.options['show_progress'] = False\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        z = np.array(solution['x']).flatten()\n",
    "\n",
    "        alphas = z[:N]\n",
    "        alphas_star = z[N:]\n",
    "\n",
    "        w = np.sum((alphas - alphas_star)[:, np.newaxis] * X_train, axis=0)\n",
    "\n",
    "        support_indices = np.where((alphas > 1e-5) & (alphas < self.C - 1e-5))[0]\n",
    "        if len(support_indices) == 0:\n",
    "            support_indices = np.where((alphas_star > 1e-5) & (alphas_star < self.C - 1e-5))[0]\n",
    "\n",
    "        b_values = []\n",
    "        for i in support_indices:\n",
    "            if alphas[i] > 0:\n",
    "                b_i = y_train[i] - np.dot(w, X_train[i]) - self.epsilon\n",
    "            else:\n",
    "                b_i = y_train[i] - np.dot(w, X_train[i]) + self.epsilon\n",
    "            b_values.append(b_i)\n",
    "        b = np.mean(b_values) if b_values else 0.0\n",
    "\n",
    "        return w, b, alphas, alphas_star\n",
    "\n",
    "    def train_rbf_svr(self, X_train, y_train, gamma):\n",
    "\n",
    "        N = X_train.shape[0]\n",
    "        K = self.rbf_kernel(X_train, X_train, gamma)\n",
    "\n",
    "        # Dual Formulation Setup\n",
    "        # Maximize: -1/2 * sum_i sum_j (alpha_i - alpha_i*)(alpha_j - alpha_j*) * K(x_i, x_j)\n",
    "        #           - epsilon * sum_i (alpha_i + alpha_i*) + sum_i y_i (alpha_i - alpha_i*)\n",
    "        # Subject to: sum_i (alpha_i - alpha_i*) = 0, 0 <= alpha_i, alpha_i* <= C\n",
    "\n",
    "        P = np.block([[K, -K], [-K, K]])\n",
    "        P = matrix(P)\n",
    "\n",
    "        q = self.epsilon * np.ones(2 * N)\n",
    "        q[:N] -= y_train\n",
    "        q[N:] += y_train\n",
    "        q = matrix(q)\n",
    "\n",
    "        A = np.hstack([np.ones(N), -np.ones(N)])\n",
    "        A = matrix(A, (1, 2 * N))\n",
    "        b = matrix(0.0)\n",
    "\n",
    "        G = np.vstack([-np.eye(2 * N), np.eye(2 * N)])\n",
    "        h = np.hstack([np.zeros(2 * N), self.C * np.ones(2 * N)])\n",
    "        G = matrix(G)\n",
    "        h = matrix(h)\n",
    "\n",
    "        solvers.options['show_progress'] = False\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        z = np.array(solution['x']).flatten()\n",
    "\n",
    "        alphas = z[:N]\n",
    "        alphas_star = z[N:]\n",
    "\n",
    "        support_indices = np.where((alphas > 1e-5) & (alphas < self.C - 1e-5))[0]\n",
    "        if len(support_indices) == 0:\n",
    "            support_indices = np.where((alphas_star > 1e-5) & (alphas_star < self.C - 1e-5))[0]\n",
    "\n",
    "        b_values = []\n",
    "        for i in support_indices:\n",
    "            kernel_row = K[i]\n",
    "            kernel_sum = np.sum((alphas - alphas_star) * kernel_row)\n",
    "            if alphas[i] > 0:\n",
    "                b_i = y_train[i] - kernel_sum - self.epsilon\n",
    "            else:\n",
    "                b_i = y_train[i] - kernel_sum + self.epsilon\n",
    "            b_values.append(b_i)\n",
    "        b = np.mean(b_values) if b_values else 0.0\n",
    "\n",
    "        return alphas, alphas_star, b\n",
    "\n",
    "    def task_1(self, data_splits):\n",
    "        \"\"\"\n",
    "        Task 1: Train linear SVR models for t in {7, 30, 90} and print results.\n",
    "\n",
    "        Args:\n",
    "            data_splits (dict): Dictionary containing preprocessed data for each t.\n",
    "        \"\"\"\n",
    "        print(\"Task 1: Training Linear SVR Models\\n\")\n",
    "        for t in [7, 30, 90]:\n",
    "            X_train = data_splits[t]['X_train']\n",
    "            y_train = data_splits[t]['y_train']\n",
    "            \n",
    "            w, b, alphas, alphas_star = self.train_linear_svr(X_train, y_train)\n",
    "            \n",
    "            # Store the model\n",
    "            if t not in self.models:\n",
    "                self.models[t] = {}\n",
    "            self.models[t]['linear'] = {\n",
    "                'w': w,\n",
    "                'b': b,\n",
    "                'alphas': alphas,\n",
    "                'alphas_star': alphas_star,\n",
    "                'X_train': X_train,\n",
    "                'y_train': y_train\n",
    "            }\n",
    "            \n",
    "            num_support_vectors = np.sum((alphas > 1e-5) | (alphas_star > 1e-5))\n",
    "            print(f\"Trained Linear SVR for t = {t}:\")\n",
    "            print(f\"  w shape: {w.shape}, b: {b:.4f}\")\n",
    "            print(f\"  Number of support vectors: {num_support_vectors}\\n\")\n",
    "\n",
    "    def task_2(self, data_splits):\n",
    "        \"\"\"\n",
    "        Task 2: Train RBF SVR models for t in {7, 30, 90} and gamma in {1, 0.1, 0.01, 0.001}.\n",
    "\n",
    "        Args:\n",
    "            data_splits (dict): Dictionary containing preprocessed data for each t.\n",
    "        \"\"\"\n",
    "        gamma_values = [1, 0.1, 0.01, 0.001]\n",
    "        print(\"Task 2: Training RBF SVR Models\\n\")\n",
    "        for t in [7, 30, 90]:\n",
    "            X_train = data_splits[t]['X_train']\n",
    "            y_train = data_splits[t]['y_train']\n",
    "            \n",
    "            for gamma in gamma_values:\n",
    "                alphas, alphas_star, b = self.train_rbf_svr(X_train, y_train, gamma)\n",
    "                \n",
    "                # Store the model\n",
    "                if t not in self.models:\n",
    "                    self.models[t] = {}\n",
    "                self.models[t][gamma] = {\n",
    "                    'alphas': alphas,\n",
    "                    'alphas_star': alphas_star,\n",
    "                    'b': b,\n",
    "                    'X_train': X_train,\n",
    "                    'y_train': y_train\n",
    "                }\n",
    "                \n",
    "                num_support_vectors = np.sum((alphas > 1e-5) | (alphas_star > 1e-5))\n",
    "                print(f\"Trained RBF SVR for t = {t}, gamma = {gamma}:\")\n",
    "                print(f\"  Number of support vectors: {num_support_vectors}\")\n",
    "                print(f\"  Bias term b: {b:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables :\n",
    "\n",
    "For each SVR trained, plot a graph on the test set containing the following:\n",
    "1. Predicted closing price value.\n",
    "2. Actual closing price value.\n",
    "3. Average price on the previous t days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Training Linear SVR Models\n",
      "\n",
      "Trained Linear SVR for t = 7:\n",
      "  w shape: (7,), b: 0.0004\n",
      "  Number of support vectors: 121\n",
      "\n",
      "Trained Linear SVR for t = 30:\n",
      "  w shape: (30,), b: 0.0075\n",
      "  Number of support vectors: 137\n",
      "\n",
      "Trained Linear SVR for t = 90:\n",
      "  w shape: (90,), b: 0.0144\n",
      "  Number of support vectors: 169\n",
      "\n",
      "Task 2: Training RBF SVR Models\n",
      "\n",
      "Trained RBF SVR for t = 7, gamma = 1:\n",
      "  Number of support vectors: 158\n",
      "  Bias term b: -0.3682\n",
      "\n",
      "Trained RBF SVR for t = 7, gamma = 0.1:\n",
      "  Number of support vectors: 142\n",
      "  Bias term b: -0.4617\n",
      "\n",
      "Trained RBF SVR for t = 7, gamma = 0.01:\n",
      "  Number of support vectors: 178\n",
      "  Bias term b: -0.6180\n",
      "\n",
      "Trained RBF SVR for t = 7, gamma = 0.001:\n",
      "  Number of support vectors: 246\n",
      "  Bias term b: 0.0000\n",
      "\n",
      "Trained RBF SVR for t = 30, gamma = 1:\n",
      "  Number of support vectors: 259\n",
      "  Bias term b: -0.1084\n",
      "\n",
      "Trained RBF SVR for t = 30, gamma = 0.1:\n",
      "  Number of support vectors: 165\n",
      "  Bias term b: -0.1637\n",
      "\n",
      "Trained RBF SVR for t = 30, gamma = 0.01:\n",
      "  Number of support vectors: 191\n",
      "  Bias term b: -0.3396\n",
      "\n",
      "Trained RBF SVR for t = 30, gamma = 0.001:\n",
      "  Number of support vectors: 288\n",
      "  Bias term b: -0.8986\n",
      "\n",
      "Trained RBF SVR for t = 90, gamma = 1:\n",
      "  Number of support vectors: 515\n",
      "  Bias term b: -0.0675\n",
      "\n",
      "Trained RBF SVR for t = 90, gamma = 0.1:\n",
      "  Number of support vectors: 151\n",
      "  Bias term b: -0.1156\n",
      "\n",
      "Trained RBF SVR for t = 90, gamma = 0.01:\n",
      "  Number of support vectors: 206\n",
      "  Bias term b: -0.1599\n",
      "\n",
      "Trained RBF SVR for t = 90, gamma = 0.001:\n",
      "  Number of support vectors: 280\n",
      "  Bias term b: -0.5314\n",
      "\n",
      "Plots generated for all SVR models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma):\n",
    "    N1, t = X1.shape\n",
    "    N2, _ = X2.shape\n",
    "    K = np.zeros((N1, N2))\n",
    "    for i in range(N1):\n",
    "        for j in range(N2):\n",
    "            diff = X1[i] - X2[j]\n",
    "            K[i, j] = np.exp(-gamma * np.sum(diff ** 2))\n",
    "    return K\n",
    "\n",
    "def predict_linear(X, model):\n",
    "    w = model['w']\n",
    "    b = model['b']\n",
    "    y_pred = X @ w + b\n",
    "    return y_pred\n",
    "\n",
    "def predict_rbf(X, model, gamma):\n",
    "    alphas = model['alphas']\n",
    "    alphas_star = model['alphas_star']\n",
    "    b = model['b']\n",
    "    X_train = model['X_train']\n",
    "    K = rbf_kernel(X, X_train, gamma)\n",
    "    y_pred = np.sum((alphas - alphas_star) * K, axis=1) + b\n",
    "    return y_pred\n",
    "\n",
    "def compute_average_prices(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def plot_results(data_splits, svr_trainer):\n",
    "    gamma_values = [1, 0.1, 0.01, 0.001]\n",
    "    \n",
    "    for t in [7, 30, 90]:\n",
    "        X_test = data_splits[t]['X_test']\n",
    "        y_test = data_splits[t]['y_test']\n",
    "        scaler = data_splits[t]['scaler']\n",
    "        \n",
    "        # Denormalize the actual prices\n",
    "        y_test_denorm = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Compute the average prices and denormalize\n",
    "        avg_prices = compute_average_prices(X_test)\n",
    "        avg_prices_denorm = scaler.inverse_transform(avg_prices.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Plot for Linear SVR\n",
    "        model_linear = svr_trainer.models[t]['linear']\n",
    "        y_pred_linear = predict_linear(X_test, model_linear)\n",
    "        y_pred_linear_denorm = scaler.inverse_transform(y_pred_linear.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_pred_linear_denorm, label='Predicted (Linear SVR)', color='blue')\n",
    "        plt.plot(y_test_denorm, label='Actual', color='green')\n",
    "        plt.plot(avg_prices_denorm, label=f'Average of previous {t} days', color='orange', linestyle='--')\n",
    "        plt.title(f'Linear SVR (t = {t})')\n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.ylabel('Closing Price (USD)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'linear_svr_t_{t}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot for each RBF SVR model\n",
    "        for gamma in gamma_values:\n",
    "            model_rbf = svr_trainer.models[t][gamma]\n",
    "            y_pred_rbf = predict_rbf(X_test, model_rbf, gamma)\n",
    "            y_pred_rbf_denorm = scaler.inverse_transform(y_pred_rbf.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(y_pred_rbf_denorm, label=f'Predicted (RBF SVR, gamma={gamma})', color='blue')\n",
    "            plt.plot(y_test_denorm, label='Actual', color='green')\n",
    "            plt.plot(avg_prices_denorm, label=f'Average of previous {t} days', color='orange', linestyle='--')\n",
    "            plt.title(f'RBF SVR (t = {t}, gamma = {gamma})')\n",
    "            plt.xlabel('Test Sample Index')\n",
    "            plt.ylabel('Closing Price (USD)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'rbf_svr_t_{t}_gamma_{gamma}.png')\n",
    "            plt.close()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume data_splits is available from preprocessing\n",
    "    svr_trainer = SVR(epsilon=0.1, C=1.0)\n",
    "    svr_trainer.task_1(data_splits)\n",
    "    svr_trainer.task_2(data_splits)\n",
    "    plot_results(data_splits, svr_trainer)\n",
    "    print(\"Plots generated for all SVR models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
