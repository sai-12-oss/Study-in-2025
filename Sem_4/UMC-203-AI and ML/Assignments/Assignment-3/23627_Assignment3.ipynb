{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1rNHXSVtFjd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "with open(\"themes.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    THEMES = json.load(f)\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "maze_size = (20, 20)\n",
        "participant_id =  23627 ## SR.No\n",
        "enable_enemy = False \n",
        "enable_trap_boost = False\n",
        "save_path = f\"{participant_id}_disabled_1.pkl\"\n",
        "\n",
        "# Q-learning parameters\n",
        "###################################\n",
        "#      WRITE YOUR CODE BELOW      #\n",
        "num_actions = 4\n",
        "gamma = 0.9               # between 0 - 1\n",
        "alpha = 0.7               # between 0 - 1\n",
        "epsilon = 0.7             # between 0 - 1\n",
        "epsilon_decay = 0.999      # between 0.1 - 1\n",
        "min_epsilon = 0.1\n",
        "num_episodes = 10000   \n",
        "max_steps = 2000\n",
        "###################################     \n",
        "\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
        "# Reward Configuration - 1\n",
        "# ========== REWARDS ==========\n",
        "###################################\n",
        "#      WRITE YOUR CODE BELOW      #\n",
        "REWARD_GOAL     = 100\n",
        "REWARD_TRAP     = -250\n",
        "REWARD_OBSTACLE = -100\n",
        "REWARD_REVISIT  = -10\n",
        "REWARD_ENEMY    = -100\n",
        "REWARD_STEP     = -0.5\n",
        "REWARD_BOOST    = 5\n",
        "###################################\n",
        "# Reward Configuration - 2\n",
        "# # ========== REWARDS ==========\n",
        "# ###################################\n",
        "# #      WRITE YOUR CODE BELOW      #\n",
        "# REWARD_GOAL     = 100   # Reward for reaching goal.\n",
        "# REWARD_TRAP     = -500  # Trap cell (keep high to enforce avoidance).\n",
        "# REWARD_OBSTACLE = -100  # Obstacle cell (unchanged).\n",
        "# REWARD_REVISIT  = -10   # Revisiting same cell (unchanged).\n",
        "# REWARD_ENEMY    = -100  # Getting caught by enemy (unchanged).\n",
        "# REWARD_STEP     = -1    # Per-step time penalty (increased from -0.5 to -1).\n",
        "# REWARD_BOOST    = 10     # Boost cell (increased from 5 to 10).\n",
        "# ###################################\n",
        "# # ============================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FKJ5KM_rtJ3q"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "class MazeGymEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        initialize the maze_size, participant_id, enable_enemy, enable_trap_boost and max_steps variables\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        # Assigning the configured values to the instance variables\n",
        "        self.maze_size = maze_size\n",
        "        self.participant_id = participant_id\n",
        "        self.enable_enemy = enable_enemy # not used in main tasks \n",
        "        self.enable_trap_boost = enable_trap_boost\n",
        "        self.max_steps = max_steps\n",
        "        ###################################\n",
        "\n",
        "        self.action_space = spaces.Discrete(4) # 4 actions: (0)up, (1)down, (2)left, (3)right\n",
        "        self.observation_space = spaces.Tuple((\n",
        "            spaces.Discrete(maze_size[0]),\n",
        "            spaces.Discrete(maze_size[1])\n",
        "        ))\n",
        "\n",
        "        \"\"\"\n",
        "        generate  self.maze using the _generate_obstacles method\n",
        "        make self.start as the top left cell of the maze and self.goal as the bottom right\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        self.maze = self._generate_obstacles()\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
        "        self.visited = set() # keep track of visited cells within an episode\n",
        "        ###################################\n",
        "\n",
        "        if self.enable_trap_boost:\n",
        "            self.trap_cells, self.boost_cells = self._generate_traps_and_boosts(self.maze)\n",
        "        else:\n",
        "            self.trap_cells, self.boost_cells = ([], [])\n",
        "\n",
        "        self.enemy_cells = []\n",
        "        self.current_step = 0\n",
        "        self.agent_pos = None\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _generate_obstacles(self):\n",
        "        \"\"\"\n",
        "        generates the maze with random obstacles based on the SR.No.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.participant_id)\n",
        "        maze = np.zeros(self.maze_size, dtype=int)\n",
        "        mask = np.ones(self.maze_size, dtype=bool)\n",
        "        safe_cells = [\n",
        "            (0, 0), (0, 1), (1, 0),\n",
        "            (self.maze_size[0]-1, self.maze_size[1]-1), (self.maze_size[0]-2, self.maze_size[1]-1),\n",
        "            (self.maze_size[0]-1, self.maze_size[1]-2)\n",
        "        ]\n",
        "        for row, col in safe_cells:\n",
        "            mask[row, col] = False\n",
        "        maze[mask] = np.random.choice([0, 1], size=mask.sum(), p=[0.9, 0.1])\n",
        "        return maze\n",
        "\n",
        "    def _generate_traps_and_boosts(self, maze):\n",
        "        \"\"\"\n",
        "        generates special cells, traps and boosts. While training our agent,\n",
        "        we want to pass thru more number of boost cells and avoid trap cells \n",
        "        \"\"\"\n",
        "        if not self.enable_trap_boost:\n",
        "            return [], []\n",
        "        exclusions = {self.start, self.goal}\n",
        "        empty_cells = list(zip(*np.where(maze == 0)))\n",
        "        valid_cells = [cell for cell in empty_cells if cell not in exclusions]\n",
        "        num_traps = self.maze_size[0] * 2\n",
        "        num_boosts = self.maze_size[0] * 2\n",
        "        random.seed(self.participant_id)\n",
        "        trap_cells = random.sample(valid_cells, num_traps)\n",
        "        trap_cells_ = trap_cells\n",
        "        remaining_cells = [cell for cell in valid_cells if cell not in trap_cells]\n",
        "        boost_cells = random.sample(remaining_cells, num_boosts)\n",
        "        boost_cells_ = boost_cells\n",
        "        return trap_cells, boost_cells\n",
        "\n",
        "    def move_enemy(self, enemy_pos):\n",
        "        possible_moves = []\n",
        "        for dx, dy in actions:\n",
        "            new_pos = (enemy_pos[0] + dx, enemy_pos[1] + dy)\n",
        "            if (0 <= new_pos[0] < self.maze_size[0] and\n",
        "                0 <= new_pos[1] < self.maze_size[1] and\n",
        "                self.maze[new_pos] != 1):\n",
        "                possible_moves.append(new_pos)\n",
        "        return random.choice(possible_moves) if possible_moves else enemy_pos\n",
        "\n",
        "    def update_enemies(self):\n",
        "        if self.enable_enemy:\n",
        "            self.enemy_cells = [self.move_enemy(enemy) for enemy in self.enemy_cells]\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        empty_cells = list(zip(*np.where(self.maze == 0)))\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
        "\n",
        "        for pos in (self.start, self.goal):\n",
        "            if pos in self.trap_cells:\n",
        "                self.trap_cells.remove(pos)\n",
        "            if pos in self.boost_cells:\n",
        "                self.boost_cells.remove(pos)\n",
        "\n",
        "        if self.enable_enemy:\n",
        "            enemy_candidates = [cell for cell in empty_cells if cell not in {self.start, self.goal}]\n",
        "            num_enemies = max(1, int((self.maze_size[0] * self.maze_size[1]) / 100))\n",
        "            self.enemy_cells = random.sample(enemy_candidates, min(num_enemies, len(enemy_candidates)))\n",
        "        else:\n",
        "            self.enemy_cells = []\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.agent_pos = self.start\n",
        "        self.visited = set()\n",
        "\n",
        "\n",
        "        return self.agent_pos, {}\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        if state == self.goal:\n",
        "            return REWARD_GOAL\n",
        "        elif state in self.trap_cells:\n",
        "            return REWARD_TRAP\n",
        "        elif state in self.boost_cells:\n",
        "            return REWARD_BOOST\n",
        "        elif self.maze[state] == 1:\n",
        "            return REWARD_OBSTACLE\n",
        "        else:\n",
        "            return REWARD_STEP\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        attempted_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "        if (0 <= attempted_state[0] < self.maze_size[0] and\n",
        "            0 <= attempted_state[1] < self.maze_size[1] and\n",
        "            self.maze[attempted_state] != 1):\n",
        "            return attempted_state, False\n",
        "        else:\n",
        "            return state, True\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        next_state, wall_collision = self.take_action(self.agent_pos, action)\n",
        "        if wall_collision:\n",
        "            reward = REWARD_OBSTACLE\n",
        "            next_state = self.agent_pos\n",
        "        else:\n",
        "            if self.enable_enemy:\n",
        "                self.update_enemies()\n",
        "            if self.enable_enemy and next_state in self.enemy_cells:\n",
        "                reward = REWARD_ENEMY\n",
        "                done = True\n",
        "                truncated = True\n",
        "                info = {'terminated_by': 'enemy'}\n",
        "                self.agent_pos = next_state\n",
        "                return self.agent_pos, reward, done, truncated, info\n",
        "            else:\n",
        "                revisit_penalty = REWARD_REVISIT if next_state in self.visited else 0\n",
        "                self.visited.add(next_state)\n",
        "                reward = self.get_reward(next_state) + revisit_penalty\n",
        "        self.agent_pos = next_state\n",
        "\n",
        "        if self.agent_pos == self.goal:\n",
        "            done = True\n",
        "            truncated = False\n",
        "            info = {'completed_by': 'goal'}\n",
        "        elif self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            truncated = True\n",
        "            info = {'terminated_by': 'timeout'}\n",
        "        else:\n",
        "            done = False\n",
        "            truncated = False\n",
        "            info = {\n",
        "                'current_step': self.current_step,\n",
        "                'agent_position': self.agent_pos,\n",
        "                'remaining_steps': self.max_steps - self.current_step\n",
        "            }\n",
        "\n",
        "        return self.agent_pos, reward, done, truncated, info\n",
        "\n",
        "    def render(self, path=None, theme=\"racing\"):\n",
        "        icons = THEMES.get(theme, THEMES[\"racing\"])\n",
        "        clear_output(wait=True)\n",
        "        grid = np.full(self.maze_size, icons[\"empty\"])\n",
        "        grid[self.maze == 1] = icons[\"obstacle\"]\n",
        "        for cell in self.trap_cells:\n",
        "            grid[cell] = icons[\"trap\"]\n",
        "        for cell in self.boost_cells:\n",
        "            grid[cell] = icons[\"boost\"]\n",
        "        grid[self.start] = icons[\"start\"]\n",
        "        grid[self.goal] = icons[\"goal\"]\n",
        "        if path is not None:\n",
        "            for cell in path[1:-1]:\n",
        "                if grid[cell] not in (icons[\"goal\"], icons[\"obstacle\"], icons[\"trap\"], icons[\"boost\"]):\n",
        "                    grid[cell] = icons[\"path\"]\n",
        "        if self.agent_pos is not None:\n",
        "            if grid[self.agent_pos] not in (icons[\"goal\"], icons[\"obstacle\"]):\n",
        "                grid[self.agent_pos] = icons[\"agent\"]\n",
        "        if self.enable_enemy:\n",
        "            for enemy in self.enemy_cells:\n",
        "                grid[enemy] = icons[\"enemy\"]\n",
        "        df = pd.DataFrame(grid)\n",
        "        print(df.to_string(index=False, header=False))\n",
        "\n",
        "    def print_final_message(self, success, interrupted, caught, theme):\n",
        "        msgs = THEMES.get(theme, THEMES[\"racing\"]).get(\"final_messages\", {})\n",
        "        if interrupted:\n",
        "            print(f\"\\n{msgs.get('Interrupted', 'ğŸ›‘ Interrupted.')}\")\n",
        "        elif caught:\n",
        "            print(f\"\\n{msgs.get('Defeat', 'ğŸš“ Caught by enemy.')}\")\n",
        "        elif success:\n",
        "            print(f\"\\n{msgs.get('Triumph', 'ğŸ Success.')}\")\n",
        "        else:\n",
        "            print(f\"\\n{msgs.get('TimeOut', 'â›½ Time Out.')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-En22A4ftMdT"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, maze_size, num_actions, alpha=0.1, gamma=0.99):\n",
        "        \"\"\"\n",
        "        initialize self.num_actions, self.alpha, self.gamma\n",
        "        initialize self.q_table based on number of states and number of actions\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = np.zeros(maze_size + (num_actions,)) # Dimensions: (maze_height, maze_width, num_actions)\n",
        "        ###################################\n",
        "        \n",
        "\n",
        "    def choose_action(self, env, state, epsilon):\n",
        "        \"\"\"\n",
        "        returns an integer between [0,3]\n",
        "\n",
        "        epsilon is a parameter between 0 and 1.\n",
        "        It is the probability with which we choose an exploratory action (random action)\n",
        "        Eg: ---\n",
        "        If epsilon = 0.25, probability of choosing action from q_table = 0.75\n",
        "                           probability of choosing random action = 0.25\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        random_probability = random.uniform(0, 1)\n",
        "        if random_probability < epsilon:\n",
        "            action = random.randint(0, self.num_actions-1)\n",
        "        # Otherwise, exploit: choose the best action based on current Q-values\n",
        "        else:\n",
        "            if not isinstance(state, tuple):\n",
        "                pass\n",
        "            # Find the action index with the highest Q-value for the current state\n",
        "            action = np.argmax(self.q_table[state])\n",
        "        return action\n",
        "        ###################################\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Use the Q-learning update equation to update the Q-Table\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        current_q = self.q_table[state][action]\n",
        "        max_future_q = np.max(self.q_table[next_state])\n",
        "        # Q-learning update rule\n",
        "        # new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_future_q)\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "        # Update the Q-table with the newly calculated Q-value\n",
        "        self.q_table[state][action] = new_q\n",
        "        ###################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ebdQy-8tPw-",
        "outputId": "ac14a863-71c6-47af-f00e-842c43502cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint found. Loading...\n",
            "Resuming from episode 1274 with epsilon 0.2795, best reward 81.50 and best step 38\n",
            "Episode 2000/10000 - Epsilon: 0.1351 - Total Steps: 40 - Episode Reward: 60.50 - Best Reward: 81.50\n",
            "Episode 3000/10000 - Epsilon: 0.1000 - Total Steps: 40 - Episode Reward: 60.50 - Best Reward: 81.50\n",
            "Episode 4000/10000 - Epsilon: 0.1000 - Total Steps: 42 - Episode Reward: 39.50 - Best Reward: 81.50\n",
            "Episode 5000/10000 - Epsilon: 0.1000 - Total Steps: 44 - Episode Reward: -1.50 - Best Reward: 81.50\n",
            "Episode 6000/10000 - Epsilon: 0.1000 - Total Steps: 40 - Episode Reward: 60.50 - Best Reward: 81.50\n",
            "Episode 7000/10000 - Epsilon: 0.1000 - Total Steps: 42 - Episode Reward: -0.50 - Best Reward: 81.50\n",
            "Episode 8000/10000 - Epsilon: 0.1000 - Total Steps: 48 - Episode Reward: 16.50 - Best Reward: 81.50\n",
            "Episode 9000/10000 - Epsilon: 0.1000 - Total Steps: 40 - Episode Reward: 40.50 - Best Reward: 81.50\n",
            "\n",
            "Training completed. Total episodes: 9999\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
        "agent = QLearningAgent(maze_size, num_actions)\n",
        "\n",
        "start_episode = 0\n",
        "best_reward = -np.inf\n",
        "best_q_table = None\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    print(\"Checkpoint found. Loading...\")\n",
        "    with open(save_path, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "        agent.q_table = checkpoint['q_table']\n",
        "        start_episode = checkpoint['episode']\n",
        "        epsilon = checkpoint['epsilon']\n",
        "        best_q_table = checkpoint.get('best_q_table', agent.q_table.copy())\n",
        "        best_reward = checkpoint.get('best_reward', -np.inf)\n",
        "        best_step_counter = checkpoint.get('best_step_counter', 0)\n",
        "    print(f\"Resuming from episode {start_episode} with epsilon {epsilon:.4f}, best reward {best_reward:.2f} and best step {best_step_counter}\")\n",
        "else:\n",
        "    epsilon = 1.0\n",
        "\n",
        "try:\n",
        "    for episode in range(start_episode, num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        visited_states = set()\n",
        "        episode_reward = 0\n",
        "        step_counter = 0\n",
        "\n",
        "        while not done and step_counter < max_steps:\n",
        "            action = agent.choose_action(env, state, epsilon)\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            if next_state in visited_states:\n",
        "                reward += REWARD_REVISIT\n",
        "            visited_states.add(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            step_counter += 1\n",
        "\n",
        "            if state == env.goal:\n",
        "                done = True\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            best_q_table = agent.q_table.copy()\n",
        "            best_step_counter = step_counter\n",
        "            with open(save_path, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'q_table': agent.q_table,\n",
        "                    'episode': episode,\n",
        "                    'epsilon': epsilon,\n",
        "                    'best_q_table': best_q_table,\n",
        "                    'best_reward': best_reward,\n",
        "                    'best_step_counter': best_step_counter\n",
        "                }, f)\n",
        "            print(f\"New best at episode {episode}: {step_counter} steps and Reward {best_reward:.2f}\")\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        if episode % 1000 == 0:\n",
        "            print(f\"Episode {episode}/{num_episodes} - Epsilon: {epsilon:.4f} - Total Steps: {step_counter} - Episode Reward: {episode_reward:.2f} - Best Reward: {best_reward:.2f}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted.\")\n",
        "    print(f\"Interrupted at episode {episode} with epsilon: {epsilon:.4f}, Total Steps: {step_counter}, Episode Reward: {episode_reward:.2f}\")\n",
        "else:\n",
        "    print(f\"\\nTraining completed. Total episodes: {episode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bVahczp5tZTY"
      },
      "outputs": [],
      "source": [
        "def test_agent(env, agent, animated, delay, theme):\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    state = obs\n",
        "    path = [state]\n",
        "    visited_states = set()\n",
        "    total_reward = 0\n",
        "    reward_breakdown = {\n",
        "        'goal':     {'count': 0, 'reward': 0.0},\n",
        "        'trap':     {'count': 0, 'reward': 0.0},\n",
        "        'boost':    {'count': 0, 'reward': 0.0},\n",
        "        'obstacle': {'count': 0, 'reward': 0.0},\n",
        "        'step':     {'count': 0, 'reward': 0.0},\n",
        "        'revisit':  {'count': 0, 'reward': 0.0}\n",
        "    }\n",
        "    caught_by_enemy = False\n",
        "    success = False\n",
        "    interrupted = False\n",
        "\n",
        "    try:\n",
        "        for step in range(env.max_steps):\n",
        "            visited_states.add(state)\n",
        "\n",
        "            action = agent.choose_action(env, state, epsilon=0.0)\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            if info.get('terminated_by') == 'enemy':\n",
        "                caught_by_enemy = True\n",
        "                reward_breakdown.setdefault('enemy', {'count': 0, 'reward': 0.0})\n",
        "                reward_breakdown['enemy']['count'] += 1\n",
        "                reward_breakdown['enemy']['reward'] += reward\n",
        "                total_reward += reward\n",
        "                path.append(next_state)\n",
        "                break\n",
        "            else:\n",
        "                if (next_state == state) and (reward == REWARD_OBSTACLE):\n",
        "                    reward_breakdown['obstacle']['count'] += 1\n",
        "                    reward_breakdown['obstacle']['reward'] += REWARD_OBSTACLE\n",
        "                elif next_state == env.goal:\n",
        "                    reward_breakdown['goal']['count'] += 1\n",
        "                    reward_breakdown['goal']['reward'] += REWARD_GOAL\n",
        "                elif next_state in env.trap_cells:\n",
        "                    reward_breakdown['trap']['count'] += 1\n",
        "                    reward_breakdown['trap']['reward'] += REWARD_TRAP\n",
        "                elif next_state in env.boost_cells:\n",
        "                    reward_breakdown['boost']['count'] += 1\n",
        "                    reward_breakdown['boost']['reward'] += REWARD_BOOST\n",
        "                elif next_state in visited_states:\n",
        "                    reward += REWARD_REVISIT\n",
        "                    reward_breakdown['revisit']['count'] += 1\n",
        "                    reward_breakdown['revisit']['reward'] += REWARD_REVISIT\n",
        "                reward_breakdown['step']['count'] += 1\n",
        "                reward_breakdown['step']['reward'] += REWARD_STEP\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            path.append(state)\n",
        "\n",
        "            if animated:\n",
        "                env.render(path, theme)\n",
        "                print(f\"\\nTotal Allowed Ateps: {env.max_steps}\")\n",
        "                print(f\"Current Reward: {total_reward:.2f}\")\n",
        "                print(\"Live Reward Breakdown:\")\n",
        "                df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
        "                print(df)\n",
        "                time.sleep(delay)\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        interrupted = True\n",
        "\n",
        "    if state == env.goal:\n",
        "        success = True\n",
        "\n",
        "    return path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NkA-fGy4EAS",
        "outputId": "a0658c4e-39a0-4fe6-ac6c-f449d295ea6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint found. Loading best Q-table for testing...\n",
            "Best Q-table loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
        "agent = QLearningAgent(maze_size, num_actions)\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    print(\"Checkpoint found. Loading best Q-table for testing...\")\n",
        "    with open(save_path, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "        best_q_table = checkpoint.get('best_q_table', checkpoint['q_table'])\n",
        "        agent.q_table = best_q_table\n",
        "    print(\"Best Q-table loaded successfully.\")\n",
        "else:\n",
        "    print(\"No checkpoint found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqEw69hYsmtA",
        "outputId": "af0971ad-cfba-496e-de79-fb3e10fc1051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§š ğŸŸ© ğŸŒ² ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ²\n",
            "ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŒ²\n",
            "ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸƒ ğŸƒ ğŸƒ ğŸŒ² ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŒ² ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŒ² ğŸƒ ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŸ© ğŸŒ² ğŸŒ² ğŸŸ© ğŸŸ©\n",
            "ğŸŒ² ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ©\n",
            "ğŸŸ© ğŸŒ² ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸŸ© ğŸƒ ğŸƒ ğŸƒ ğŸƒ ğŸ¡\n",
            "\n",
            "ğŸ¡ Safe in your burrow! You made it home!\n",
            "         Count  Reward\n",
            "Goal         1   100.0\n",
            "Trap         0     0.0\n",
            "Boost        0     0.0\n",
            "Obstacle     0     0.0\n",
            "Step        38   -19.0\n",
            "Revisit      0     0.0\n",
            "Total             81.0\n",
            "\n",
            "Total Allowed Ateps: 2000\n"
          ]
        }
      ],
      "source": [
        "# Run test.\n",
        "\n",
        "theme = random.choice(list(THEMES.keys()))\n",
        "plot_delay = 0.1  # Adjust delay as needed\n",
        "\n",
        "path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy = test_agent(env, agent, animated=True, delay=plot_delay, theme=theme)\n",
        "\n",
        "env.render(path, theme=theme)\n",
        "env.print_final_message(success, interrupted, caught=caught_by_enemy, theme=theme)\n",
        "\n",
        "reward_df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
        "reward_df.index = reward_df.index.str.title()\n",
        "reward_df = reward_df.rename(columns={'count': 'Count', 'reward': 'Reward'})\n",
        "total_row = pd.DataFrame({\n",
        "    'Count': [''],\n",
        "    'Reward': [reward_df['Reward'].sum()]\n",
        "}, index=['Total'])\n",
        "reward_df = pd.concat([reward_df, total_row])\n",
        "\n",
        "print(reward_df)\n",
        "print(f\"\\nTotal Allowed Ateps: {max_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual Q-Value Calculation Data (First 5 Steps):\n",
            "Step 1:\n",
            "  Current State: (0, 0)\n",
            "  Action: 1 (Down)\n",
            "  Reward: -0.5\n",
            "  Next State: (1, 0)\n",
            "  Q(s, a): -26.674739147347736\n",
            "  max Q(s', a'): -23.018050660073072\n",
            "Step 2:\n",
            "  Current State: (1, 0)\n",
            "  Action: 1 (Down)\n",
            "  Reward: -0.5\n",
            "  Next State: (2, 0)\n",
            "  Q(s, a): -23.018050660073072\n",
            "  max Q(s', a'): -17.5190335475612\n",
            "Step 3:\n",
            "  Current State: (2, 0)\n",
            "  Action: 1 (Down)\n",
            "  Reward: -0.5\n",
            "  Next State: (3, 0)\n",
            "  Q(s, a): -17.5190335475612\n",
            "  max Q(s', a'): -11.94973862499872\n",
            "Step 4:\n",
            "  Current State: (3, 0)\n",
            "  Action: 1 (Down)\n",
            "  Reward: -0.5\n",
            "  Next State: (4, 0)\n",
            "  Q(s, a): -11.94973862499872\n",
            "  max Q(s', a'): -8.048991963711163\n",
            "Step 5:\n",
            "  Current State: (4, 0)\n",
            "  Action: 3 (Right)\n",
            "  Reward: -0.5\n",
            "  Next State: (4, 1)\n",
            "  Q(s, a): -8.048991963711163\n",
            "  max Q(s', a'): -4.539418701146321\n",
            "Learning Rate (alpha): 0.7\n",
            "Discount Factor (gamma): 0.9\n"
          ]
        }
      ],
      "source": [
        "# Simulate first 5 steps for manual Q-value calculation\n",
        "import numpy as np\n",
        "\n",
        "# Reset the environment\n",
        "state, _ = env.reset()  # Starts at (0, 0)\n",
        "agent.alpha = 0.7  # From your training config\n",
        "agent.gamma = 0.9  # From your training config\n",
        "\n",
        "print(\"Manual Q-Value Calculation Data (First 5 Steps):\")\n",
        "for step in range(5):\n",
        "    # Choose action greedily (epsilon = 0)\n",
        "    action = np.argmax(agent.q_table[state])\n",
        "    \n",
        "    # Get current Q-value\n",
        "    q_current = agent.q_table[state][action]\n",
        "    \n",
        "    # Take the step\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    \n",
        "    # Get max Q-value for next state\n",
        "    max_q_next = np.max(agent.q_table[next_state])\n",
        "    \n",
        "    # Print values for this step\n",
        "    print(f\"Step {step + 1}:\")\n",
        "    print(f\"  Current State: {state}\")\n",
        "    print(f\"  Action: {action} ({['Up', 'Down', 'Left', 'Right'][action]})\")\n",
        "    print(f\"  Reward: {reward}\")\n",
        "    print(f\"  Next State: {next_state}\")\n",
        "    print(f\"  Q(s, a): {q_current}\")\n",
        "    print(f\"  max Q(s', a'): {max_q_next}\")\n",
        "    \n",
        "    # Update state for next step\n",
        "    state = next_state\n",
        "    \n",
        "    # Stop if goal is reached early (unlikely in 5 steps)\n",
        "    if done:\n",
        "        print(\"Goal reached early!\")\n",
        "        break\n",
        "\n",
        "print(f\"Learning Rate (alpha): {agent.alpha}\")\n",
        "print(f\"Discount Factor (gamma): {agent.gamma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename and Submit this file as **SRNO(5digit)_Assignment3.ipynb**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
