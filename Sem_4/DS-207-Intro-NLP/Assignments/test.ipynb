{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/saisandeshk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 319\u001b[0m\n\u001b[1;32m    317\u001b[0m nb \u001b[38;5;241m=\u001b[39m NaiveBayesClassifier()\n\u001b[1;32m    318\u001b[0m X_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeautiful day\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# vocab = nb.create_vocabulary(texts)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# array = nb.extract_features(texts, vocab)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# print(vocab)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# print(array)\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# class_probs = nb.calculate_class_probabilities([0, 0, 1, 1, 0, 1])\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# print(class_probs)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 311\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.predict\u001b[0;34m(self, X_text)\u001b[0m\n\u001b[1;32m    309\u001b[0m         class_scores[c] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(class_probs[c])\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m w_idx \u001b[38;5;129;01min\u001b[39;00m word_indices:\n\u001b[0;32m--> 311\u001b[0m             class_scores[c] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(word_probs[c][\u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw_idx\u001b[49m\u001b[43m]\u001b[49m])\n\u001b[1;32m    312\u001b[0m     predicted_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmax\u001b[39m(class_scores, key\u001b[38;5;241m=\u001b[39mclass_scores\u001b[38;5;241m.\u001b[39mget))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_labels\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, min_freq=2):\n",
    "        \"\"\"\n",
    "        Initialize the Naive Bayes classifier.\n",
    "\n",
    "        Args:\n",
    "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
    "                           Words appearing less than min_freq times will be treated as UNK token.\n",
    "                           Default: 1 (include all words)\n",
    "\n",
    "        Attributes:\n",
    "            class_probs (dict): P(class) for each class\n",
    "                Example: {0: 0.5, 1: 0.5}\n",
    "\n",
    "            word_probs (dict): P(word|class) for each word and class\n",
    "                Example: {\n",
    "                    0: {'hello': 0.5, 'world': 0.4, '<UNK>': 0.1},\n",
    "                    1: {'hello': 0.3, 'world': 0.5, '<UNK>': 0.2}\n",
    "                }\n",
    "\n",
    "            vocabulary (dict): Word to index mapping, including special UNK token\n",
    "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
    "\n",
    "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
    "                Example: If min_freq=2, words must appear at least twice to be included\n",
    "\n",
    "        Note:\n",
    "            - Words appearing less than min_freq times in training data will be mapped to <UNK>\n",
    "            - <UNK> token is automatically added to vocabulary as first token (index 0)\n",
    "            - Probability for <UNK> is calculated during training based on rare words\n",
    "        \"\"\"\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None\n",
    "        self.vocabulary = None\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text by converting to lowercase, removing non-word characters,\n",
    "        and filtering out common stop words.\n",
    "\n",
    "        Args:\n",
    "            text (str): Raw input text\n",
    "                Example: \"Hello, World! How are you doing today?\"\n",
    "\n",
    "        Returns:\n",
    "            list: List of cleaned, tokenized, and filtered words with stop words removed\n",
    "                Example: ['hello', 'world', 'doing', 'today']\n",
    "\n",
    "        Note:\n",
    "            - Converts all text to lowercase\n",
    "            - Removes punctuation and special characters\n",
    "            - Splits text into individual tokens\n",
    "            - Removes common English stop words (e.g., 'a', 'an', 'the', 'is', 'are', 'how')\n",
    "            - Stop words are removed using NLTK's English stop words list\n",
    "        \"\"\"\n",
    "        # Import stop words from NLTK\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Extract word characters only and split into tokens\n",
    "        tokens = re.findall(r'\\w+', text)\n",
    "\n",
    "        # Remove stop words\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        return filtered_tokens\n",
    "\n",
    "    def create_vocabulary(self, texts):\n",
    "        \"\"\"\n",
    "        Create vocabulary from training texts by mapping unique words to indices,\n",
    "        considering minimum frequency threshold and adding UNK token.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of text documents\n",
    "                Example: [\n",
    "                    \"Hello world hello\",\n",
    "                    \"Hello there\",\n",
    "                    \"World is beautiful\"\n",
    "                ]\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of words to unique indices, including UNK token\n",
    "                Example (with min_freq=2): {\n",
    "                    '<UNK>': 0,    # Special token for rare/unseen words\n",
    "                    'hello': 1,    # Frequency=3, included in vocab\n",
    "                    'world': 2,    # Frequency=2, included in vocab\n",
    "                    # 'there' and 'beautiful' not included (frequency=1 < min_freq=2)\n",
    "                }\n",
    "\n",
    "        Note:\n",
    "            - Always includes <UNK> token at index 0\n",
    "            - Only includes words that appear >= min_freq times\n",
    "            - Word frequency is counted across all documents\n",
    "            - Uses preprocess_text function for preprocessing\n",
    "            - Words below frequency threshold will be mapped to UNK during feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : naive_bayes.create_vocabulary\n",
    "        class_vocab = {}\n",
    "        min_freq = self.min_freq\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            for word in words:\n",
    "                if word in class_vocab:\n",
    "                    class_vocab[word] += 1\n",
    "                else:\n",
    "                    class_vocab[word] = 1\n",
    "        # print(class_vocab)\n",
    "        vocabulary = {'<UNK>': 0}\n",
    "        index = 1\n",
    "        for word in class_vocab:\n",
    "            if class_vocab[word] >= min_freq:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "        return vocabulary\n",
    "    def extract_features(self, texts, vocabulary):\n",
    "        \"\"\"\n",
    "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
    "        where each element represents the count of word occurrences (not binary presence/absence).\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of text documents\n",
    "                Example: [\"hello world hello\", \"world is beautiful\"]\n",
    "            vocabulary (dict): Word to index mapping with UNK token\n",
    "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
    "\n",
    "        Returns:\n",
    "            np.array: Feature matrix where each row is a document vector\n",
    "                Example: For the above input with min_freq=2:\n",
    "                array([\n",
    "                    [0, 2, 1],  # First doc: 0 UNKs, 2 'hello's, 1 'world'\n",
    "                    [2, 0, 1]   # Second doc: 2 UNKs (one each for 'is' and 'beautiful'), 0 'hello's, 1 'world',\n",
    "                ])\n",
    "\n",
    "        Note:\n",
    "            - Each row represents one document\n",
    "            - Each column represents the count of a specific word\n",
    "            - First column is always UNK token count\n",
    "            - Words not in vocabulary are counted as UNK\n",
    "            - Shape of output: (n_documents, len(vocabulary))\n",
    "            - Uses preprocess_text function for preprocessing\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : naive_bayes.extract_features\n",
    "        # class_vocab = {}\n",
    "        # min_freq = self.min_freq\n",
    "        # for text in texts:\n",
    "        #     words = self.preprocess_text(text)\n",
    "        #     for word in words:\n",
    "        #         if word in class_vocab:\n",
    "        #             class_vocab[word] += 1\n",
    "        #         else:\n",
    "        #             class_vocab[word] = 1\n",
    "        array = np.array([[0 for i in range(len(vocabulary))] for j in range(len(texts))])\n",
    "        for i in range(len(texts)):\n",
    "            words = self.preprocess_text(texts[i])\n",
    "            for word in words:\n",
    "                if word not in vocabulary :\n",
    "                    array[i][0] += 1\n",
    "                else:\n",
    "                    array[i][vocabulary[word]] += 1\n",
    "        return array \n",
    "    def calculate_class_probabilities(self, y):\n",
    "        \"\"\"\n",
    "        Estimate probability P(class) for each class from training labels.\n",
    "\n",
    "        Args:\n",
    "            y (list): List of class labels\n",
    "                Example: [0, 0, 1, 1, 0, 1]\n",
    "\n",
    "        Returns:\n",
    "            dict: Estimated probability for each class\n",
    "                Example: {\n",
    "                    0: 0.5,    # 3 out of 6 samples are class 0\n",
    "                    1: 0.5     # 3 out of 6 samples are class 1\n",
    "                }\n",
    "\n",
    "        Note:\n",
    "            - Probabilities sum to 1 across all classes\n",
    "            - Handles any number of unique classes\n",
    "        \"\"\"\n",
    "        # BEGIN CODE : naive_bayes.extract_features\n",
    "        array = y\n",
    "        class_probs = {}\n",
    "        for i in range(len(array)):\n",
    "            if array[i] in class_probs:\n",
    "                class_probs[array[i]] += 1\n",
    "            else:\n",
    "                class_probs[array[i]] = 1\n",
    "        for key in class_probs:\n",
    "            class_probs[key] /= len(array)\n",
    "        return class_probs\n",
    "    def calculate_word_probabilities(self, X, y, vocabulary, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Calculate conditional probability P(word|class) for each word and class,\n",
    "        including probability for UNK token.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Document-term matrix (with UNK counts in first column)\n",
    "                Example: array([\n",
    "                    [0, 2, 1],  # Document 1: 0 UNKs, 2 of word 1, 1 of word 2\n",
    "                    [1, 0, 1],  # Document 2: 1 UNK, 0 of word 1, 1 of word 2\n",
    "                ])\n",
    "            y (list): Class labels\n",
    "                Example: [0, 1]\n",
    "            vocabulary (dict): Word to index mapping with UNK token\n",
    "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
    "            alpha (float): Laplace smoothing parameter, default=1.0\n",
    "\n",
    "        Returns:\n",
    "            dict: Nested dict with P(word|class) for each word and class\n",
    "                Example: {\n",
    "                    0: {\n",
    "                        '<UNK>': 0.167,    # P(word=UNK|class=0)\n",
    "                        'hello': 0.5,     # P(word='hello'|class=0)\n",
    "                        'world': 0.333      # P(word='world'|class=0)\n",
    "                    },\n",
    "                    1: {\n",
    "                        '<UNK>': 0.4,    # P(word=UNK|class=1)\n",
    "                        'hello': 0.2,     # P(word='hello'|class=1)\n",
    "                        'world': 0.4      # P(word='world'|class=1)\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        Note:\n",
    "            - Uses Laplace smoothing to handle unseen words\n",
    "            - UNK token probability is learned from training data\n",
    "            - Formula: P(word|class) = (count(word,class) + α) / (total_words_in_class + α|V|)\n",
    "            - |V| is vocabulary size (including UNK token)\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : naive_bayes.calculate_word_probabilities\n",
    "        V = len(vocabulary)\n",
    "        classes = set(y)  # unique class labels\n",
    "        word_probs = {c: {} for c in classes}\n",
    "\n",
    "        for c in classes:\n",
    "            # get indices of documents that belong to class c\n",
    "            class_indices = [idx for idx, label in enumerate(y) if label == c]\n",
    "\n",
    "            # sum counts for all words in these documents\n",
    "            total_words = 0\n",
    "            word_counts = [0] * V\n",
    "            for idx in class_indices:\n",
    "                for w in range(V):\n",
    "                    word_counts[w] += X[idx][w]\n",
    "                total_words += sum(X[idx])\n",
    "\n",
    "            # compute probabilities with Laplace smoothing\n",
    "            for word, w_idx in vocabulary.items():\n",
    "                word_probs[c][word] = (word_counts[w_idx] + alpha) / (total_words + alpha * V)\n",
    "\n",
    "        return word_probs\n",
    "    def predict(self, X_text):\n",
    "        \"\"\"\n",
    "        Predict classes for new documents using Naive Bayes algorithm,\n",
    "        handling unknown words using UNK token.\n",
    "\n",
    "        Args:\n",
    "            X_text (list): List of text documents\n",
    "                Example: [\n",
    "                    \"hello world\",\n",
    "                    \"beautiful day\"  # 'day' is unknown, treated as UNK\n",
    "                ]\n",
    "\n",
    "        Returns:\n",
    "            list: Predicted class labels\n",
    "                Example: [0, 1]\n",
    "\n",
    "        Theory:\n",
    "            The standard Naive Bayes formula for text classification is:\n",
    "            P(class|document) ∝ P(class) * ∏ P(word|class)\n",
    "\n",
    "            For unknown words not in vocabulary:\n",
    "            - They are mapped to UNK token\n",
    "            - P(UNK|class) is used in probability calculation\n",
    "\n",
    "            We use log space to prevent numerical underflow:\n",
    "            log(P(class|document)) ∝ log(P(class)) + Σ log(P(word|class))\n",
    "\n",
    "        Implementation:\n",
    "            For each document:\n",
    "            1. Preprocess and tokenize text\n",
    "            2. Replace unknown words with UNK token\n",
    "            3. Calculate log probabilities using appropriate word or UNK probabilities\n",
    "            4. Return class with highest log probability score\n",
    "\n",
    "        Note:\n",
    "            - Uses preprocess_text function for preprocessing\n",
    "            - Words not in vocabulary are treated as UNK token\n",
    "            - UNK probability is used for out-of-vocabulary words\n",
    "        \"\"\"\n",
    "        # BEGIN CODE : naive_bayes.predict\n",
    "        predicted_labels = []\n",
    "        y = [0 for i in range(len(X_text))]\n",
    "        vocabulary = self.create_vocabulary(X_text)\n",
    "        X = self.extract_features(X_text, vocabulary)\n",
    "        class_probs = self.calculate_class_probabilities(y)\n",
    "        word_probs = self.calculate_word_probabilities(X, y, vocabulary)\n",
    "        for text in X_text:\n",
    "            words = self.preprocess_text(text)\n",
    "            word_indices = [vocabulary[word] if word in vocabulary else 0 for word in words]\n",
    "            class_scores = {}\n",
    "            for c in class_probs:\n",
    "                class_scores[c] = np.log(class_probs[c])\n",
    "                for w_idx in word_indices:\n",
    "                    class_scores[c] += np.log(word_probs[c][vocabulary[w_idx]])\n",
    "            predicted_labels.append(max(class_scores, key=class_scores.get))\n",
    "        return predicted_labels\n",
    "        # END CODE\n",
    "        # END CODE\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "X_text = [\"hello world\", \"beautiful day\"]\n",
    "print(nb.predict(X_text))\n",
    "# vocab = nb.create_vocabulary(texts)\n",
    "# array = nb.extract_features(texts, vocab)\n",
    "# print(vocab)\n",
    "# print(array)\n",
    "# class_probs = nb.calculate_class_probabilities([0, 0, 1, 1, 0, 1])\n",
    "# print(class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoyo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
