{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ39Vxvm7S0a"
      },
      "source": [
        "# Assignment 3: Sequence-to-Sequence Modeling (TA: Shivashish Naithani)\n",
        "\n",
        "In this assignment, you will perform the task of translating Indian Names to English, a sequence-to-sequence modeling task, using character-level conditional language models.\n",
        "\n",
        "As before, please make a copy of this notebook (locally or on Colab). Ensure you adhere to the guidelines and submission instructions (mentioned below) for attempting and submitting the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uysmp8X-7S0i"
      },
      "source": [
        "### Guidelines for Attempting the Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_vklna37S0j"
      },
      "source": [
        "1. Write your logic in the cells which have the comment `# ADD YOUR CODE HERE`, between the `# BEGIN CODE` and `# END CODE` comments. These cells are also demarcated by the special start (`## ==== BEGIN EVALUATION PORTION`) and end (`## ==== END EVALUATION PORTION`) comments. Do **NOT** remove any of these comments from the designated cells, otherwise your assigment may not be evaluated correctly.\n",
        "\n",
        "2. All imports that should be necessary are already provided as part of the notebook. Should you require additional imports, add them in the cells to be graded, but outside the `# BEGIN CODE` and `# END CODE` block. For example, if you need to import a package called `mypackage`, add it as follows in a graded cell:\n",
        "\n",
        "``` python\n",
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "import mypackage # <===\n",
        "\n",
        "def function_to_be_implemented(*args, **kwargs):\n",
        "\n",
        "    ...\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "    # BEGIN CODE\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "    ...\n",
        "\n",
        "## ==== END EVALUATION PORTION\n",
        "\n",
        "```\n",
        "4. Do not modify anything in the cells which start with `# Please do not change anything in the following cell`.\n",
        "\n",
        "5. Only write your code in the cells designated for auto-evaluation. If you encounter any errors in the supporting cells during execution, contact the respective TAs.\n",
        "\n",
        "6. **Important**: Use of AI-assistive technologies such as ChatGPT or GitHub CoPilot is not permitted for this assignment. Ensure that all attempts are solely your own. Not following this rule can incur heavy penalty, including getting NO GRADE for this assignment, which will affect your grade significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK3moP9d7S0l"
      },
      "source": [
        "### Submission Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Y-T9hT7S0m"
      },
      "source": [
        "1. Ensure your code follows all guidelines mentioned above before submission.\n",
        "\n",
        "2. Try to avoid any unnecessary print statements across the code. We will evaluate specific output lines which begin with the phrase `EVALUATION`. Ensure you do not modify these print statements howsoever, as they are used for auto-evaluation.\n",
        "\n",
        "3. When you have completely attempted the assignment, export the current notebook as a `.py` file, with the following name: `SAPName_SRNo_assignment3.py`, where `SAPName` would be your name as per SAP record, and `SRNo` will be the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use `Twyla_Linda_15329_assignment3.py`.\n",
        "\n",
        "4. Once you have executed the code, certain additional files will be created. Once you are done executing all associated cells, ensure the folder structure looks as follows:\n",
        "\n",
        "``` python\n",
        "└─── SAPName_SRNo\n",
        "     ├─── SAPName_SRNo_assignment3.py\n",
        "     ├─── src-tokenizer\n",
        "     │    └─── tokenizer.pkl\n",
        "     ├─── tgt-tokenizer\n",
        "     │    └─── tokenizer.pkl\n",
        "     ├─── rnn.enc-dec\n",
        "     │    ├─── model.pt\n",
        "     │    ├─── loss.json\n",
        "     │    ├─── outputs.csv\n",
        "     │    └─── metadata.json\n",
        "     └─── rnn.enc-dec.attn\n",
        "          ├─── model.pt\n",
        "          ├─── loss.json\n",
        "          ├─── outputs.csv\n",
        "          └─── metadata.json\n",
        "```\n",
        "\n",
        "5. Once you have validated the folder structure as above, add the exported `.py` file to the folder and submit the folder as a ZIP archive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWNGKx3C7S0n"
      },
      "source": [
        "In the cell below, replace `SAPName` with your name as per SAP record, and `SRNo` with the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use:\n",
        "\n",
        "```python\n",
        "STUDENT_SAP_NAME  = \"Twyla Linda\"\n",
        "STUDENT_SR_NUMBER = \"15329\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "juYQICX47S0p"
      },
      "outputs": [],
      "source": [
        "STUDENT_SAP_NAME  = \"Nitin Vetcha\"\n",
        "STUDENT_SR_NUMBER = \"22207\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JmPFqkT7S0r"
      },
      "source": [
        "**Important Notes**:\n",
        "\n",
        "- Some of the tasks in this assignment are compute intensive, and are better performed on an accelerator device (GPU, etc.). Unless you have one locally, prefer using a GPU instance on Colab for execution.\n",
        "- Due to resource restrictions on Colab, training some models may not finish in time. In such a case, ensure you store checkpoints to a persistent directory so that you may resume training once your resource limits are restored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puFcXNQD2ItL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbZjfBWF7S0t"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQVZHQIJ7S0t"
      },
      "source": [
        "Through the last assignment, you have seen that neural language models are able to successfully capture patterns across Indian names. In this assignment, you will extend upon that idea to learn conditional language models for the task of transliteration: converting Indian names in the English alphabet to Hindi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElSrTdBy7S0u"
      },
      "source": [
        "### Marks Distribution\n",
        "\n",
        "- Agnostic Task-Specific Training: 5 marks\n",
        "- Seq-2-Seq via RNN: 40 marks\n",
        "- Seq-2-Seq via RNN with Attention: 35 marks\n",
        "- Evaluation\n",
        "- Decoding Strategy: 20 marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeose21P7S0v"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1ApHWVd7S0v"
      },
      "source": [
        "The following cells perform the basic setup such as importing the necessary packages. You will not require any additional libraries, so importing any additional libraries is not allowed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m1rW15no7S0w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tqdm in /home/arnavbhatt/.local/lib/python3.10/site-packages (4.66.6)\n",
            "Requirement already satisfied: nltk in /home/arnavbhatt/.local/lib/python3.10/site-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /home/arnavbhatt/.local/lib/python3.10/site-packages (3.9.2)\n",
            "Requirement already satisfied: numpy in /home/arnavbhatt/.local/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: pandas in /home/arnavbhatt/.local/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: joblib in /home/arnavbhatt/.local/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/arnavbhatt/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Installs packages, if using locally. Feel free to add other missing packages as required.\n",
        "\n",
        "%pip install tqdm nltk matplotlib numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NijuBljo7S0w"
      },
      "outputs": [],
      "source": [
        "# Built-in imports, no installations required.\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import pickle\n",
        "import subprocess\n",
        "import collections\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z2liLT4Y7S0x"
      },
      "outputs": [],
      "source": [
        "# 3rd-party package imports, may require installation if not on a platform such as Colab.\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "from nltk.translate import bleu_score\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m3XUKaQ37S0x"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "# Find and load fonts that can display Hindi characters, for Matplotlib\n",
        "result = subprocess.run([ 'fc-list', ':lang=hi', 'family' ], capture_output=True)\n",
        "found_hindi_fonts = result.stdout.decode('utf-8').strip().split('\\n')\n",
        "\n",
        "matplotlib.rcParams['font.sans-serif'] = [\n",
        "    'Source Han Sans TW', 'sans-serif', 'Arial Unicode MS',\n",
        "    *found_hindi_fonts\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x2X96lFz7S0y"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "DIRECTORY_NAME = f\"{STUDENT_SAP_NAME.replace(' ', '_')}_{STUDENT_SR_NUMBER}\"\n",
        "\n",
        "os.makedirs(DIRECTORY_NAME, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bw94_afp7S0y"
      },
      "outputs": [],
      "source": [
        "def sync_vram():\n",
        "    \"\"\" Synchronizes the VRAM across the GPUs, reclaiming unused memory. \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEg1ZOP17S0y"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_lcFfUf7S0y"
      },
      "source": [
        "We'll load the data for the task, which comprises of a parallel corpus of Indian Names and their Hindi equivalents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gFbB8qWx7S0z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-25 18:23:44--  https://docs.google.com/spreadsheets/d/e/2PACX-1vQOYe_Oy8eMzVFYq6hBSyPLslqA1PeMeK8S5nPs2-viuCNzx0i3Fl_ptFmD0YD3kTA_olYdOIx7iPOh/pub?gid=1482240395&single=true&output=csv\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.205.14, 2404:6800:4007:800::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.205.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-08-6c-sheets.googleusercontent.com/pub/54bogvaave6cua4cdnls17ksc4/9rre1cgv7jc8s6gi952fv1k2p4/1742907225000/118252164104506406045/*/e@2PACX-1vQOYe_Oy8eMzVFYq6hBSyPLslqA1PeMeK8S5nPs2-viuCNzx0i3Fl_ptFmD0YD3kTA_olYdOIx7iPOh?gid=1482240395&single=true&output=csv [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-03-25 18:23:45--  https://doc-08-6c-sheets.googleusercontent.com/pub/54bogvaave6cua4cdnls17ksc4/9rre1cgv7jc8s6gi952fv1k2p4/1742907225000/118252164104506406045/*/e@2PACX-1vQOYe_Oy8eMzVFYq6hBSyPLslqA1PeMeK8S5nPs2-viuCNzx0i3Fl_ptFmD0YD3kTA_olYdOIx7iPOh?gid=1482240395&single=true&output=csv\n",
            "Resolving doc-08-6c-sheets.googleusercontent.com (doc-08-6c-sheets.googleusercontent.com)... 142.250.183.1, 2404:6800:4007:826::2001\n",
            "Connecting to doc-08-6c-sheets.googleusercontent.com (doc-08-6c-sheets.googleusercontent.com)|142.250.183.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘data.train.csv’\n",
            "\n",
            "data.train.csv          [ <=>                ] 113.10K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-25 18:23:46 (1.56 MB/s) - ‘data.train.csv’ saved [115815]\n",
            "\n",
            "--2025-03-25 18:23:46--  https://docs.google.com/spreadsheets/d/e/2PACX-1vTKa_jeysYhx869fmTb7VUchlSiChUq0vqotWRGMmnTXWZ8H2PkF8s6hRr2vdo6v54JJx8CEuVo8MZ3/pub?gid=1579594041&single=true&output=csv\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.205.14, 2404:6800:4007:800::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.205.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-08-6c-sheets.googleusercontent.com/pub/54bogvaave6cua4cdnls17ksc4/2du6d13oriksoalt53dc2gpcco/1742907225000/118252164104506406045/*/e@2PACX-1vTKa_jeysYhx869fmTb7VUchlSiChUq0vqotWRGMmnTXWZ8H2PkF8s6hRr2vdo6v54JJx8CEuVo8MZ3?gid=1579594041&single=true&output=csv [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-03-25 18:23:47--  https://doc-08-6c-sheets.googleusercontent.com/pub/54bogvaave6cua4cdnls17ksc4/2du6d13oriksoalt53dc2gpcco/1742907225000/118252164104506406045/*/e@2PACX-1vTKa_jeysYhx869fmTb7VUchlSiChUq0vqotWRGMmnTXWZ8H2PkF8s6hRr2vdo6v54JJx8CEuVo8MZ3?gid=1579594041&single=true&output=csv\n",
            "Resolving doc-08-6c-sheets.googleusercontent.com (doc-08-6c-sheets.googleusercontent.com)... 142.250.183.1, 2404:6800:4007:826::2001\n",
            "Connecting to doc-08-6c-sheets.googleusercontent.com (doc-08-6c-sheets.googleusercontent.com)|142.250.183.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘data.valid.csv’\n",
            "\n",
            "data.valid.csv          [ <=>                ]  10.10K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-03-25 18:23:47 (1.28 MB/s) - ‘data.valid.csv’ saved [10342]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make sure your code is not dependent on any of the file names as below.\n",
        "\n",
        "# Download the training and validation datasets\n",
        "!wget -O data.train.csv \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQOYe_Oy8eMzVFYq6hBSyPLslqA1PeMeK8S5nPs2-viuCNzx0i3Fl_ptFmD0YD3kTA_olYdOIx7iPOh/pub?gid=1482240395&single=true&output=csv\"\n",
        "!wget -O data.valid.csv \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTKa_jeysYhx869fmTb7VUchlSiChUq0vqotWRGMmnTXWZ8H2PkF8s6hRr2vdo6v54JJx8CEuVo8MZ3/pub?gid=1579594041&single=true&output=csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1L8ZDlFm7S0z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of training data: 4484\n",
            "Length of validation data: 400\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "def read_dataframe(ds_type):\n",
        "    \"\"\" Loads a dataframe based on the given partition type.\n",
        "\n",
        "    Args:\n",
        "        ds_type (str): Dataset type: train (train) or validation (valid)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Pandas Dataframe for the specified partition.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(f\"data.{ds_type}.csv\", header=0)\n",
        "    df = df[~df.isna()]\n",
        "    df['Name'] = df['Name'].astype(str)\n",
        "    df['Translation'] = df['Translation'].astype(str)\n",
        "    return df\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_data      = read_dataframe(\"train\")\n",
        "validation_data = read_dataframe(\"valid\")\n",
        "\n",
        "print(f\"Length of training data: {len(train_data)}\\nLength of validation data: {len(validation_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8sRvHo7S00"
      },
      "source": [
        "Here are some examples from the training dataset. Note that the dataset may be noisy so some examples may not be perfect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kZH2EVyE7S00"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Translation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>वाशु</td>\n",
              "      <td>vashu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>शबीना</td>\n",
              "      <td>shabina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>मालती</td>\n",
              "      <td>malti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3507</th>\n",
              "      <td>अरबाज</td>\n",
              "      <td>arbaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2878</th>\n",
              "      <td>विदित</td>\n",
              "      <td>vidit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Name Translation\n",
              "353    वाशु       vashu\n",
              "89    शबीना     shabina\n",
              "320   मालती       malti\n",
              "3507  अरबाज       arbaj\n",
              "2878  विदित       vidit"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0B5XhUx7S01"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizKxo-j7S01"
      },
      "source": [
        "An implementation of tokenization is already in place. Do not modify it, as it can affect your evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8Y3ri4PD7S02"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\" Represents the tokenizer for text data.\n",
        "        Provides methods to encode and decode strings (as instance or as a batch). \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Initializes a new tokenizer.\n",
        "\n",
        "            Any variables required in intermediate operations are declared here.\n",
        "            You will also need to define things like special tokens and other things here.\n",
        "\n",
        "            All variables declared in this function will be serialized\n",
        "                and deserialized when loading and saving the Tokenizer.\n",
        "            \"\"\"\n",
        "\n",
        "        self.special_tokens = { '[BOS]': 1, '[EOS]': 2, '[PAD]': 0 }\n",
        "        self.vocab = { bytes([ i ]): i+len(self.special_tokens) for i in range(256)  }\n",
        "        self.merge_rules = {  }\n",
        "        self.inv_vocab = { _id: token for token, _id in self.vocab.items() }\n",
        "        self.inv_vocab.update({ _id: token.encode() for token, _id in self.special_tokens.items() })\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\" Loads a pre-trained tokenizer from the given directory.\n",
        "           This directory will have a tokenizer.pkl file that contains all the tokenizer variables.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to load the tokenizer from.\n",
        "        \"\"\"\n",
        "        tokenizer_file = os.path.join(path, \"tokenizer.pkl\")\n",
        "\n",
        "        if not os.path.exists(path) or not os.path.exists(os.path.join(path, \"tokenizer.pkl\")):\n",
        "            raise ValueError(cls.load.__name__ + \": No tokenizer found at the specified directory\")\n",
        "\n",
        "        with open(tokenizer_file, \"rb\") as ifile:\n",
        "            return pickle.load(ifile)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\" Saves a trained tokenizer to a given directory, inside a tokenizer.pkl file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Directory to save the tokenizer in.\n",
        "        \"\"\"\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        with open(os.path.join(path, \"tokenizer.pkl\"), 'wb') as ofile:\n",
        "            pickle.dump(self, ofile)\n",
        "\n",
        "    def train(self, data, vocab_size):\n",
        "        \"\"\" Trains a tokenizer to learn meaningful representations from input data.\n",
        "            In the end, learns a vocabulary of a fixed size over the given data.\n",
        "            Special tokens, if any, must not be counted towards this vocabulary.\n",
        "\n",
        "        Args:\n",
        "            data (list[str]): List of input strings from a text corpus.\n",
        "            vocab_size (int): Final desired size of the vocab to be learnt.\n",
        "        \"\"\"\n",
        "\n",
        "        self.vocab = { bytes([ i ]): i+len(self.special_tokens) for i in range(256)  }\n",
        "        self.vocab.update({ token.encode('utf-8'): _id for token, _id in self.special_tokens.items() })\n",
        "\n",
        "        self.merge_rules = {  }\n",
        "        self.inv_vocab   = { _id: token for token, _id in self.vocab.items() }\n",
        "\n",
        "        data = [ [ i+len(self.special_tokens) for i in instance.encode('utf-8') ] for instance in data ]\n",
        "\n",
        "        while len(self.vocab) < len(self.special_tokens) + vocab_size:\n",
        "            # Compute stats\n",
        "            counts = collections.defaultdict(int)\n",
        "            for tok_str in data:\n",
        "                for tok, next_tok in zip(tok_str, tok_str[1:]):\n",
        "                    counts[(tok, next_tok)] += 1\n",
        "\n",
        "            # Learn a new merge rule\n",
        "            best_pair = max(counts, key=counts.get)\n",
        "            new_token, new_id = self.inv_vocab[best_pair[0]] + self.inv_vocab[best_pair[1]], len(self.vocab)\n",
        "            self.merge_rules[best_pair] = new_id\n",
        "            self.inv_vocab[new_id] = new_token\n",
        "            self.vocab[new_token]  = new_id\n",
        "\n",
        "            # Update tokens\n",
        "            new_data = []\n",
        "            for tok_str in data:\n",
        "                i, new_tok_str = 0, []\n",
        "                while i < len(tok_str):\n",
        "                    if i < len(tok_str) - 1 and (tok_str[i], tok_str[i+1]) == best_pair:\n",
        "                        new_tok_str.append(new_id)\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_tok_str.append(tok_str[i])\n",
        "                        i += 1\n",
        "                new_data.append(new_tok_str)\n",
        "            data = new_data\n",
        "\n",
        "    def pad(self, tokens, length):\n",
        "        \"\"\" Pads a tokenized string to a specified length, for batch processing.\n",
        "\n",
        "        Args:\n",
        "            tokens (list[int]): Encoded token string to be padded.\n",
        "            length (int): Length of tokens to pad to.\n",
        "\n",
        "        Returns:\n",
        "            list[int]: Token string padded to desired length.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if len(tokens) < length:\n",
        "            tokens = [ *tokens ]\n",
        "            tokens += ([ self.special_tokens['[PAD]'] ] * (length - len(tokens)))\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def unpad(self, tokens):\n",
        "        \"\"\" Removes padding from a token string.\n",
        "\n",
        "        Args:\n",
        "            tokens (list[int]): Encoded token string with padding.\n",
        "\n",
        "        Returns:\n",
        "            list[int]: Token string with padding removed.\n",
        "        \"\"\"\n",
        "\n",
        "        no_pad_len = len(tokens)\n",
        "        while tokens[no_pad_len-1] == self.special_tokens['[PAD]']: no_pad_len -= 1\n",
        "\n",
        "        return tokens[:no_pad_len]\n",
        "\n",
        "    def get_special_tokens(self):\n",
        "        \"\"\" Returns the associated special tokens.\n",
        "\n",
        "            Returns:\n",
        "                dict[str, int]: Mapping describing the special tokens, if any.\n",
        "                    This is a mapping between a string segment (token) and its associated id (token_id).\n",
        "        \"\"\"\n",
        "\n",
        "        return self.special_tokens\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        \"\"\" Returns the learnt vocabulary post the training process.\n",
        "\n",
        "            Returns:\n",
        "                dict[str, int]: Mapping describing the vocabulary and special tokens, if any.\n",
        "                    This is a mapping between a string segment (token) and its associated id (token_id).\n",
        "        \"\"\"\n",
        "\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, string, add_start=True, add_end=True):\n",
        "        \"\"\" Encodes a string into a list of tokens.\n",
        "\n",
        "        Args:\n",
        "            string (str): Input string to be tokenized.\n",
        "            add_start (bool): If true, adds the start of sequence token.\n",
        "            add_end (bool): If true, adds the end of sequence token.\n",
        "        Returns:\n",
        "            list[int]: List of tokens (unpadded).\n",
        "        \"\"\"\n",
        "\n",
        "        string = unicodedata.normalize('NFKC', string)\n",
        "\n",
        "        tokens = [ i+len(self.special_tokens) for i in string.encode('utf-8') ]\n",
        "\n",
        "        while len(tokens) > 1:\n",
        "            pairs = set()\n",
        "            for tok, next_tok in zip(tokens, tokens[1:]):\n",
        "                pairs.add((tok, next_tok))\n",
        "\n",
        "            merge_pair = min(pairs, key=lambda x: self.merge_rules.get(x, float(\"inf\")))\n",
        "            if merge_pair not in self.merge_rules: break\n",
        "\n",
        "            i, new_tokens = 0, []\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == merge_pair:\n",
        "                    new_tokens.append(self.merge_rules[merge_pair])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        if add_start: tokens = [ self.special_tokens['[BOS]'] ] + tokens\n",
        "        if add_end  : tokens = tokens + [ self.special_tokens['[EOS]'] ]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def decode(self, tokens, strip_special=True):\n",
        "        \"\"\" Decodes a string from a list of tokens.\n",
        "            Undoes the tokenization, returning back the input string.\n",
        "\n",
        "        Args:\n",
        "            tokens (list[int]): List of encoded tokens to be decoded. No padding is assumed.\n",
        "            strip_special (bool): Whether to remove special tokens or not.\n",
        "\n",
        "        Returns:\n",
        "            str: Decoded string.\n",
        "        \"\"\"\n",
        "\n",
        "        if strip_special:\n",
        "            special_tokens = set(self.special_tokens.values())\n",
        "            tokens = [ token for token in tokens if token not in special_tokens ]\n",
        "\n",
        "        return (b''.join(self.inv_vocab[tok_id] for tok_id in tokens)).decode('utf-8', errors='replace')\n",
        "\n",
        "    def batch_encode(self, batch, padding=None, add_start=True, add_end=True):\n",
        "        \"\"\"Encodes multiple strings in a batch to list of tokens padded to a given size.\n",
        "\n",
        "        Args:\n",
        "            batch (list[str]): List of strings to be tokenized.\n",
        "            padding (int, optional): Optional, desired tokenized length. Outputs will be padded to fit this length.\n",
        "            add_start (bool): If true, adds the start of sequence token.\n",
        "            add_end (bool): If true, adds the end of sequence token.\n",
        "\n",
        "        Returns:\n",
        "            list[list[int]]: List of tokenized outputs, padded to the same length.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_output = [ self.encode(string, add_start, add_end) for string in batch ]\n",
        "        if padding:\n",
        "            for i, tokens in enumerate(batch_output):\n",
        "                if len(tokens) < padding:\n",
        "                    batch_output[i] = self.pad(tokens, padding)\n",
        "        return batch_output\n",
        "\n",
        "    def batch_decode(self, batch, strip_special=True):\n",
        "        \"\"\" Decodes a batch of encoded tokens to normal strings.\n",
        "\n",
        "        Args:\n",
        "            batch (list[list[int]]): List of encoded token strings, optionally padded.\n",
        "            strip_special (bool): Whether to remove special tokens or not.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: Decoded strings after padding is removed.\n",
        "        \"\"\"\n",
        "        return [ self.decode(self.unpad(tokens), strip_special=strip_special) for tokens in batch ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__Tts8p7S04"
      },
      "source": [
        "Now with the tokenizer class, initialize and train the tokenizers for processing the parallel corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4A_U8cl_7S04"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "# Initialize the tokenizers as per the desired strategy.\n",
        "src_tokenizer = Tokenizer()\n",
        "tgt_tokenizer = Tokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OBYyr0Es7S04"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "# Edit the hyperparameters below as desired.\n",
        "SRC_VOCAB_SIZE = 300\n",
        "TGT_VOCAB_SIZE = 400\n",
        "\n",
        "\n",
        "# Train your tokenizer(s)\n",
        "src_tokenizer.train(train_data['Name']       , vocab_size=SRC_VOCAB_SIZE)\n",
        "tgt_tokenizer.train(train_data['Translation'], vocab_size=TGT_VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a2vmqRKH7S05"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "# Save the trained tokenizers\n",
        "src_tokenizer.save(os.path.join(DIRECTORY_NAME, \"src-tokenizer\"))\n",
        "tgt_tokenizer.save(os.path.join(DIRECTORY_NAME, \"tgt-tokenizer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s1O2qJYg7S06"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "def render_glyph(token):\n",
        "    \"\"\" Renders a token, handling invalid bytes in a safe, error-proof manner. \"\"\"\n",
        "\n",
        "    token = token.decode('utf-8', errors='replace') if isinstance(token, bytes) else token\n",
        "    return \"\".join([ c if unicodedata.category(c)[0] != \"C\" else f\"\\\\u{ord(c):04x}\" for c in token ])\n",
        "\n",
        "def inverse_vocabulary(tokenizer):\n",
        "    \"\"\" Generates an inverse vocabulary with rendered tokens.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (Tokenizer): Tokenizer whose vocabulary must be used.\n",
        "    \"\"\"\n",
        "\n",
        "    return { id: render_glyph(token) for token, id in tokenizer.get_vocabulary().items() }\n",
        "\n",
        "def apply_inverse_vocab(tokens, inv_vocab):\n",
        "    \"\"\" Decodes using the given inverse vocabulary.\n",
        "\n",
        "    Args:\n",
        "        tokens (list[int]): Tokens to process.\n",
        "        inv_vocab (dict[int, str]): Inverse vocabulary for mapping ids to tokens.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Mapped token glyphs.\n",
        "    \"\"\"\n",
        "\n",
        "    return [ inv_vocab[id] for id in tokens ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3VKC9YY7S06"
      },
      "source": [
        "We visualize a few outputs of the learnt tokenizers to assess their working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "75gjtCVW7S07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name           : दीपक\n",
            "Tokens         : [1, 274, 265, 278, 273, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'द', 'ी', 'प', 'क', '[EOS]']\n",
            "Decoded        : दीपक\n",
            "\n",
            "Name           : दामोदर\n",
            "Tokens         : [1, 274, 261, 264, 282, 274, 262, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'द', 'ा', 'म', 'ो', 'द', 'र', '[EOS]']\n",
            "Decoded        : दामोदर\n",
            "\n",
            "Name           : आमरिन\n",
            "Tokens         : [1, 300, 264, 262, 267, 263, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'आ', 'म', 'र', 'ि', 'न', '[EOS]']\n",
            "Decoded        : आमरिन\n",
            "\n",
            "Name           : बनिता\n",
            "Tokens         : [1, 279, 263, 267, 297, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'ब', 'न', 'ि', 'ता', '[EOS]']\n",
            "Decoded        : बनिता\n",
            "\n",
            "Name           : घन्स्याम\n",
            "Tokens         : [1, 259, 155, 263, 266, 268, 266, 291, 264, 2]\n",
            "Tokens (glyphs): ['[BOS]', '�', '�', 'न', '्', 'स', '्', 'या', 'म', '[EOS]']\n",
            "Decoded        : घन्स्याम\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "src_id_to_token = inverse_vocabulary(src_tokenizer)\n",
        "\n",
        "for example in train_data['Name'].sample(n=5, random_state=20240227):\n",
        "    print(\"Name           :\", example)\n",
        "    tokens = src_tokenizer.encode(example)\n",
        "    print(\"Tokens         :\", tokens)\n",
        "    print(\"Tokens (glyphs):\", apply_inverse_vocab(tokens, src_id_to_token))\n",
        "    print(\"Decoded        :\", src_tokenizer.decode(tokens), end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MJT2frJu7S08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name           : deepak\n",
            "Tokens         : [1, 332, 318, 110, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'dee', 'pa', 'k', '[EOS]']\n",
            "Decoded        : deepak\n",
            "\n",
            "Name           : damodar\n",
            "Tokens         : [1, 103, 264, 114, 103, 261, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'd', 'am', 'o', 'd', 'ar', '[EOS]']\n",
            "Decoded        : damodar\n",
            "\n",
            "Name           : aamrin\n",
            "Tokens         : [1, 100, 264, 117, 265, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'a', 'am', 'r', 'in', '[EOS]']\n",
            "Decoded        : aamrin\n",
            "\n",
            "Name           : banita\n",
            "Tokens         : [1, 101, 259, 327, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'b', 'an', 'ita', '[EOS]']\n",
            "Decoded        : banita\n",
            "\n",
            "Name           : ghansyam\n",
            "Tokens         : [1, 106, 345, 118, 375, 2]\n",
            "Tokens (glyphs): ['[BOS]', 'g', 'han', 's', 'yam', '[EOS]']\n",
            "Decoded        : ghansyam\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "tgt_id_to_token = inverse_vocabulary(tgt_tokenizer)\n",
        "\n",
        "for example in train_data['Translation'].sample(n=5, random_state=20240227):\n",
        "    print(\"Name           :\", example)\n",
        "    tokens = tgt_tokenizer.encode(example)\n",
        "    print(\"Tokens         :\", tokens)\n",
        "    print(\"Tokens (glyphs):\", apply_inverse_vocab(tokens, tgt_id_to_token))\n",
        "    print(\"Decoded        :\", tgt_tokenizer.decode(tokens), end='\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUOLUDqy7S09"
      },
      "source": [
        "We now abstract away the tokenizer into a pytorch compatible TokenizedDataset that will handle the tokenization internally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0BFaB2Nr7S09"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell\n",
        "\n",
        "class TokenizerDataset(TensorDataset):\n",
        "    \"\"\" Abstraction of the tokenizer functions as a pytorch dataset. \"\"\"\n",
        "\n",
        "    def __init__(self, data, src_tokenizer, tgt_tokenizer, src_padding=None, tgt_padding=None):\n",
        "        \"\"\" Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame of input and output strings.\n",
        "            src_tokenizer (Tokenizer): Tokenizer for the source language.\n",
        "            tgt_tokenizer (Tokenizer): Tokenizer for the target language.\n",
        "            src_padding (int, optional): Padding length for the source text. Defaults to None.\n",
        "            tgt_padding (int, optional): Padding length for the target text. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = data\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_padding = src_padding\n",
        "        self.tgt_padding = tgt_padding\n",
        "\n",
        "    def collate(self, batch):\n",
        "        \"\"\" Collates data instances into a batch of tokenized tensors.\n",
        "\n",
        "        Args:\n",
        "            batch (list[tuple]): List of x, y pairs.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor|PackedSequence, torch.Tensor|PackedSequence]: pair of tokenized tensors.\n",
        "        \"\"\"\n",
        "\n",
        "        x_batch = [ data[0] for data in batch ]\n",
        "        y_batch = [ data[1] for data in batch ]\n",
        "\n",
        "        x_batch = self.src_tokenizer.batch_encode(x_batch, self.src_padding)\n",
        "        y_batch = self.tgt_tokenizer.batch_encode(y_batch, self.tgt_padding)\n",
        "\n",
        "        if self.src_padding is None:\n",
        "            x_batch = torch.nn.utils.rnn.pack_sequence([ torch.tensor(tokens) for tokens in x_batch ], False)\n",
        "        else:\n",
        "            x_batch = torch.tensor(x_batch)\n",
        "\n",
        "        if self.tgt_padding is None:\n",
        "            y_batch = torch.nn.utils.rnn.pack_sequence([ torch.tensor(tokens) for tokens in y_batch ], False)\n",
        "        else:\n",
        "            y_batch = torch.tensor(y_batch)\n",
        "\n",
        "        return x_batch, y_batch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns the nth instance from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the instance to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple[str, str]: Untokenized instance pair.\n",
        "        \"\"\"\n",
        "\n",
        "        return (\n",
        "            self.data['Name'][index],\n",
        "            self.data['Translation'][index]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Returns the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBnISg0K7S1G"
      },
      "source": [
        "## Model-Agnostic Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUF3_vkA7S1G"
      },
      "source": [
        "Next, you'll implement a Trainer to train different models, since the data and tokenizer remains the same for all models.\n",
        "\n",
        "This trainer will receive the model, a loss function, an optimizer, a training and (optionally) a validation dataset and use these to train (and validate) the model.\n",
        "\n",
        "The trainer will also take care of handling checkpoints for training, which can be used to resume training across sessions.\n",
        "\n",
        "Derived classes can also be defined to handle different architectures, as to be done in the model-specific classes below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Kcq8zymS7S1H"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\" Performs model training in a model-agnostic manner.\n",
        "        Requires specifying the model instance, the loss criterion to optimize,\n",
        "          the optimizer to use and the directory to save data to.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, directory, model, criterion, optimizer):\n",
        "        \"\"\" Initializes the trainer.\n",
        "\n",
        "        Args:\n",
        "            directory (str): Directory to save checkpoints and the model data in.\n",
        "            model (torch.nn.Module): Torch model (must inherit `torch.nn.Module`) to train.\n",
        "            criterion (torch.nn.Function): Loss criterion, i.e., the loss function to optimize for training.\n",
        "            optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model            = model\n",
        "        self.optimizer        = optimizer\n",
        "        self.criterion        = criterion\n",
        "        self.directory        = directory\n",
        "        self.last_checkpoint  = 0\n",
        "        self.loss_history     = { 'train': [], 'valid': [] }\n",
        "\n",
        "        os.makedirs(self.directory, exist_ok=True)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataloader(dataset, shuffle_data=True, batch_size=8, collate_fn=None):\n",
        "        \"\"\" Create a dataloader for a torch Dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (torch.utils.data.Dataset): Dataset to process.\n",
        "            shuffle_data (bool, optional): If true, shuffles the data. Defaults to True.\n",
        "            batch_size (int, optional): Number of items per batch. Defaults to 8.\n",
        "            collate_fn (function, optional): Function to use for collating instances to a batch.\n",
        "\n",
        "        Returns:\n",
        "            torch.utils.data.DataLoader: Dataloader over the given data, post processing.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : trainer.make_dataloader\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        if collate_fn is None:\n",
        "            collate_fn = torch.utils.data.default_collate\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset, \n",
        "            batch_size=batch_size, \n",
        "            shuffle=shuffle_data, \n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def train_step(self, x_batch, y_batch):\n",
        "        \"\"\" Performs a step of training, on the training batch.\n",
        "\n",
        "        Args:\n",
        "            x_batch (torch.Tensor): Input batch.\n",
        "            y_batch (torch.Tensor): Output batch.\n",
        "\n",
        "        Returns:\n",
        "            float: Training loss with the current model, on this batch.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : trainer.train_step\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        x_batch = x_batch.to(self.device)\n",
        "        y_batch = y_batch.to(self.device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(x_batch)\n",
        "        loss = self.criterion(outputs, y_batch)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def eval_step(self, validation_dataloader):\n",
        "        \"\"\" Perfoms an evaluation step, on the validation dataloader.\n",
        "\n",
        "        Args:\n",
        "            validation_dataloader (torch.utils.data.DataLoader): Dataloader for the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            float: Validation loss with the current model checkpoint.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : trainer.eval_step\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Disable gradient computation for validation\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in validation_dataloader:\n",
        "                # Move tensors to the device\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(x_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        # Compute average loss\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "\n",
        "        # Set model back to training mode\n",
        "        self.model.train()\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def train(self, train_dataset, validation_dataset=None,\n",
        "              num_epochs=10, batch_size=8, shuffle=True,\n",
        "              save_steps=100, eval_steps=100, collate_fn=None):\n",
        "        \"\"\" Handles the training loop for the model.\n",
        "\n",
        "        Args:\n",
        "            train_dataset (torch.utils.data.Dataset): Dataset to train on.\n",
        "            validation_dataset (torch.utils.data.Dataset, optional): Data to validate on. Defaults to None.\n",
        "            num_epochs (int, optional): Number of epochs to train for. Defaults to 10.\n",
        "            batch_size (int, optional): Number of items to process per batch. Defaults to 8.\n",
        "            shuffle (bool, optional): Whether to shuffle the data or not. Defaults to True.\n",
        "            save_steps (int, optional): Number of steps post which a checkpoint should be saved. Defaults to 100.\n",
        "            eval_steps (int, optional): Number of steps post which the model should be evaluated. Defaults to 100.\n",
        "            collate_fn (function, optional): Function to use for collating instances to a batch.\n",
        "        \"\"\"\n",
        "\n",
        "        current_checkpoint = 0\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        with tqdm.tqdm(total = math.ceil(len(train_dataset) / batch_size) * num_epochs) as pbar:\n",
        "            for epoch in range(num_epochs):\n",
        "                train_dataloader      = self.make_dataloader(train_dataset, shuffle, batch_size, collate_fn)\n",
        "                if validation_dataset is not None:\n",
        "                    validation_dataloader = self.make_dataloader(validation_dataset, shuffle, batch_size, collate_fn)\n",
        "\n",
        "                for batch, (x_batch, y_batch) in enumerate(train_dataloader):\n",
        "                    pbar.set_description(f\"Epoch {epoch+1} / {num_epochs}\")\n",
        "\n",
        "                    # If we are resuming training, skip this iteration\n",
        "                    if current_checkpoint < self.last_checkpoint:\n",
        "                        current_checkpoint += 1\n",
        "                        pbar.update()\n",
        "                        continue\n",
        "\n",
        "                    # Do a step of training\n",
        "                    loss = self.train_step(x_batch, y_batch)\n",
        "                    self.loss_history['train'].append(loss)\n",
        "                    pbar.set_postfix({ 'batch': batch+1, 'loss': loss })\n",
        "\n",
        "                    current_checkpoint += 1\n",
        "                    pbar.update()\n",
        "\n",
        "                    # Evaluate after every eval_steps\n",
        "                    if (current_checkpoint) % eval_steps == 0:\n",
        "                        if validation_dataset is not None:\n",
        "                            val_loss = self.eval_step(validation_dataloader)\n",
        "                            self.loss_history['valid'].append(val_loss)\n",
        "                        else:\n",
        "                            val_loss = None\n",
        "\n",
        "                        print('[>]', f\"epoch #{epoch+1:{len(str(num_epochs))}},\",\n",
        "                              f\"batch #{batch+1:{len(str(len(train_dataloader)))}}:\",\n",
        "                              \"loss:\", f\"{loss:.8f}\", '|', \"val_loss:\", f\"{val_loss:.8f}\")\n",
        "\n",
        "                    # Save after every save_steps\n",
        "                    if (current_checkpoint) % save_steps == 0:\n",
        "                        self.save(current_checkpoint, { 'loss': loss, 'checkpoint': current_checkpoint })\n",
        "\n",
        "                    # free unused resources\n",
        "                    sync_vram()\n",
        "\n",
        "            self.save(current_checkpoint)\n",
        "\n",
        "    def resume(self):\n",
        "        \"\"\" Resumes training session from the most recent checkpoint. \"\"\"\n",
        "\n",
        "        if checkpoints := os.listdir(self.directory):\n",
        "            self.last_checkpoint = max(map(lambda x: int(x[11:]), filter(lambda x: 'checkpoint-' in x, checkpoints)))\n",
        "            checkpoint_dir = os.path.join(self.directory, f\"checkpoint-{self.last_checkpoint}\")\n",
        "            self.model.load_state_dict(torch.load(\n",
        "                os.path.join(checkpoint_dir, \"model.pt\"),\n",
        "                map_location=self.device\n",
        "            ))\n",
        "            self.model.to(self.device)\n",
        "            self.optimizer.load_state_dict(torch.load(\n",
        "                os.path.join(checkpoint_dir, \"optimizer.pt\"),\n",
        "                map_location=self.device\n",
        "            ))\n",
        "            with open(os.path.join(checkpoint_dir, \"loss.json\"), 'r', encoding='utf-8') as ifile:\n",
        "                self.loss_history = json.load(ifile)\n",
        "\n",
        "    def save(self, checkpoint=None, metadata=None):\n",
        "        \"\"\" Saves an associated model or a training checkpoint.\n",
        "\n",
        "            If a checkpoint is specified, saves a checkpoint specific directory with optimizer data\n",
        "                so that training can be resumed post that checkpoint.\n",
        "\n",
        "        Args:\n",
        "            checkpoint (int, optional): Checkpoint index. Defaults to None.\n",
        "            metadata (dict[str, any], optional): Additional metadata to save alongside a checkpoint. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            checkpoint_dir = os.path.join(self.directory, f\"checkpoint-{checkpoint}\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            torch.save(self.model.state_dict(), os.path.join(checkpoint_dir, \"model.pt\"))\n",
        "            torch.save(self.optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n",
        "            with open(os.path.join(checkpoint_dir, \"loss.json\"), \"w+\", encoding='utf-8') as ofile:\n",
        "                json.dump(self.loss_history, ofile, ensure_ascii=False, indent=2)\n",
        "            if metadata:\n",
        "                with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w+\", encoding='utf-8') as ofile:\n",
        "                    json.dump(metadata, ofile, ensure_ascii=False, indent=2)\n",
        "        else:\n",
        "            torch.save(self.model, os.path.join(self.directory, \"model.pt\"))\n",
        "            with open(os.path.join(self.directory, \"loss.json\"), \"w+\", encoding='utf-8') as ofile:\n",
        "                json.dump(self.loss_history, ofile, ensure_ascii=False, indent=2)\n",
        "            if metadata:\n",
        "                with open(os.path.join(self.directory, \"metadata.json\"), \"w+\", encoding='utf-8') as ofile:\n",
        "                    json.dump(metadata, ofile, ensure_ascii=False, indent=2)\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojRHAENJ7S1I"
      },
      "source": [
        "To test that the trainer works, try training a simple MLP network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "I8Qo2vdo7S1I"
      },
      "outputs": [],
      "source": [
        "X_train = torch.rand((500, 2))                      # (N x 2)\n",
        "X_dev   = torch.rand((20 , 2))                      # (N x 2)\n",
        "\n",
        "Y_train = (X_train[:, 0] - X_train[:, 1])[:, None]  # (N x 1)\n",
        "Y_dev   = (X_dev  [:, 0] - X_dev  [:, 1])[:, None]  # (N x 1)\n",
        "\n",
        "dummy_train_dataset = TensorDataset(X_train, Y_train)\n",
        "dummy_val_dataset   = TensorDataset(X_dev  , Y_dev  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i63ykUv37S1I"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac98db774d748df85297cf3a0d829cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[>] epoch # 2, batch #50: loss: 0.17097174 | val_loss: 0.10748805\n",
            "[>] epoch # 4, batch #50: loss: 0.18449108 | val_loss: 0.10368222\n",
            "[>] epoch # 6, batch #50: loss: 0.16746183 | val_loss: 0.10077672\n",
            "[>] epoch # 8, batch #50: loss: 0.22289558 | val_loss: 0.09598729\n",
            "[>] epoch #10, batch #50: loss: 0.07243576 | val_loss: 0.08913989\n"
          ]
        }
      ],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 4),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(4, 1)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "trainer = Trainer(\"mlp\", model, loss_fn, optimizer)\n",
        "trainer.train(dummy_train_dataset, dummy_val_dataset, batch_size=10, save_steps=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4hvE3VK7S1J"
      },
      "source": [
        "## Seq-2-Seq Modeling with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q32giBvl7S1J"
      },
      "source": [
        "In this section, you will implement an encoder-decoder network using RNNs, to learn a conditional language model for the task of translating the names to Hindi.\n",
        "\n",
        "You can use any type of RNN for this purpose: `RNN`, `GRU`, `LSTM`, etc. Consult the pytorch documentation for additional information.\n",
        "\n",
        "Additional tips for training:\n",
        "- Use regularization: Dropout, etc.\n",
        "- Use a suitable optimizer, such as Adam.\n",
        "- Format data accordingly before passing it to the trainer, using the helper functions.\n",
        "- Do you need to pad sequences when processing inputs as a batch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qgJfdMYX7S1K"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class RNNEncoderDecoderLM(torch.nn.Module):\n",
        "    \"\"\" Implements an Encoder-Decoder network, using RNN units. \"\"\"\n",
        "\n",
        "    # Feel free to add additional parameters to __init__\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embd_dims, hidden_size, num_layers=1, dropout=0.1):\n",
        "        \"\"\" Initializes the encoder-decoder network, implemented via RNNs.\n",
        "\n",
        "        Args:\n",
        "            src_vocab_size (int): Source vocabulary size.\n",
        "            tgt_vocab_size (int): Target vocabulary size.\n",
        "            embd_dims (int): Embedding dimensions.\n",
        "            hidden_size (int): Size/Dimensions for the hidden states.\n",
        "        \"\"\"\n",
        "\n",
        "        super(RNNEncoderDecoderLM, self).__init__()\n",
        "\n",
        "        # Dummy parameter to track the model device. Do not modify.\n",
        "        self._dummy_param = torch.nn.Parameter(torch.Tensor(0), requires_grad=False)\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn.init\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        self.src_embedding = torch.nn.Embedding(src_vocab_size, embd_dims, padding_idx=0)\n",
        "        self.tgt_embedding = torch.nn.Embedding(tgt_vocab_size, embd_dims, padding_idx=0)\n",
        "        \n",
        "        self.encoder = torch.nn.GRU(\n",
        "            input_size=embd_dims, \n",
        "            hidden_size=hidden_size, \n",
        "            num_layers=num_layers, \n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.decoder = torch.nn.GRU(\n",
        "            input_size=embd_dims, \n",
        "            hidden_size=hidden_size, \n",
        "            num_layers=num_layers, \n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.output_layer = torch.nn.Linear(hidden_size, tgt_vocab_size)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        \"\"\" Returns the device the model parameters are on. \"\"\"\n",
        "        return self._dummy_param.device\n",
        "\n",
        "    def forward(self, inputs, decoder_inputs, decoder_hidden_state=None):\n",
        "        \"\"\" Performs a forward pass over the encoder-decoder network.\n",
        "\n",
        "            Accepts inputs for the encoder, inputs for the decoder, and hidden state for\n",
        "                the decoder to continue generation after the given input.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): tensor of shape [batch_size?, max_seq_length]\n",
        "            decoder_inputs (torch.Tensor): tensor of shape [batch_size?, 1]\n",
        "            decoder_hidden_state (any): tensor to represent decoder hidden state from time step T-1.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, any]: output from the decoder, and associated hidden state for the next step.\n",
        "            Decoder outputs should be log probabilities over the target vocabulary.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn.forward\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        if isinstance(inputs, torch.nn.utils.rnn.PackedSequence):\n",
        "            inputs, batch_lengths = torch.nn.utils.rnn.pad_packed_sequence(inputs, batch_first=True)\n",
        "        \n",
        "        src_embedded = self.dropout(self.src_embedding(inputs))\n",
        "        \n",
        "        if isinstance(inputs, torch.nn.utils.rnn.PackedSequence):\n",
        "            packed_src = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "                src_embedded, \n",
        "                batch_lengths.cpu(), \n",
        "                batch_first=True, \n",
        "                enforce_sorted=False\n",
        "            )\n",
        "            _, encoder_hidden = self.encoder(packed_src)\n",
        "        else:\n",
        "            _, encoder_hidden = self.encoder(src_embedded)\n",
        "        \n",
        "        if decoder_hidden_state is None:\n",
        "            decoder_hidden_state = encoder_hidden\n",
        "        \n",
        "        tgt_embedded = self.dropout(self.tgt_embedding(decoder_inputs))\n",
        "        \n",
        "        decoder_output, decoder_hidden = self.decoder(tgt_embedded, decoder_hidden_state)\n",
        "        \n",
        "        output_logits = self.output_layer(decoder_output)\n",
        "        \n",
        "        log_probs = torch.log_softmax(output_logits, dim=-1)\n",
        "        \n",
        "        return log_probs, decoder_hidden\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def log_probability(self, seq_x, seq_y):\n",
        "        \"\"\" Compute the conditional log probability of seq_y given seq_x, i.e., log P(seq_y | seq_x).\n",
        "\n",
        "        Args:\n",
        "            seq_x (torch.tensor): Input sequence of tokens.\n",
        "            seq_y (torch.tensor): Output sequence of tokens.\n",
        "\n",
        "        Returns:\n",
        "            float: Log probability of seq_y given seq_x\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn.log_probability\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        self.train()\n",
        "        \n",
        "        seq_x = seq_x.to(self.device)\n",
        "        seq_y = seq_y.to(self.device)\n",
        "        \n",
        "        decoder_input = seq_y[:, :-1]  \n",
        "        decoder_target = seq_y[:, 1:]  # All but first token\n",
        "        \n",
        "        log_probs, _ = self(seq_x, decoder_input)\n",
        "        \n",
        "        log_probs = log_probs.reshape(-1, log_probs.size(-1))\n",
        "        decoder_target = decoder_target.reshape(-1)\n",
        "        \n",
        "        loss = torch.nn.functional.nll_loss(\n",
        "            log_probs, \n",
        "            decoder_target, \n",
        "            ignore_index=0  \n",
        "        )\n",
        "        \n",
        "        return -loss\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZmH1UT7S1K"
      },
      "source": [
        "To train the above model, implement for training and evaluation steps in the `RNNEncoderDecoderTrainer` class below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "plixWQjn7S1K"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class RNNEncoderDecoderTrainer(Trainer):\n",
        "    \"\"\" Performs model training for RNN-based Encoder-Decoder models. \"\"\"\n",
        "\n",
        "    def __init__(self, directory, model, criterion, optimizer):\n",
        "        \"\"\" Initializes the trainer.\n",
        "\n",
        "        Args:\n",
        "            directory (str): Directory to save checkpoints and the model data in.\n",
        "            model (torch.nn.Module): Torch model to train.\n",
        "            criterion (torch.nn.Function): Loss Criterion.\n",
        "            optimizer (torch.optim.Optimizer): Optimizer to use.\n",
        "        \"\"\"\n",
        "\n",
        "        super(RNNEncoderDecoderTrainer, self).__init__(directory, model, criterion, optimizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataloader(dataset, shuffle_data=True, batch_size=8, collate_fn=None):\n",
        "        \"\"\" Create a dataloader for a torch Dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (torch.utils.data.Dataset): Dataset to process.\n",
        "            shuffle_data (bool, optional): If true, shuffles the data. Defaults to True.\n",
        "            batch_size (int, optional): Number of items per batch. Defaults to 8.\n",
        "            collate_fn (function, optional): Function to collate instances in a batch.\n",
        "\n",
        "        Returns:\n",
        "            torch.utils.data.DataLoader: Dataloader over the given data, post processing.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : rnn-enc-dec-trainer.make_dataloader\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset, \n",
        "            batch_size=batch_size, \n",
        "            shuffle=shuffle_data, \n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def train_step(self, x_batch, y_batch):\n",
        "        \"\"\" Performs a step of training, on the training batch.\n",
        "\n",
        "        Args:\n",
        "            x_batch (torch.Tensor): Input batch tensor, of shape [batch_size, *instance_shape].\n",
        "              For RNNs this is [batch_size, src_padding] or a torch.nn.utils.rnn.PackedSequence of varying lengths per batch (depends on padding).\n",
        "            y_batch (torch.Tensor): Output batch tensor, of shape [batch_size, *instance_shape].\n",
        "              For RNNs this is [batch_size, tgt_padding] or a torch.nn.utils.rnn.PackedSequence of varying lengths per batch (depends on padding).\n",
        "\n",
        "        Returns:\n",
        "            float: Training loss with the current model, on this batch.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : rnn-enc-dec-trainer.train_step\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        x_batch = x_batch.to(self.model.device)\n",
        "        y_batch = y_batch.to(self.model.device)\n",
        "        \n",
        "        try:\n",
        "            loss = -self.model.log_probability(x_batch, y_batch)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n",
        "            \n",
        "            self.optimizer.step()\n",
        "            \n",
        "            return loss.item()\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Training step error: {e}\")\n",
        "            return float('inf')\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def eval_step(self, validation_dataloader):\n",
        "        \"\"\" Perfoms an evaluation step, on the validation dataloader.\n",
        "\n",
        "        Args:\n",
        "            validation_dataloader (torch.utils.data.DataLoader): Dataloader for the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            float: Validation loss with the current model checkpoint.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : rnn-enc-dec-trainer.eval_step\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        self.model.eval()  \n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in validation_dataloader:\n",
        "                x_batch = x_batch.to(self.model.device)\n",
        "                y_batch = y_batch.to(self.model.device)\n",
        "                \n",
        "                try:\n",
        "                    loss = -self.model.log_probability(x_batch, y_batch)\n",
        "                    total_loss += loss.item()\n",
        "                    total_batches += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Validation step error: {e}\")\n",
        "        \n",
        "        return total_loss / total_batches if total_batches > 0 else float('inf')\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AX2RQOe87S1L"
      },
      "outputs": [],
      "source": [
        "## == BEGIN EVALUATION PORTION\n",
        "\n",
        "# Edit the hyperparameters below to your desired values.\n",
        "\n",
        "# BEGIN CODE : rnn-enc-dec.params\n",
        "\n",
        "# Add parameters related to the model here.\n",
        "rnn_enc_dec_params = {\n",
        "    'src_vocab_size': len(src_tokenizer.get_vocabulary()),\n",
        "    'tgt_vocab_size': len(tgt_tokenizer.get_vocabulary()),\n",
        "    'embd_dims'     : 64,\n",
        "    'hidden_size'   : 128,\n",
        "    'dropout'       : 0.2,\n",
        "    'num_layers'    : 2\n",
        "}\n",
        "\n",
        "# Add parameters related to the dataset processing here.\n",
        "rnn_enc_dec_data_params = dict(\n",
        "    src_padding=max(len(src_tokenizer.encode(x)) for x in train_data['Name']),\n",
        "    tgt_padding=max(len(tgt_tokenizer.encode(x)) for x in train_data['Translation']),\n",
        ")\n",
        "\n",
        "# Add parameters related to training here.\n",
        "rnn_enc_dec_training_params = dict(\n",
        "    num_epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    save_steps=5,\n",
        "    eval_steps=2\n",
        ")\n",
        "\n",
        "# END CODE\n",
        "\n",
        "# Do not forget to set a deterministic seed.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = RNNEncoderDecoderLM(**rnn_enc_dec_params)\n",
        "\n",
        "# BEGIN CODE : rnn-enc-dec.train\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "\n",
        "# END CODE\n",
        "\n",
        "trainer = RNNEncoderDecoderTrainer(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\"),\n",
        "    model, criterion, optimizer\n",
        ")\n",
        "\n",
        "## == END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZGSIuMYP7S1L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3681005/399644002.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(\n",
            "/tmp/ipykernel_3681005/399644002.py:211: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.optimizer.load_state_dict(torch.load(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5024bb192574a68996419779f57fbf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7050 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[>] epoch # 1, batch #112: loss: 5.92198706 | val_loss: 5.86930378\n",
            "[>] epoch # 1, batch #114: loss: 5.82932520 | val_loss: 5.75476276\n",
            "[>] epoch # 1, batch #116: loss: 5.69143343 | val_loss: 5.56809194\n",
            "[>] epoch # 1, batch #118: loss: 5.44062519 | val_loss: 5.26597808\n",
            "[>] epoch # 1, batch #120: loss: 5.12778139 | val_loss: 4.85221221\n",
            "[>] epoch # 1, batch #122: loss: 4.67108822 | val_loss: 4.51300394\n",
            "[>] epoch # 1, batch #124: loss: 4.55991030 | val_loss: 4.39114978\n",
            "[>] epoch # 1, batch #126: loss: 4.47902489 | val_loss: 4.31103042\n",
            "[>] epoch # 1, batch #128: loss: 4.50118017 | val_loss: 4.24913527\n",
            "[>] epoch # 1, batch #130: loss: 4.27246428 | val_loss: 4.19805772\n",
            "[>] epoch # 1, batch #132: loss: 4.12167740 | val_loss: 4.17610920\n",
            "[>] epoch # 1, batch #134: loss: 4.30896950 | val_loss: 4.16238803\n",
            "[>] epoch # 1, batch #136: loss: 4.17692423 | val_loss: 4.13170616\n",
            "[>] epoch # 1, batch #138: loss: 4.22578049 | val_loss: 4.09695875\n",
            "[>] epoch # 1, batch #140: loss: 4.03238153 | val_loss: 4.06904272\n",
            "[>] epoch # 2, batch #  1: loss: 4.14857864 | val_loss: 4.06802663\n",
            "[>] epoch # 2, batch #  3: loss: 4.06740046 | val_loss: 4.04973412\n",
            "[>] epoch # 2, batch #  5: loss: 4.07812595 | val_loss: 4.03348079\n",
            "[>] epoch # 2, batch #  7: loss: 4.07042217 | val_loss: 4.01863496\n",
            "[>] epoch # 2, batch #  9: loss: 4.00846338 | val_loss: 4.01666649\n",
            "[>] epoch # 2, batch # 11: loss: 4.04935360 | val_loss: 4.01605091\n",
            "[>] epoch # 2, batch # 13: loss: 4.02290249 | val_loss: 4.00233210\n",
            "[>] epoch # 2, batch # 15: loss: 3.94299865 | val_loss: 3.97847942\n",
            "[>] epoch # 2, batch # 17: loss: 4.08989239 | val_loss: 3.97790669\n",
            "[>] epoch # 2, batch # 19: loss: 4.10310698 | val_loss: 3.98325577\n",
            "[>] epoch # 2, batch # 21: loss: 3.94885349 | val_loss: 3.97721017\n",
            "[>] epoch # 2, batch # 23: loss: 3.96418262 | val_loss: 3.96572518\n",
            "[>] epoch # 2, batch # 25: loss: 3.93735337 | val_loss: 3.96769582\n",
            "[>] epoch # 2, batch # 27: loss: 4.02413988 | val_loss: 3.95879944\n",
            "[>] epoch # 2, batch # 29: loss: 4.03182554 | val_loss: 3.94956002\n",
            "[>] epoch # 2, batch # 31: loss: 3.94955921 | val_loss: 3.94446259\n",
            "[>] epoch # 2, batch # 33: loss: 4.03651237 | val_loss: 3.93446416\n",
            "[>] epoch # 2, batch # 35: loss: 3.92884016 | val_loss: 3.92913640\n",
            "[>] epoch # 2, batch # 37: loss: 3.97797108 | val_loss: 3.92749977\n",
            "[>] epoch # 2, batch # 39: loss: 3.99056029 | val_loss: 3.92053575\n",
            "[>] epoch # 2, batch # 41: loss: 3.79379654 | val_loss: 3.92455869\n",
            "[>] epoch # 2, batch # 43: loss: 3.96622920 | val_loss: 3.91399191\n",
            "[>] epoch # 2, batch # 45: loss: 3.94323111 | val_loss: 3.90892366\n",
            "[>] epoch # 2, batch # 47: loss: 3.94520926 | val_loss: 3.91224027\n",
            "[>] epoch # 2, batch # 49: loss: 3.98342824 | val_loss: 3.89620755\n",
            "[>] epoch # 2, batch # 51: loss: 3.96950865 | val_loss: 3.87932075\n",
            "[>] epoch # 2, batch # 53: loss: 3.83925843 | val_loss: 3.90187779\n",
            "[>] epoch # 2, batch # 55: loss: 3.84526086 | val_loss: 3.87609619\n",
            "[>] epoch # 2, batch # 57: loss: 3.81330347 | val_loss: 3.89752403\n",
            "[>] epoch # 2, batch # 59: loss: 3.97765398 | val_loss: 3.88643043\n",
            "[>] epoch # 2, batch # 61: loss: 3.92623329 | val_loss: 3.87037517\n",
            "[>] epoch # 2, batch # 63: loss: 3.85647345 | val_loss: 3.90691924\n",
            "[>] epoch # 2, batch # 65: loss: 4.08506489 | val_loss: 3.88724685\n",
            "[>] epoch # 2, batch # 67: loss: 3.90295506 | val_loss: 3.85522655\n",
            "[>] epoch # 2, batch # 69: loss: 3.86516428 | val_loss: 3.87675355\n",
            "[>] epoch # 2, batch # 71: loss: 3.92546558 | val_loss: 3.87677466\n",
            "[>] epoch # 2, batch # 73: loss: 3.92009568 | val_loss: 3.84955632\n",
            "[>] epoch # 2, batch # 75: loss: 3.92546415 | val_loss: 3.85573495\n",
            "[>] epoch # 2, batch # 77: loss: 3.93510294 | val_loss: 3.85282738\n",
            "[>] epoch # 2, batch # 79: loss: 3.88984680 | val_loss: 3.84166131\n",
            "[>] epoch # 2, batch # 81: loss: 3.89340448 | val_loss: 3.83515826\n",
            "[>] epoch # 2, batch # 83: loss: 3.92379642 | val_loss: 3.84509298\n",
            "[>] epoch # 2, batch # 85: loss: 3.87229753 | val_loss: 3.82117468\n",
            "[>] epoch # 2, batch # 87: loss: 3.90395021 | val_loss: 3.81366075\n",
            "[>] epoch # 2, batch # 89: loss: 3.86742163 | val_loss: 3.80876195\n",
            "[>] epoch # 2, batch # 91: loss: 3.99561667 | val_loss: 3.81355746\n",
            "[>] epoch # 2, batch # 93: loss: 3.85385489 | val_loss: 3.80767081\n",
            "[>] epoch # 2, batch # 95: loss: 3.78001499 | val_loss: 3.79608677\n",
            "[>] epoch # 2, batch # 97: loss: 3.91266513 | val_loss: 3.78639007\n",
            "[>] epoch # 2, batch # 99: loss: 3.85398293 | val_loss: 3.78954660\n",
            "[>] epoch # 2, batch #101: loss: 3.85226059 | val_loss: 3.77759746\n",
            "[>] epoch # 2, batch #103: loss: 3.78371286 | val_loss: 3.81215717\n",
            "[>] epoch # 2, batch #105: loss: 3.76371050 | val_loss: 3.76350540\n",
            "[>] epoch # 2, batch #107: loss: 3.79088092 | val_loss: 3.77345280\n",
            "[>] epoch # 2, batch #109: loss: 3.74362516 | val_loss: 3.77035486\n",
            "[>] epoch # 2, batch #111: loss: 3.77917624 | val_loss: 3.74365693\n",
            "[>] epoch # 2, batch #113: loss: 3.74653888 | val_loss: 3.77267265\n",
            "[>] epoch # 2, batch #115: loss: 3.79708934 | val_loss: 3.75767526\n",
            "[>] epoch # 2, batch #117: loss: 3.70222878 | val_loss: 3.74076310\n",
            "[>] epoch # 2, batch #119: loss: 3.72411013 | val_loss: 3.75233566\n",
            "[>] epoch # 2, batch #121: loss: 3.56233668 | val_loss: 3.73462483\n",
            "[>] epoch # 2, batch #123: loss: 3.75302148 | val_loss: 3.71571136\n",
            "[>] epoch # 2, batch #125: loss: 3.78554654 | val_loss: 3.71094896\n",
            "[>] epoch # 2, batch #127: loss: 3.73359346 | val_loss: 3.69594851\n",
            "[>] epoch # 2, batch #129: loss: 3.72500277 | val_loss: 3.69257969\n",
            "[>] epoch # 2, batch #131: loss: 3.76378226 | val_loss: 3.68207354\n",
            "[>] epoch # 2, batch #133: loss: 3.70507932 | val_loss: 3.67389763\n",
            "[>] epoch # 2, batch #135: loss: 3.83120179 | val_loss: 3.67747443\n",
            "[>] epoch # 2, batch #137: loss: 3.62859797 | val_loss: 3.67509405\n",
            "[>] epoch # 2, batch #139: loss: 3.68207741 | val_loss: 3.68286324\n",
            "[>] epoch # 2, batch #141: loss: 3.57859445 | val_loss: 3.65743833\n",
            "[>] epoch # 3, batch #  2: loss: 3.62805367 | val_loss: 3.66726136\n",
            "[>] epoch # 3, batch #  4: loss: 3.59169126 | val_loss: 3.64417984\n",
            "[>] epoch # 3, batch #  6: loss: 3.73450708 | val_loss: 3.63684099\n",
            "[>] epoch # 3, batch #  8: loss: 3.63237882 | val_loss: 3.62382245\n",
            "[>] epoch # 3, batch # 10: loss: 3.55473995 | val_loss: 3.63076819\n",
            "[>] epoch # 3, batch # 12: loss: 3.58451128 | val_loss: 3.62067430\n",
            "[>] epoch # 3, batch # 14: loss: 3.54945588 | val_loss: 3.61307439\n",
            "[>] epoch # 3, batch # 16: loss: 3.76639700 | val_loss: 3.61513638\n",
            "[>] epoch # 3, batch # 18: loss: 3.71650004 | val_loss: 3.59393384\n",
            "[>] epoch # 3, batch # 20: loss: 3.56042099 | val_loss: 3.60485541\n",
            "[>] epoch # 3, batch # 22: loss: 3.55799747 | val_loss: 3.59390642\n",
            "[>] epoch # 3, batch # 24: loss: 3.56470680 | val_loss: 3.57659212\n",
            "[>] epoch # 3, batch # 26: loss: 3.54142213 | val_loss: 3.57683816\n",
            "[>] epoch # 3, batch # 28: loss: 3.43300605 | val_loss: 3.55964314\n",
            "[>] epoch # 3, batch # 30: loss: 3.69826245 | val_loss: 3.57875710\n",
            "[>] epoch # 3, batch # 32: loss: 3.61356759 | val_loss: 3.54931732\n",
            "[>] epoch # 3, batch # 34: loss: 3.69585323 | val_loss: 3.57440380\n",
            "[>] epoch # 3, batch # 36: loss: 3.54911947 | val_loss: 3.54308822\n",
            "[>] epoch # 3, batch # 38: loss: 3.64871478 | val_loss: 3.55617487\n",
            "[>] epoch # 3, batch # 40: loss: 3.50338149 | val_loss: 3.52952557\n",
            "[>] epoch # 3, batch # 42: loss: 3.50831819 | val_loss: 3.54216820\n",
            "[>] epoch # 3, batch # 44: loss: 3.54408503 | val_loss: 3.50758309\n",
            "[>] epoch # 3, batch # 46: loss: 3.62857676 | val_loss: 3.49822483\n",
            "[>] epoch # 3, batch # 48: loss: 3.46953821 | val_loss: 3.49590534\n",
            "[>] epoch # 3, batch # 50: loss: 3.55488420 | val_loss: 3.47743412\n",
            "[>] epoch # 3, batch # 52: loss: 3.51949191 | val_loss: 3.48414716\n",
            "[>] epoch # 3, batch # 54: loss: 3.49819207 | val_loss: 3.47332472\n",
            "[>] epoch # 3, batch # 56: loss: 3.56696534 | val_loss: 3.45918712\n",
            "[>] epoch # 3, batch # 58: loss: 3.48831081 | val_loss: 3.46441990\n",
            "[>] epoch # 3, batch # 60: loss: 3.56693172 | val_loss: 3.45668431\n",
            "[>] epoch # 3, batch # 62: loss: 3.44945884 | val_loss: 3.44627452\n",
            "[>] epoch # 3, batch # 64: loss: 3.45993471 | val_loss: 3.43576332\n",
            "[>] epoch # 3, batch # 66: loss: 3.65629148 | val_loss: 3.42407388\n",
            "[>] epoch # 3, batch # 68: loss: 3.34960914 | val_loss: 3.42276967\n",
            "[>] epoch # 3, batch # 70: loss: 3.56003881 | val_loss: 3.41247887\n",
            "[>] epoch # 3, batch # 72: loss: 3.32019162 | val_loss: 3.40401594\n",
            "[>] epoch # 3, batch # 74: loss: 3.53647232 | val_loss: 3.39537252\n",
            "[>] epoch # 3, batch # 76: loss: 3.42461538 | val_loss: 3.41643865\n",
            "[>] epoch # 3, batch # 78: loss: 3.42624784 | val_loss: 3.38331399\n",
            "[>] epoch # 3, batch # 80: loss: 3.57745957 | val_loss: 3.37713706\n",
            "[>] epoch # 3, batch # 82: loss: 3.39016461 | val_loss: 3.36665377\n",
            "[>] epoch # 3, batch # 84: loss: 3.42691374 | val_loss: 3.37491446\n",
            "[>] epoch # 3, batch # 86: loss: 3.36156893 | val_loss: 3.34467882\n",
            "[>] epoch # 3, batch # 88: loss: 3.49352765 | val_loss: 3.35035295\n",
            "[>] epoch # 3, batch # 90: loss: 3.43924117 | val_loss: 3.35835855\n",
            "[>] epoch # 3, batch # 92: loss: 3.21774697 | val_loss: 3.34777219\n",
            "[>] epoch # 3, batch # 94: loss: 3.44003797 | val_loss: 3.32979257\n",
            "[>] epoch # 3, batch # 96: loss: 3.29936123 | val_loss: 3.32897892\n",
            "[>] epoch # 3, batch # 98: loss: 3.25814486 | val_loss: 3.32614864\n",
            "[>] epoch # 3, batch #100: loss: 3.34420657 | val_loss: 3.32223338\n",
            "[>] epoch # 3, batch #102: loss: 3.33302927 | val_loss: 3.32077863\n",
            "[>] epoch # 3, batch #104: loss: 3.29648972 | val_loss: 3.31980562\n",
            "[>] epoch # 3, batch #106: loss: 3.30259442 | val_loss: 3.30769653\n",
            "[>] epoch # 3, batch #108: loss: 3.33166766 | val_loss: 3.31819547\n",
            "[>] epoch # 3, batch #110: loss: 3.38851452 | val_loss: 3.30107342\n",
            "[>] epoch # 3, batch #112: loss: 3.31103277 | val_loss: 3.31475375\n",
            "[>] epoch # 3, batch #114: loss: 3.28350019 | val_loss: 3.29333707\n",
            "[>] epoch # 3, batch #116: loss: 3.12291193 | val_loss: 3.29482229\n",
            "[>] epoch # 3, batch #118: loss: 3.28383923 | val_loss: 3.28161003\n",
            "[>] epoch # 3, batch #120: loss: 3.29375243 | val_loss: 3.29138349\n",
            "[>] epoch # 3, batch #122: loss: 3.37254214 | val_loss: 3.27664771\n",
            "[>] epoch # 3, batch #124: loss: 3.30642080 | val_loss: 3.28924793\n",
            "[>] epoch # 3, batch #126: loss: 3.28389931 | val_loss: 3.27754193\n",
            "[>] epoch # 3, batch #128: loss: 3.33478618 | val_loss: 3.25742192\n",
            "[>] epoch # 3, batch #130: loss: 3.16799569 | val_loss: 3.24832682\n",
            "[>] epoch # 3, batch #132: loss: 3.22381830 | val_loss: 3.25366325\n",
            "[>] epoch # 3, batch #134: loss: 3.23069453 | val_loss: 3.23419657\n",
            "[>] epoch # 3, batch #136: loss: 3.06840706 | val_loss: 3.24181784\n",
            "[>] epoch # 3, batch #138: loss: 3.32815766 | val_loss: 3.20730842\n",
            "[>] epoch # 3, batch #140: loss: 3.34213471 | val_loss: 3.23107683\n",
            "[>] epoch # 4, batch #  1: loss: 3.24493384 | val_loss: 3.20273509\n",
            "[>] epoch # 4, batch #  3: loss: 3.03818560 | val_loss: 3.19865766\n",
            "[>] epoch # 4, batch #  5: loss: 3.04007316 | val_loss: 3.18429919\n",
            "[>] epoch # 4, batch #  7: loss: 3.03451467 | val_loss: 3.18265634\n",
            "[>] epoch # 4, batch #  9: loss: 3.00999069 | val_loss: 3.20769163\n",
            "[>] epoch # 4, batch # 11: loss: 3.19463015 | val_loss: 3.17657274\n",
            "[>] epoch # 4, batch # 13: loss: 3.07049251 | val_loss: 3.17844637\n",
            "[>] epoch # 4, batch # 15: loss: 3.25139546 | val_loss: 3.18283431\n",
            "[>] epoch # 4, batch # 17: loss: 3.25020313 | val_loss: 3.17916067\n",
            "[>] epoch # 4, batch # 19: loss: 3.00706530 | val_loss: 3.17058116\n",
            "[>] epoch # 4, batch # 21: loss: 3.14721608 | val_loss: 3.16671104\n",
            "[>] epoch # 4, batch # 23: loss: 3.08831453 | val_loss: 3.16213661\n",
            "[>] epoch # 4, batch # 25: loss: 3.26323128 | val_loss: 3.17092490\n",
            "[>] epoch # 4, batch # 27: loss: 3.07092094 | val_loss: 3.14975434\n",
            "[>] epoch # 4, batch # 29: loss: 3.26258850 | val_loss: 3.16319449\n",
            "[>] epoch # 4, batch # 31: loss: 3.15010285 | val_loss: 3.13030234\n",
            "[>] epoch # 4, batch # 33: loss: 3.13534474 | val_loss: 3.14489330\n",
            "[>] epoch # 4, batch # 35: loss: 3.07232904 | val_loss: 3.12333789\n",
            "[>] epoch # 4, batch # 37: loss: 3.03969312 | val_loss: 3.12685185\n",
            "[>] epoch # 4, batch # 39: loss: 2.99346805 | val_loss: 3.10643286\n",
            "[>] epoch # 4, batch # 41: loss: 3.14780784 | val_loss: 3.12333833\n",
            "[>] epoch # 4, batch # 43: loss: 3.28218913 | val_loss: 3.08483019\n",
            "[>] epoch # 4, batch # 45: loss: 3.13095164 | val_loss: 3.09447433\n",
            "[>] epoch # 4, batch # 47: loss: 3.07189703 | val_loss: 3.07899662\n",
            "[>] epoch # 4, batch # 49: loss: 3.16728067 | val_loss: 3.08411422\n",
            "[>] epoch # 4, batch # 51: loss: 3.12534833 | val_loss: 3.05397373\n",
            "[>] epoch # 4, batch # 53: loss: 3.03986621 | val_loss: 3.07165425\n",
            "[>] epoch # 4, batch # 55: loss: 3.06086755 | val_loss: 3.07158951\n",
            "[>] epoch # 4, batch # 57: loss: 3.25377131 | val_loss: 3.05107124\n",
            "[>] epoch # 4, batch # 59: loss: 2.93017983 | val_loss: 3.03571140\n",
            "[>] epoch # 4, batch # 61: loss: 3.01011586 | val_loss: 3.05564057\n",
            "[>] epoch # 4, batch # 63: loss: 3.11950350 | val_loss: 3.03676730\n",
            "[>] epoch # 4, batch # 65: loss: 3.04585719 | val_loss: 3.06324172\n",
            "[>] epoch # 4, batch # 67: loss: 2.83476233 | val_loss: 3.04345932\n",
            "[>] epoch # 4, batch # 69: loss: 3.05678630 | val_loss: 3.02229733\n",
            "[>] epoch # 4, batch # 71: loss: 3.05161333 | val_loss: 3.02389712\n",
            "[>] epoch # 4, batch # 73: loss: 3.15114975 | val_loss: 3.00269838\n",
            "[>] epoch # 4, batch # 75: loss: 2.99639583 | val_loss: 2.99356660\n",
            "[>] epoch # 4, batch # 77: loss: 3.02142382 | val_loss: 2.99233989\n",
            "[>] epoch # 4, batch # 79: loss: 2.91246700 | val_loss: 3.00438705\n",
            "[>] epoch # 4, batch # 81: loss: 2.99279618 | val_loss: 2.98060390\n",
            "[>] epoch # 4, batch # 83: loss: 2.98914051 | val_loss: 2.98283626\n",
            "[>] epoch # 4, batch # 85: loss: 2.90507150 | val_loss: 2.97015685\n",
            "[>] epoch # 4, batch # 87: loss: 2.98594332 | val_loss: 2.97445615\n",
            "[>] epoch # 4, batch # 89: loss: 2.95191693 | val_loss: 2.96179553\n",
            "[>] epoch # 4, batch # 91: loss: 3.00259614 | val_loss: 2.96517898\n",
            "[>] epoch # 4, batch # 93: loss: 3.01434755 | val_loss: 2.95351766\n",
            "[>] epoch # 4, batch # 95: loss: 2.90795302 | val_loss: 2.93183299\n",
            "[>] epoch # 4, batch # 97: loss: 2.92484784 | val_loss: 2.92680128\n",
            "[>] epoch # 4, batch # 99: loss: 2.75754523 | val_loss: 2.93998175\n",
            "[>] epoch # 4, batch #101: loss: 2.84492278 | val_loss: 2.92749308\n",
            "[>] epoch # 4, batch #103: loss: 2.90611410 | val_loss: 2.90261034\n",
            "[>] epoch # 4, batch #105: loss: 2.78375244 | val_loss: 2.91998500\n",
            "[>] epoch # 4, batch #107: loss: 3.06730247 | val_loss: 2.90734902\n",
            "[>] epoch # 4, batch #109: loss: 2.86290622 | val_loss: 2.92707053\n",
            "[>] epoch # 4, batch #111: loss: 3.12043786 | val_loss: 2.90319074\n",
            "[>] epoch # 4, batch #113: loss: 2.85332751 | val_loss: 2.88228673\n",
            "[>] epoch # 4, batch #115: loss: 2.82960916 | val_loss: 2.88033269\n",
            "[>] epoch # 4, batch #117: loss: 2.81968474 | val_loss: 2.89442448\n",
            "[>] epoch # 4, batch #119: loss: 2.86470151 | val_loss: 2.87502562\n",
            "[>] epoch # 4, batch #121: loss: 3.01760197 | val_loss: 2.87694984\n",
            "[>] epoch # 4, batch #123: loss: 2.92171359 | val_loss: 2.85929814\n",
            "[>] epoch # 4, batch #125: loss: 2.76526976 | val_loss: 2.88099916\n",
            "[>] epoch # 4, batch #127: loss: 3.03413606 | val_loss: 2.87588862\n",
            "[>] epoch # 4, batch #129: loss: 2.85470963 | val_loss: 2.86395570\n",
            "[>] epoch # 4, batch #131: loss: 2.86526704 | val_loss: 2.84764136\n",
            "[>] epoch # 4, batch #133: loss: 2.66045713 | val_loss: 2.84913969\n",
            "[>] epoch # 4, batch #135: loss: 2.79691410 | val_loss: 2.82416322\n",
            "[>] epoch # 4, batch #137: loss: 2.89498353 | val_loss: 2.83038614\n",
            "[>] epoch # 4, batch #139: loss: 2.81235313 | val_loss: 2.82787382\n",
            "[>] epoch # 4, batch #141: loss: 2.69737363 | val_loss: 2.85245963\n",
            "[>] epoch # 5, batch #  2: loss: 2.78515816 | val_loss: 2.82762570\n",
            "[>] epoch # 5, batch #  4: loss: 2.86870384 | val_loss: 2.82850311\n",
            "[>] epoch # 5, batch #  6: loss: 2.78938627 | val_loss: 2.80730519\n",
            "[>] epoch # 5, batch #  8: loss: 2.78264046 | val_loss: 2.81300371\n",
            "[>] epoch # 5, batch # 10: loss: 2.82137346 | val_loss: 2.78931979\n",
            "[>] epoch # 5, batch # 12: loss: 2.68779588 | val_loss: 2.79962947\n",
            "[>] epoch # 5, batch # 14: loss: 2.74673486 | val_loss: 2.77911626\n",
            "[>] epoch # 5, batch # 16: loss: 2.54684329 | val_loss: 2.79953581\n",
            "[>] epoch # 5, batch # 18: loss: 2.76066804 | val_loss: 2.78662507\n",
            "[>] epoch # 5, batch # 20: loss: 2.68677187 | val_loss: 2.76047200\n",
            "[>] epoch # 5, batch # 22: loss: 2.74970675 | val_loss: 2.78792449\n",
            "[>] epoch # 5, batch # 24: loss: 2.63559961 | val_loss: 2.76351690\n",
            "[>] epoch # 5, batch # 26: loss: 2.75702024 | val_loss: 2.78017965\n",
            "[>] epoch # 5, batch # 28: loss: 2.60655999 | val_loss: 2.75101913\n",
            "[>] epoch # 5, batch # 30: loss: 2.66530466 | val_loss: 2.76703924\n",
            "[>] epoch # 5, batch # 32: loss: 2.75755668 | val_loss: 2.74789796\n",
            "[>] epoch # 5, batch # 34: loss: 2.51503134 | val_loss: 2.73036194\n",
            "[>] epoch # 5, batch # 36: loss: 2.77940369 | val_loss: 2.72990557\n",
            "[>] epoch # 5, batch # 38: loss: 2.76577568 | val_loss: 2.72824210\n",
            "[>] epoch # 5, batch # 40: loss: 2.68208408 | val_loss: 2.72470647\n",
            "[>] epoch # 5, batch # 42: loss: 2.60841465 | val_loss: 2.71523542\n",
            "[>] epoch # 5, batch # 44: loss: 2.76640821 | val_loss: 2.69827375\n",
            "[>] epoch # 5, batch # 46: loss: 2.85486269 | val_loss: 2.70455922\n",
            "[>] epoch # 5, batch # 48: loss: 2.80853939 | val_loss: 2.71537355\n",
            "[>] epoch # 5, batch # 50: loss: 2.55486560 | val_loss: 2.71562296\n",
            "[>] epoch # 5, batch # 52: loss: 2.62046146 | val_loss: 2.68193329\n",
            "[>] epoch # 5, batch # 54: loss: 2.54093409 | val_loss: 2.69130740\n",
            "[>] epoch # 5, batch # 56: loss: 2.62523270 | val_loss: 2.69743397\n",
            "[>] epoch # 5, batch # 58: loss: 2.56017256 | val_loss: 2.73537251\n",
            "[>] epoch # 5, batch # 60: loss: 2.63172340 | val_loss: 2.69064316\n",
            "[>] epoch # 5, batch # 62: loss: 2.67615128 | val_loss: 2.70116879\n",
            "[>] epoch # 5, batch # 64: loss: 2.72016478 | val_loss: 2.69499595\n",
            "[>] epoch # 5, batch # 66: loss: 2.61723661 | val_loss: 2.71139796\n",
            "[>] epoch # 5, batch # 68: loss: 2.51262546 | val_loss: 2.70376455\n",
            "[>] epoch # 5, batch # 70: loss: 2.69533730 | val_loss: 2.68424597\n",
            "[>] epoch # 5, batch # 72: loss: 2.60357451 | val_loss: 2.66827673\n",
            "[>] epoch # 5, batch # 74: loss: 2.59925842 | val_loss: 2.65973194\n",
            "[>] epoch # 5, batch # 76: loss: 2.83200336 | val_loss: 2.64734527\n",
            "[>] epoch # 5, batch # 78: loss: 2.68387341 | val_loss: 2.64192152\n",
            "[>] epoch # 5, batch # 80: loss: 2.40631723 | val_loss: 2.66139328\n",
            "[>] epoch # 5, batch # 82: loss: 2.56729698 | val_loss: 2.63805155\n",
            "[>] epoch # 5, batch # 84: loss: 2.59247875 | val_loss: 2.61988356\n",
            "[>] epoch # 5, batch # 86: loss: 2.70628572 | val_loss: 2.62493737\n",
            "[>] epoch # 5, batch # 88: loss: 2.51066589 | val_loss: 2.61664284\n",
            "[>] epoch # 5, batch # 90: loss: 2.46429849 | val_loss: 2.63003028\n",
            "[>] epoch # 5, batch # 92: loss: 2.60689688 | val_loss: 2.59401283\n",
            "[>] epoch # 5, batch # 94: loss: 2.59697986 | val_loss: 2.62209949\n",
            "[>] epoch # 5, batch # 96: loss: 2.66445017 | val_loss: 2.61094843\n",
            "[>] epoch # 5, batch # 98: loss: 2.63665056 | val_loss: 2.60608072\n",
            "[>] epoch # 5, batch #100: loss: 2.48563194 | val_loss: 2.60438332\n",
            "[>] epoch # 5, batch #102: loss: 2.59613013 | val_loss: 2.59447125\n",
            "[>] epoch # 5, batch #104: loss: 2.54748821 | val_loss: 2.60192007\n",
            "[>] epoch # 5, batch #106: loss: 2.52546740 | val_loss: 2.58843954\n",
            "[>] epoch # 5, batch #108: loss: 2.76493120 | val_loss: 2.57975731\n",
            "[>] epoch # 5, batch #110: loss: 2.56086493 | val_loss: 2.57535795\n",
            "[>] epoch # 5, batch #112: loss: 2.55367804 | val_loss: 2.57723308\n",
            "[>] epoch # 5, batch #114: loss: 2.59264469 | val_loss: 2.57578461\n",
            "[>] epoch # 5, batch #116: loss: 2.77049088 | val_loss: 2.56937883\n",
            "[>] epoch # 5, batch #118: loss: 2.59788322 | val_loss: 2.56110896\n",
            "[>] epoch # 5, batch #120: loss: 2.43131256 | val_loss: 2.55603589\n",
            "[>] epoch # 5, batch #122: loss: 2.52035022 | val_loss: 2.54949597\n",
            "[>] epoch # 5, batch #124: loss: 2.40779972 | val_loss: 2.55941312\n",
            "[>] epoch # 5, batch #126: loss: 2.41258454 | val_loss: 2.52917965\n",
            "[>] epoch # 5, batch #128: loss: 2.43288302 | val_loss: 2.51740870\n",
            "[>] epoch # 5, batch #130: loss: 2.56138039 | val_loss: 2.53261663\n",
            "[>] epoch # 5, batch #132: loss: 2.58126354 | val_loss: 2.53618378\n",
            "[>] epoch # 5, batch #134: loss: 2.32981730 | val_loss: 2.50573061\n",
            "[>] epoch # 5, batch #136: loss: 2.56445622 | val_loss: 2.50074075\n",
            "[>] epoch # 5, batch #138: loss: 2.45135212 | val_loss: 2.50222696\n",
            "[>] epoch # 5, batch #140: loss: 2.39592862 | val_loss: 2.50888287\n",
            "[>] epoch # 6, batch #  1: loss: 2.33457112 | val_loss: 2.50865496\n",
            "[>] epoch # 6, batch #  3: loss: 2.57343650 | val_loss: 2.51000322\n",
            "[>] epoch # 6, batch #  5: loss: 2.50866961 | val_loss: 2.50360526\n",
            "[>] epoch # 6, batch #  7: loss: 2.25138998 | val_loss: 2.50107474\n",
            "[>] epoch # 6, batch #  9: loss: 2.24868417 | val_loss: 2.49462291\n",
            "[>] epoch # 6, batch # 11: loss: 2.46551299 | val_loss: 2.48271146\n",
            "[>] epoch # 6, batch # 13: loss: 2.34540009 | val_loss: 2.49853079\n",
            "[>] epoch # 6, batch # 15: loss: 2.39882469 | val_loss: 2.47233983\n",
            "[>] epoch # 6, batch # 17: loss: 2.35187960 | val_loss: 2.46349456\n",
            "[>] epoch # 6, batch # 19: loss: 2.34943295 | val_loss: 2.46474994\n",
            "[>] epoch # 6, batch # 21: loss: 2.36225295 | val_loss: 2.46642678\n",
            "[>] epoch # 6, batch # 23: loss: 2.39247656 | val_loss: 2.46211509\n",
            "[>] epoch # 6, batch # 25: loss: 2.26295972 | val_loss: 2.45175006\n",
            "[>] epoch # 6, batch # 27: loss: 2.45390344 | val_loss: 2.45225850\n",
            "[>] epoch # 6, batch # 29: loss: 2.25012803 | val_loss: 2.45806876\n",
            "[>] epoch # 6, batch # 31: loss: 2.30199242 | val_loss: 2.44294060\n",
            "[>] epoch # 6, batch # 33: loss: 2.39010549 | val_loss: 2.47055028\n",
            "[>] epoch # 6, batch # 35: loss: 2.20002270 | val_loss: 2.42387320\n",
            "[>] epoch # 6, batch # 37: loss: 2.24393988 | val_loss: 2.49024558\n",
            "[>] epoch # 6, batch # 39: loss: 2.53346586 | val_loss: 2.43479057\n",
            "[>] epoch # 6, batch # 41: loss: 2.34461379 | val_loss: 2.44424079\n",
            "[>] epoch # 6, batch # 43: loss: 2.45064974 | val_loss: 2.44277110\n",
            "[>] epoch # 6, batch # 45: loss: 2.35649562 | val_loss: 2.43106222\n",
            "[>] epoch # 6, batch # 47: loss: 2.28575897 | val_loss: 2.42530913\n",
            "[>] epoch # 6, batch # 49: loss: 2.34366822 | val_loss: 2.42936961\n",
            "[>] epoch # 6, batch # 51: loss: 2.49266195 | val_loss: 2.43192396\n",
            "[>] epoch # 6, batch # 53: loss: 2.30265760 | val_loss: 2.41503842\n",
            "[>] epoch # 6, batch # 55: loss: 2.38219905 | val_loss: 2.40981801\n",
            "[>] epoch # 6, batch # 57: loss: 2.31810236 | val_loss: 2.40366499\n",
            "[>] epoch # 6, batch # 59: loss: 2.17386389 | val_loss: 2.40126656\n",
            "[>] epoch # 6, batch # 61: loss: 2.28037262 | val_loss: 2.41471784\n",
            "[>] epoch # 6, batch # 63: loss: 2.33667803 | val_loss: 2.40283390\n",
            "[>] epoch # 6, batch # 65: loss: 2.28829575 | val_loss: 2.38749818\n",
            "[>] epoch # 6, batch # 67: loss: 2.38101673 | val_loss: 2.38634674\n",
            "[>] epoch # 6, batch # 69: loss: 2.31406450 | val_loss: 2.38762458\n",
            "[>] epoch # 6, batch # 71: loss: 2.30142069 | val_loss: 2.36826400\n",
            "[>] epoch # 6, batch # 73: loss: 2.21138334 | val_loss: 2.36816071\n",
            "[>] epoch # 6, batch # 75: loss: 2.20484734 | val_loss: 2.39668461\n",
            "[>] epoch # 6, batch # 77: loss: 2.18076730 | val_loss: 2.37196192\n",
            "[>] epoch # 6, batch # 79: loss: 2.35731649 | val_loss: 2.36707779\n",
            "[>] epoch # 6, batch # 81: loss: 2.29715419 | val_loss: 2.35255269\n",
            "[>] epoch # 6, batch # 83: loss: 2.42208457 | val_loss: 2.34648969\n",
            "[>] epoch # 6, batch # 85: loss: 2.34665179 | val_loss: 2.34226977\n",
            "[>] epoch # 6, batch # 87: loss: 2.24711967 | val_loss: 2.35320544\n",
            "[>] epoch # 6, batch # 89: loss: 2.22320509 | val_loss: 2.35193663\n",
            "[>] epoch # 6, batch # 91: loss: 2.15396810 | val_loss: 2.32706780\n",
            "[>] epoch # 6, batch # 93: loss: 2.40302038 | val_loss: 2.31146517\n",
            "[>] epoch # 6, batch # 95: loss: 2.31037331 | val_loss: 2.32354793\n",
            "[>] epoch # 6, batch # 97: loss: 2.10310125 | val_loss: 2.31790790\n",
            "[>] epoch # 6, batch # 99: loss: 2.32631683 | val_loss: 2.33813354\n",
            "[>] epoch # 6, batch #101: loss: 2.29641175 | val_loss: 2.31673512\n",
            "[>] epoch # 6, batch #103: loss: 2.32227135 | val_loss: 2.33147205\n",
            "[>] epoch # 6, batch #105: loss: 2.40241241 | val_loss: 2.28883437\n",
            "[>] epoch # 6, batch #107: loss: 2.36982632 | val_loss: 2.27084442\n",
            "[>] epoch # 6, batch #109: loss: 2.40744209 | val_loss: 2.28443879\n",
            "[>] epoch # 6, batch #111: loss: 2.15393424 | val_loss: 2.27716517\n",
            "[>] epoch # 6, batch #113: loss: 2.23814726 | val_loss: 2.28324239\n",
            "[>] epoch # 6, batch #115: loss: 2.02442718 | val_loss: 2.27500732\n",
            "[>] epoch # 6, batch #117: loss: 2.22175479 | val_loss: 2.28085459\n",
            "[>] epoch # 6, batch #119: loss: 2.20815635 | val_loss: 2.25982789\n",
            "[>] epoch # 6, batch #121: loss: 2.20512033 | val_loss: 2.25515378\n",
            "[>] epoch # 6, batch #123: loss: 2.17700386 | val_loss: 2.25929132\n",
            "[>] epoch # 6, batch #125: loss: 2.13826561 | val_loss: 2.23518848\n",
            "[>] epoch # 6, batch #127: loss: 2.18785453 | val_loss: 2.24573918\n",
            "[>] epoch # 6, batch #129: loss: 1.96067333 | val_loss: 2.25321348\n",
            "[>] epoch # 6, batch #131: loss: 2.08274961 | val_loss: 2.23790404\n",
            "[>] epoch # 6, batch #133: loss: 2.12533998 | val_loss: 2.27001669\n",
            "[>] epoch # 6, batch #135: loss: 2.15192962 | val_loss: 2.24707382\n",
            "[>] epoch # 6, batch #137: loss: 2.17735195 | val_loss: 2.24612329\n",
            "[>] epoch # 6, batch #139: loss: 2.01499605 | val_loss: 2.24867441\n",
            "[>] epoch # 6, batch #141: loss: 2.57505751 | val_loss: 2.22792178\n",
            "[>] epoch # 7, batch #  2: loss: 2.15276432 | val_loss: 2.23886525\n",
            "[>] epoch # 7, batch #  4: loss: 2.08871770 | val_loss: 2.23588676\n",
            "[>] epoch # 7, batch #  6: loss: 2.32607055 | val_loss: 2.21380621\n",
            "[>] epoch # 7, batch #  8: loss: 2.20163083 | val_loss: 2.21419384\n",
            "[>] epoch # 7, batch # 10: loss: 1.98587394 | val_loss: 2.20909286\n",
            "[>] epoch # 7, batch # 12: loss: 2.02169061 | val_loss: 2.21692412\n",
            "[>] epoch # 7, batch # 14: loss: 2.03959131 | val_loss: 2.20472064\n",
            "[>] epoch # 7, batch # 16: loss: 2.20972943 | val_loss: 2.21812325\n",
            "[>] epoch # 7, batch # 18: loss: 2.05560756 | val_loss: 2.21302144\n",
            "[>] epoch # 7, batch # 20: loss: 1.92442703 | val_loss: 2.21878921\n",
            "[>] epoch # 7, batch # 22: loss: 2.00430202 | val_loss: 2.20628479\n",
            "[>] epoch # 7, batch # 24: loss: 2.08850265 | val_loss: 2.20828676\n",
            "[>] epoch # 7, batch # 26: loss: 2.22543454 | val_loss: 2.20422176\n",
            "[>] epoch # 7, batch # 28: loss: 2.18211937 | val_loss: 2.19007142\n",
            "[>] epoch # 7, batch # 30: loss: 2.01000857 | val_loss: 2.19071739\n",
            "[>] epoch # 7, batch # 32: loss: 2.02006769 | val_loss: 2.18068167\n",
            "[>] epoch # 7, batch # 34: loss: 2.01854897 | val_loss: 2.22666086\n",
            "[>] epoch # 7, batch # 36: loss: 2.25527287 | val_loss: 2.17621963\n",
            "[>] epoch # 7, batch # 38: loss: 2.22579908 | val_loss: 2.18025517\n",
            "[>] epoch # 7, batch # 40: loss: 1.89847040 | val_loss: 2.18616807\n",
            "[>] epoch # 7, batch # 42: loss: 1.99028993 | val_loss: 2.16171465\n",
            "[>] epoch # 7, batch # 44: loss: 2.08121133 | val_loss: 2.16649710\n",
            "[>] epoch # 7, batch # 46: loss: 1.85432291 | val_loss: 2.15416263\n",
            "[>] epoch # 7, batch # 48: loss: 2.19322419 | val_loss: 2.16400115\n",
            "[>] epoch # 7, batch # 50: loss: 2.17588639 | val_loss: 2.16151342\n",
            "[>] epoch # 7, batch # 52: loss: 1.87223148 | val_loss: 2.15436363\n",
            "[>] epoch # 7, batch # 54: loss: 1.83176947 | val_loss: 2.15133475\n",
            "[>] epoch # 7, batch # 56: loss: 2.08071184 | val_loss: 2.16402985\n",
            "[>] epoch # 7, batch # 58: loss: 1.97578990 | val_loss: 2.13347654\n",
            "[>] epoch # 7, batch # 60: loss: 2.02135134 | val_loss: 2.15396825\n",
            "[>] epoch # 7, batch # 62: loss: 2.06163287 | val_loss: 2.12462259\n",
            "[>] epoch # 7, batch # 64: loss: 1.98725712 | val_loss: 2.14487289\n",
            "[>] epoch # 7, batch # 66: loss: 2.01310158 | val_loss: 2.15037071\n",
            "[>] epoch # 7, batch # 68: loss: 2.00911093 | val_loss: 2.10952098\n",
            "[>] epoch # 7, batch # 70: loss: 1.89677131 | val_loss: 2.11993217\n",
            "[>] epoch # 7, batch # 72: loss: 2.32746768 | val_loss: 2.13015631\n",
            "[>] epoch # 7, batch # 74: loss: 1.91305876 | val_loss: 2.10200854\n",
            "[>] epoch # 7, batch # 76: loss: 1.99143720 | val_loss: 2.10157920\n",
            "[>] epoch # 7, batch # 78: loss: 2.04168296 | val_loss: 2.08710107\n",
            "[>] epoch # 7, batch # 80: loss: 2.05854607 | val_loss: 2.12455606\n",
            "[>] epoch # 7, batch # 82: loss: 1.98188365 | val_loss: 2.09860436\n",
            "[>] epoch # 7, batch # 84: loss: 2.13402152 | val_loss: 2.09393712\n",
            "[>] epoch # 7, batch # 86: loss: 1.61388040 | val_loss: 2.09659091\n",
            "[>] epoch # 7, batch # 88: loss: 1.95667183 | val_loss: 2.07371993\n",
            "[>] epoch # 7, batch # 90: loss: 1.84373653 | val_loss: 2.10286607\n",
            "[>] epoch # 7, batch # 92: loss: 1.86317694 | val_loss: 2.06638652\n",
            "[>] epoch # 7, batch # 94: loss: 1.94218981 | val_loss: 2.06579140\n",
            "[>] epoch # 7, batch # 96: loss: 2.09378910 | val_loss: 2.07624930\n",
            "[>] epoch # 7, batch # 98: loss: 2.09595823 | val_loss: 2.07380152\n",
            "[>] epoch # 7, batch #100: loss: 2.13817573 | val_loss: 2.06338890\n",
            "[>] epoch # 7, batch #102: loss: 1.98782527 | val_loss: 2.06447642\n",
            "[>] epoch # 7, batch #104: loss: 1.81690860 | val_loss: 2.06895786\n",
            "[>] epoch # 7, batch #106: loss: 1.85869241 | val_loss: 2.03199838\n",
            "[>] epoch # 7, batch #108: loss: 2.11577773 | val_loss: 2.07527008\n",
            "[>] epoch # 7, batch #110: loss: 1.81818497 | val_loss: 2.06815842\n",
            "[>] epoch # 7, batch #112: loss: 2.02656150 | val_loss: 2.06321719\n",
            "[>] epoch # 7, batch #114: loss: 1.74254012 | val_loss: 2.04818124\n",
            "[>] epoch # 7, batch #116: loss: 1.92637372 | val_loss: 2.06590598\n",
            "[>] epoch # 7, batch #118: loss: 2.04915762 | val_loss: 2.06152321\n",
            "[>] epoch # 7, batch #120: loss: 1.81542444 | val_loss: 2.03246270\n",
            "[>] epoch # 7, batch #122: loss: 2.09889627 | val_loss: 2.07320489\n",
            "[>] epoch # 7, batch #124: loss: 1.94795418 | val_loss: 2.01911913\n",
            "[>] epoch # 7, batch #126: loss: 1.78937411 | val_loss: 2.02358435\n",
            "[>] epoch # 7, batch #128: loss: 1.84498465 | val_loss: 2.02227244\n",
            "[>] epoch # 7, batch #130: loss: 2.11889482 | val_loss: 2.00701545\n",
            "[>] epoch # 7, batch #132: loss: 1.88139474 | val_loss: 2.02798734\n",
            "[>] epoch # 7, batch #134: loss: 1.67579973 | val_loss: 1.99481429\n",
            "[>] epoch # 7, batch #136: loss: 1.78234339 | val_loss: 1.98713389\n",
            "[>] epoch # 7, batch #138: loss: 1.98535049 | val_loss: 2.01528111\n",
            "[>] epoch # 7, batch #140: loss: 1.92520738 | val_loss: 2.00648946\n",
            "[>] epoch # 8, batch #  1: loss: 1.73761654 | val_loss: 2.00177092\n",
            "[>] epoch # 8, batch #  3: loss: 1.89646947 | val_loss: 1.99123724\n",
            "[>] epoch # 8, batch #  5: loss: 2.00058222 | val_loss: 2.00095967\n",
            "[>] epoch # 8, batch #  7: loss: 1.89310622 | val_loss: 1.99966018\n",
            "[>] epoch # 8, batch #  9: loss: 1.83907425 | val_loss: 1.98471582\n",
            "[>] epoch # 8, batch # 11: loss: 1.86830759 | val_loss: 2.00347304\n",
            "[>] epoch # 8, batch # 13: loss: 1.90816629 | val_loss: 1.98556308\n",
            "[>] epoch # 8, batch # 15: loss: 1.86310577 | val_loss: 1.97726449\n",
            "[>] epoch # 8, batch # 17: loss: 1.88057649 | val_loss: 1.95964125\n",
            "[>] epoch # 8, batch # 19: loss: 1.99983120 | val_loss: 1.97615494\n",
            "[>] epoch # 8, batch # 21: loss: 1.87169564 | val_loss: 1.96650789\n",
            "[>] epoch # 8, batch # 23: loss: 1.70046580 | val_loss: 1.98173296\n",
            "[>] epoch # 8, batch # 25: loss: 1.85062897 | val_loss: 1.97018621\n",
            "[>] epoch # 8, batch # 27: loss: 1.77822173 | val_loss: 1.95654982\n",
            "[>] epoch # 8, batch # 29: loss: 1.74309421 | val_loss: 1.97293936\n",
            "[>] epoch # 8, batch # 31: loss: 1.94622266 | val_loss: 1.96664818\n",
            "[>] epoch # 8, batch # 33: loss: 1.79001546 | val_loss: 1.95387972\n",
            "[>] epoch # 8, batch # 35: loss: 1.54098761 | val_loss: 1.94921906\n",
            "[>] epoch # 8, batch # 37: loss: 1.79873443 | val_loss: 1.93786987\n",
            "[>] epoch # 8, batch # 39: loss: 1.59827721 | val_loss: 1.94849386\n",
            "[>] epoch # 8, batch # 41: loss: 1.59257615 | val_loss: 1.92846733\n",
            "[>] epoch # 8, batch # 43: loss: 1.78989804 | val_loss: 1.91262504\n",
            "[>] epoch # 8, batch # 45: loss: 1.96691895 | val_loss: 1.95041883\n",
            "[>] epoch # 8, batch # 47: loss: 1.68809450 | val_loss: 1.92903201\n",
            "[>] epoch # 8, batch # 49: loss: 1.74558473 | val_loss: 1.94305002\n",
            "[>] epoch # 8, batch # 51: loss: 1.77017939 | val_loss: 1.92937052\n",
            "[>] epoch # 8, batch # 53: loss: 1.69580150 | val_loss: 1.93394802\n",
            "[>] epoch # 8, batch # 55: loss: 2.06684923 | val_loss: 1.93011535\n",
            "[>] epoch # 8, batch # 57: loss: 1.83848619 | val_loss: 1.91901398\n",
            "[>] epoch # 8, batch # 59: loss: 1.64999259 | val_loss: 1.94461679\n",
            "[>] epoch # 8, batch # 61: loss: 1.82442677 | val_loss: 1.92276025\n",
            "[>] epoch # 8, batch # 63: loss: 1.76455498 | val_loss: 1.94064833\n",
            "[>] epoch # 8, batch # 65: loss: 1.82788086 | val_loss: 1.95871586\n",
            "[>] epoch # 8, batch # 67: loss: 1.66756988 | val_loss: 1.89462280\n",
            "[>] epoch # 8, batch # 69: loss: 1.81772757 | val_loss: 1.92305959\n",
            "[>] epoch # 8, batch # 71: loss: 1.65072536 | val_loss: 1.91797269\n",
            "[>] epoch # 8, batch # 73: loss: 1.79029369 | val_loss: 1.92957150\n",
            "[>] epoch # 8, batch # 75: loss: 1.65115821 | val_loss: 1.89044075\n",
            "[>] epoch # 8, batch # 77: loss: 1.69004703 | val_loss: 1.89836920\n",
            "[>] epoch # 8, batch # 79: loss: 1.81879508 | val_loss: 1.89319164\n",
            "[>] epoch # 8, batch # 81: loss: 1.77581728 | val_loss: 1.89090509\n",
            "[>] epoch # 8, batch # 83: loss: 1.71144521 | val_loss: 1.88319397\n",
            "[>] epoch # 8, batch # 85: loss: 1.51000118 | val_loss: 1.86486480\n",
            "[>] epoch # 8, batch # 87: loss: 1.73687208 | val_loss: 1.87907828\n",
            "[>] epoch # 8, batch # 89: loss: 1.82286775 | val_loss: 1.87498500\n",
            "[>] epoch # 8, batch # 91: loss: 1.47769129 | val_loss: 1.89743237\n",
            "[>] epoch # 8, batch # 93: loss: 1.65475357 | val_loss: 1.88612722\n",
            "[>] epoch # 8, batch # 95: loss: 1.74364090 | val_loss: 1.86986274\n",
            "[>] epoch # 8, batch # 97: loss: 1.97762954 | val_loss: 1.87195009\n",
            "[>] epoch # 8, batch # 99: loss: 1.88814008 | val_loss: 1.87041766\n",
            "[>] epoch # 8, batch #101: loss: 1.84007335 | val_loss: 1.84703726\n",
            "[>] epoch # 8, batch #103: loss: 1.68329489 | val_loss: 1.84538875\n",
            "[>] epoch # 8, batch #105: loss: 1.66119778 | val_loss: 1.85090390\n",
            "[>] epoch # 8, batch #107: loss: 1.55647039 | val_loss: 1.85789198\n",
            "[>] epoch # 8, batch #109: loss: 1.73526895 | val_loss: 1.83632742\n",
            "[>] epoch # 8, batch #111: loss: 1.76931846 | val_loss: 1.83894104\n",
            "[>] epoch # 8, batch #113: loss: 1.65403235 | val_loss: 1.84764816\n",
            "[>] epoch # 8, batch #115: loss: 1.80408657 | val_loss: 1.84255244\n",
            "[>] epoch # 8, batch #117: loss: 1.69571912 | val_loss: 1.82284450\n",
            "[>] epoch # 8, batch #119: loss: 1.63364947 | val_loss: 1.84911490\n",
            "[>] epoch # 8, batch #121: loss: 1.70987070 | val_loss: 1.84092859\n",
            "[>] epoch # 8, batch #123: loss: 1.69086075 | val_loss: 1.82663711\n",
            "[>] epoch # 8, batch #125: loss: 1.67552912 | val_loss: 1.81297940\n",
            "[>] epoch # 8, batch #127: loss: 1.65672338 | val_loss: 1.82797470\n",
            "[>] epoch # 8, batch #129: loss: 1.76615691 | val_loss: 1.80188555\n",
            "[>] epoch # 8, batch #131: loss: 1.70829999 | val_loss: 1.78156358\n",
            "[>] epoch # 8, batch #133: loss: 1.53130758 | val_loss: 1.81214252\n",
            "[>] epoch # 8, batch #135: loss: 1.64545405 | val_loss: 1.81022383\n",
            "[>] epoch # 8, batch #137: loss: 1.50980842 | val_loss: 1.78370472\n",
            "[>] epoch # 8, batch #139: loss: 1.72718132 | val_loss: 1.80520625\n",
            "[>] epoch # 8, batch #141: loss: 1.43773687 | val_loss: 1.79926422\n",
            "[>] epoch # 9, batch #  2: loss: 1.49583197 | val_loss: 1.82571084\n",
            "[>] epoch # 9, batch #  4: loss: 1.90610814 | val_loss: 1.81348953\n",
            "[>] epoch # 9, batch #  6: loss: 1.45728302 | val_loss: 1.78953947\n",
            "[>] epoch # 9, batch #  8: loss: 1.56895280 | val_loss: 1.77917120\n",
            "[>] epoch # 9, batch # 10: loss: 1.67032552 | val_loss: 1.80236937\n",
            "[>] epoch # 9, batch # 12: loss: 1.46227169 | val_loss: 1.77229802\n",
            "[>] epoch # 9, batch # 14: loss: 1.50180352 | val_loss: 1.78524518\n",
            "[>] epoch # 9, batch # 16: loss: 1.54685163 | val_loss: 1.75011858\n",
            "[>] epoch # 9, batch # 18: loss: 1.47195137 | val_loss: 1.75686876\n",
            "[>] epoch # 9, batch # 20: loss: 1.52011836 | val_loss: 1.76766095\n",
            "[>] epoch # 9, batch # 22: loss: 1.52481580 | val_loss: 1.78082412\n",
            "[>] epoch # 9, batch # 24: loss: 1.63001692 | val_loss: 1.77386953\n",
            "[>] epoch # 9, batch # 26: loss: 1.51553810 | val_loss: 1.78185153\n",
            "[>] epoch # 9, batch # 28: loss: 1.64710081 | val_loss: 1.77842438\n",
            "[>] epoch # 9, batch # 30: loss: 1.71234679 | val_loss: 1.75971888\n",
            "[>] epoch # 9, batch # 32: loss: 1.65851784 | val_loss: 1.75269594\n",
            "[>] epoch # 9, batch # 34: loss: 1.72870731 | val_loss: 1.74934771\n",
            "[>] epoch # 9, batch # 36: loss: 1.42199218 | val_loss: 1.73655857\n",
            "[>] epoch # 9, batch # 38: loss: 1.57306099 | val_loss: 1.74105807\n",
            "[>] epoch # 9, batch # 40: loss: 1.72235096 | val_loss: 1.74132728\n",
            "[>] epoch # 9, batch # 42: loss: 1.44170010 | val_loss: 1.75974452\n",
            "[>] epoch # 9, batch # 44: loss: 1.54769087 | val_loss: 1.73930433\n",
            "[>] epoch # 9, batch # 46: loss: 1.43743718 | val_loss: 1.72446543\n",
            "[>] epoch # 9, batch # 48: loss: 1.58871961 | val_loss: 1.75300007\n",
            "[>] epoch # 9, batch # 50: loss: 1.42520344 | val_loss: 1.76709481\n",
            "[>] epoch # 9, batch # 52: loss: 1.73889971 | val_loss: 1.71972205\n",
            "[>] epoch # 9, batch # 54: loss: 1.59967411 | val_loss: 1.72139149\n",
            "[>] epoch # 9, batch # 56: loss: 1.43640685 | val_loss: 1.73433026\n",
            "[>] epoch # 9, batch # 58: loss: 1.50400174 | val_loss: 1.77552833\n",
            "[>] epoch # 9, batch # 60: loss: 1.38298106 | val_loss: 1.72462097\n",
            "[>] epoch # 9, batch # 62: loss: 1.63299239 | val_loss: 1.72540469\n",
            "[>] epoch # 9, batch # 64: loss: 1.32087135 | val_loss: 1.79434955\n",
            "[>] epoch # 9, batch # 66: loss: 1.54161143 | val_loss: 1.73308056\n",
            "[>] epoch # 9, batch # 68: loss: 1.47511590 | val_loss: 1.71417274\n",
            "[>] epoch # 9, batch # 70: loss: 1.51541245 | val_loss: 1.73881136\n",
            "[>] epoch # 9, batch # 72: loss: 1.57977057 | val_loss: 1.70842942\n",
            "[>] epoch # 9, batch # 74: loss: 1.65880430 | val_loss: 1.71381972\n",
            "[>] epoch # 9, batch # 76: loss: 1.48387575 | val_loss: 1.71723595\n",
            "[>] epoch # 9, batch # 78: loss: 1.52340424 | val_loss: 1.69954934\n",
            "[>] epoch # 9, batch # 80: loss: 1.73217523 | val_loss: 1.71203641\n",
            "[>] epoch # 9, batch # 82: loss: 1.51788080 | val_loss: 1.72374161\n",
            "[>] epoch # 9, batch # 84: loss: 1.61677408 | val_loss: 1.66861129\n",
            "[>] epoch # 9, batch # 86: loss: 1.47867203 | val_loss: 1.73637154\n",
            "[>] epoch # 9, batch # 88: loss: 1.43087494 | val_loss: 1.71408822\n",
            "[>] epoch # 9, batch # 90: loss: 1.50768411 | val_loss: 1.66998352\n",
            "[>] epoch # 9, batch # 92: loss: 1.38695621 | val_loss: 1.70074745\n",
            "[>] epoch # 9, batch # 94: loss: 1.64289415 | val_loss: 1.70132649\n",
            "[>] epoch # 9, batch # 96: loss: 1.42857122 | val_loss: 1.67623609\n",
            "[>] epoch # 9, batch # 98: loss: 1.60177970 | val_loss: 1.69104214\n",
            "[>] epoch # 9, batch #100: loss: 1.52435958 | val_loss: 1.66287483\n",
            "[>] epoch # 9, batch #102: loss: 1.45639455 | val_loss: 1.64454752\n",
            "[>] epoch # 9, batch #104: loss: 1.66733837 | val_loss: 1.67180920\n",
            "[>] epoch # 9, batch #106: loss: 1.28689826 | val_loss: 1.67386602\n",
            "[>] epoch # 9, batch #108: loss: 1.72519314 | val_loss: 1.63285498\n",
            "[>] epoch # 9, batch #110: loss: 1.56870413 | val_loss: 1.65870092\n",
            "[>] epoch # 9, batch #112: loss: 1.42749357 | val_loss: 1.64523211\n",
            "[>] epoch # 9, batch #114: loss: 1.52825522 | val_loss: 1.67254707\n",
            "[>] epoch # 9, batch #116: loss: 1.64732957 | val_loss: 1.64346107\n",
            "[>] epoch # 9, batch #118: loss: 1.40332258 | val_loss: 1.62579562\n",
            "[>] epoch # 9, batch #120: loss: 1.62753332 | val_loss: 1.62802865\n",
            "[>] epoch # 9, batch #122: loss: 1.55996680 | val_loss: 1.63986704\n",
            "[>] epoch # 9, batch #124: loss: 1.87152600 | val_loss: 1.63633794\n",
            "[>] epoch # 9, batch #126: loss: 1.32776558 | val_loss: 1.64065927\n",
            "[>] epoch # 9, batch #128: loss: 1.46358609 | val_loss: 1.61405957\n",
            "[>] epoch # 9, batch #130: loss: 1.60515463 | val_loss: 1.62425489\n",
            "[>] epoch # 9, batch #132: loss: 1.46899724 | val_loss: 1.65404497\n",
            "[>] epoch # 9, batch #134: loss: 1.47708666 | val_loss: 1.62512274\n",
            "[>] epoch # 9, batch #136: loss: 1.58542168 | val_loss: 1.63390029\n",
            "[>] epoch # 9, batch #138: loss: 1.48682165 | val_loss: 1.62175699\n",
            "[>] epoch # 9, batch #140: loss: 1.27217650 | val_loss: 1.61936789\n",
            "[>] epoch #10, batch #  1: loss: 1.48205471 | val_loss: 1.62006971\n",
            "[>] epoch #10, batch #  3: loss: 1.42864859 | val_loss: 1.62912805\n",
            "[>] epoch #10, batch #  5: loss: 1.27406013 | val_loss: 1.62002659\n",
            "[>] epoch #10, batch #  7: loss: 1.26594055 | val_loss: 1.62330949\n",
            "[>] epoch #10, batch #  9: loss: 1.33147442 | val_loss: 1.60893327\n",
            "[>] epoch #10, batch # 11: loss: 1.41622305 | val_loss: 1.59406263\n",
            "[>] epoch #10, batch # 13: loss: 1.40452290 | val_loss: 1.59844978\n",
            "[>] epoch #10, batch # 15: loss: 1.37925601 | val_loss: 1.61366504\n",
            "[>] epoch #10, batch # 17: loss: 1.36656106 | val_loss: 1.59566211\n",
            "[>] epoch #10, batch # 19: loss: 1.62782836 | val_loss: 1.60413143\n",
            "[>] epoch #10, batch # 21: loss: 1.22796166 | val_loss: 1.61045339\n",
            "[>] epoch #10, batch # 23: loss: 1.30727220 | val_loss: 1.59389525\n",
            "[>] epoch #10, batch # 25: loss: 1.46087945 | val_loss: 1.59157533\n",
            "[>] epoch #10, batch # 27: loss: 1.38416004 | val_loss: 1.57762573\n",
            "[>] epoch #10, batch # 29: loss: 1.38781500 | val_loss: 1.58606242\n",
            "[>] epoch #10, batch # 31: loss: 1.48837113 | val_loss: 1.57697042\n",
            "[>] epoch #10, batch # 33: loss: 1.26940072 | val_loss: 1.59992769\n",
            "[>] epoch #10, batch # 35: loss: 1.50848699 | val_loss: 1.58751059\n",
            "[>] epoch #10, batch # 37: loss: 1.30243051 | val_loss: 1.56507258\n",
            "[>] epoch #10, batch # 39: loss: 1.28449738 | val_loss: 1.57360315\n",
            "[>] epoch #10, batch # 41: loss: 1.41841710 | val_loss: 1.58871482\n",
            "[>] epoch #10, batch # 43: loss: 1.19682622 | val_loss: 1.58643675\n",
            "[>] epoch #10, batch # 45: loss: 1.27698064 | val_loss: 1.56741031\n",
            "[>] epoch #10, batch # 47: loss: 1.32951796 | val_loss: 1.57893825\n",
            "[>] epoch #10, batch # 49: loss: 1.44213772 | val_loss: 1.58759064\n",
            "[>] epoch #10, batch # 51: loss: 1.42454040 | val_loss: 1.57164542\n",
            "[>] epoch #10, batch # 53: loss: 1.47622013 | val_loss: 1.53928351\n",
            "[>] epoch #10, batch # 55: loss: 1.32633805 | val_loss: 1.56242720\n",
            "[>] epoch #10, batch # 57: loss: 1.26555753 | val_loss: 1.54884239\n",
            "[>] epoch #10, batch # 59: loss: 1.38651264 | val_loss: 1.56426579\n",
            "[>] epoch #10, batch # 61: loss: 1.38138175 | val_loss: 1.56123070\n",
            "[>] epoch #10, batch # 63: loss: 1.35656428 | val_loss: 1.58022913\n",
            "[>] epoch #10, batch # 65: loss: 1.45975959 | val_loss: 1.54510338\n",
            "[>] epoch #10, batch # 67: loss: 1.48264956 | val_loss: 1.53447818\n",
            "[>] epoch #10, batch # 69: loss: 1.38793755 | val_loss: 1.53394382\n",
            "[>] epoch #10, batch # 71: loss: 1.44393325 | val_loss: 1.55680624\n",
            "[>] epoch #10, batch # 73: loss: 1.33059633 | val_loss: 1.51899831\n",
            "[>] epoch #10, batch # 75: loss: 1.27418923 | val_loss: 1.57417725\n",
            "[>] epoch #10, batch # 77: loss: 1.44961953 | val_loss: 1.54570689\n",
            "[>] epoch #10, batch # 79: loss: 1.51944101 | val_loss: 1.54360338\n",
            "[>] epoch #10, batch # 81: loss: 1.41557956 | val_loss: 1.53674337\n",
            "[>] epoch #10, batch # 83: loss: 1.48786056 | val_loss: 1.53719091\n",
            "[>] epoch #10, batch # 85: loss: 1.37778509 | val_loss: 1.53890248\n",
            "[>] epoch #10, batch # 87: loss: 1.22273850 | val_loss: 1.52226422\n",
            "[>] epoch #10, batch # 89: loss: 1.12780130 | val_loss: 1.54858571\n",
            "[>] epoch #10, batch # 91: loss: 1.27256620 | val_loss: 1.54332029\n",
            "[>] epoch #10, batch # 93: loss: 1.31844354 | val_loss: 1.54390800\n",
            "[>] epoch #10, batch # 95: loss: 1.54611337 | val_loss: 1.54513871\n",
            "[>] epoch #10, batch # 97: loss: 1.40348029 | val_loss: 1.49155242\n",
            "[>] epoch #10, batch # 99: loss: 1.21771240 | val_loss: 1.52593877\n",
            "[>] epoch #10, batch #101: loss: 1.36123109 | val_loss: 1.52697841\n",
            "[>] epoch #10, batch #103: loss: 1.24163032 | val_loss: 1.52436866\n",
            "[>] epoch #10, batch #105: loss: 1.29365051 | val_loss: 1.50908792\n",
            "[>] epoch #10, batch #107: loss: 1.36460829 | val_loss: 1.52664721\n",
            "[>] epoch #10, batch #109: loss: 1.23568714 | val_loss: 1.51055828\n",
            "[>] epoch #10, batch #111: loss: 1.18389046 | val_loss: 1.51697724\n",
            "[>] epoch #10, batch #113: loss: 1.29264832 | val_loss: 1.48612672\n",
            "[>] epoch #10, batch #115: loss: 1.19637203 | val_loss: 1.48559497\n",
            "[>] epoch #10, batch #117: loss: 1.35702932 | val_loss: 1.52597264\n",
            "[>] epoch #10, batch #119: loss: 1.39185011 | val_loss: 1.48277318\n",
            "[>] epoch #10, batch #121: loss: 1.32439899 | val_loss: 1.49391148\n",
            "[>] epoch #10, batch #123: loss: 1.52013588 | val_loss: 1.47111846\n",
            "[>] epoch #10, batch #125: loss: 1.60530460 | val_loss: 1.49922217\n",
            "[>] epoch #10, batch #127: loss: 1.33019066 | val_loss: 1.49655794\n",
            "[>] epoch #10, batch #129: loss: 1.35828793 | val_loss: 1.47935596\n",
            "[>] epoch #10, batch #131: loss: 1.40373290 | val_loss: 1.49553967\n",
            "[>] epoch #10, batch #133: loss: 1.55658579 | val_loss: 1.48604537\n",
            "[>] epoch #10, batch #135: loss: 1.29026020 | val_loss: 1.48971987\n",
            "[>] epoch #10, batch #137: loss: 1.24416041 | val_loss: 1.48758328\n",
            "[>] epoch #10, batch #139: loss: 1.43782938 | val_loss: 1.49296340\n",
            "[>] epoch #10, batch #141: loss: 1.43335140 | val_loss: 1.49254885\n",
            "[>] epoch #11, batch #  2: loss: 1.16887355 | val_loss: 1.50969195\n",
            "[>] epoch #11, batch #  4: loss: 1.33675587 | val_loss: 1.50454074\n",
            "[>] epoch #11, batch #  6: loss: 1.14875042 | val_loss: 1.49931727\n",
            "[>] epoch #11, batch #  8: loss: 1.23890924 | val_loss: 1.50277421\n",
            "[>] epoch #11, batch # 10: loss: 1.12152171 | val_loss: 1.48301831\n",
            "[>] epoch #11, batch # 12: loss: 1.32381976 | val_loss: 1.50221009\n",
            "[>] epoch #11, batch # 14: loss: 1.10346389 | val_loss: 1.48127034\n",
            "[>] epoch #11, batch # 16: loss: 1.29684401 | val_loss: 1.47378050\n",
            "[>] epoch #11, batch # 18: loss: 1.05196309 | val_loss: 1.46041490\n",
            "[>] epoch #11, batch # 20: loss: 1.13678777 | val_loss: 1.44667145\n",
            "[>] epoch #11, batch # 22: loss: 1.18891251 | val_loss: 1.45622603\n",
            "[>] epoch #11, batch # 24: loss: 0.96014017 | val_loss: 1.48984133\n",
            "[>] epoch #11, batch # 26: loss: 1.22020137 | val_loss: 1.46097054\n",
            "[>] epoch #11, batch # 28: loss: 1.31369603 | val_loss: 1.48738393\n",
            "[>] epoch #11, batch # 30: loss: 1.23377597 | val_loss: 1.47078517\n",
            "[>] epoch #11, batch # 32: loss: 1.17929649 | val_loss: 1.46706528\n",
            "[>] epoch #11, batch # 34: loss: 1.25375211 | val_loss: 1.49002858\n",
            "[>] epoch #11, batch # 36: loss: 1.21406686 | val_loss: 1.46827497\n",
            "[>] epoch #11, batch # 38: loss: 1.20928240 | val_loss: 1.46227226\n",
            "[>] epoch #11, batch # 40: loss: 1.35116243 | val_loss: 1.46821492\n",
            "[>] epoch #11, batch # 42: loss: 1.14553571 | val_loss: 1.46297799\n",
            "[>] epoch #11, batch # 44: loss: 1.07547736 | val_loss: 1.45145727\n",
            "[>] epoch #11, batch # 46: loss: 1.04538059 | val_loss: 1.43026253\n",
            "[>] epoch #11, batch # 48: loss: 1.13216853 | val_loss: 1.43194777\n",
            "[>] epoch #11, batch # 50: loss: 1.30524766 | val_loss: 1.46433787\n",
            "[>] epoch #11, batch # 52: loss: 1.31230438 | val_loss: 1.44086277\n",
            "[>] epoch #11, batch # 54: loss: 1.07724690 | val_loss: 1.44225747\n",
            "[>] epoch #11, batch # 56: loss: 1.10350156 | val_loss: 1.44106493\n",
            "[>] epoch #11, batch # 58: loss: 1.12104344 | val_loss: 1.42517392\n",
            "[>] epoch #11, batch # 60: loss: 1.46227336 | val_loss: 1.42784059\n",
            "[>] epoch #11, batch # 62: loss: 1.17283046 | val_loss: 1.40921186\n",
            "[>] epoch #11, batch # 64: loss: 1.32816720 | val_loss: 1.40409511\n",
            "[>] epoch #11, batch # 66: loss: 1.14640343 | val_loss: 1.41258052\n",
            "[>] epoch #11, batch # 68: loss: 1.17303431 | val_loss: 1.41613869\n",
            "[>] epoch #11, batch # 70: loss: 1.27988493 | val_loss: 1.42961554\n",
            "[>] epoch #11, batch # 72: loss: 1.00736213 | val_loss: 1.42275432\n",
            "[>] epoch #11, batch # 74: loss: 1.17422271 | val_loss: 1.41774898\n",
            "[>] epoch #11, batch # 76: loss: 1.16006339 | val_loss: 1.42926924\n",
            "[>] epoch #11, batch # 78: loss: 1.00286067 | val_loss: 1.43337284\n",
            "[>] epoch #11, batch # 80: loss: 1.25295413 | val_loss: 1.44413005\n",
            "[>] epoch #11, batch # 82: loss: 1.20200157 | val_loss: 1.42855577\n",
            "[>] epoch #11, batch # 84: loss: 1.41279387 | val_loss: 1.43285393\n",
            "[>] epoch #11, batch # 86: loss: 1.08507752 | val_loss: 1.42200979\n",
            "[>] epoch #11, batch # 88: loss: 1.14542758 | val_loss: 1.41774764\n",
            "[>] epoch #11, batch # 90: loss: 1.23989570 | val_loss: 1.41814297\n",
            "[>] epoch #11, batch # 92: loss: 1.11964285 | val_loss: 1.39105614\n",
            "[>] epoch #11, batch # 94: loss: 1.25985277 | val_loss: 1.43785329\n",
            "[>] epoch #11, batch # 96: loss: 1.11654007 | val_loss: 1.43044603\n",
            "[>] epoch #11, batch # 98: loss: 1.22512424 | val_loss: 1.39354280\n",
            "[>] epoch #11, batch #100: loss: 1.27685702 | val_loss: 1.37891509\n",
            "[>] epoch #11, batch #102: loss: 1.12509716 | val_loss: 1.37418567\n",
            "[>] epoch #11, batch #104: loss: 1.21749294 | val_loss: 1.40680122\n",
            "[>] epoch #11, batch #106: loss: 1.11013746 | val_loss: 1.40347502\n",
            "[>] epoch #11, batch #108: loss: 1.33536339 | val_loss: 1.43243225\n",
            "[>] epoch #11, batch #110: loss: 1.22783375 | val_loss: 1.40773771\n",
            "[>] epoch #11, batch #112: loss: 1.24518979 | val_loss: 1.37737551\n",
            "[>] epoch #11, batch #114: loss: 1.41405237 | val_loss: 1.36673557\n",
            "[>] epoch #11, batch #116: loss: 1.28166163 | val_loss: 1.36351389\n",
            "[>] epoch #11, batch #118: loss: 1.05448830 | val_loss: 1.37203849\n",
            "[>] epoch #11, batch #120: loss: 1.21743047 | val_loss: 1.37714139\n",
            "[>] epoch #11, batch #122: loss: 1.08163571 | val_loss: 1.36255001\n",
            "[>] epoch #11, batch #124: loss: 1.08744335 | val_loss: 1.35505188\n",
            "[>] epoch #11, batch #126: loss: 0.94638783 | val_loss: 1.36388470\n",
            "[>] epoch #11, batch #128: loss: 1.13847566 | val_loss: 1.35470449\n",
            "[>] epoch #11, batch #130: loss: 1.30249155 | val_loss: 1.36817689\n",
            "[>] epoch #11, batch #132: loss: 1.14612031 | val_loss: 1.35293697\n",
            "[>] epoch #11, batch #134: loss: 1.13818598 | val_loss: 1.35261861\n",
            "[>] epoch #11, batch #136: loss: 1.08742321 | val_loss: 1.35994429\n",
            "[>] epoch #11, batch #138: loss: 1.16568995 | val_loss: 1.36038277\n",
            "[>] epoch #11, batch #140: loss: 1.10859168 | val_loss: 1.35280736\n",
            "[>] epoch #12, batch #  1: loss: 1.04098833 | val_loss: 1.36926082\n",
            "[>] epoch #12, batch #  3: loss: 1.03011394 | val_loss: 1.37538161\n",
            "[>] epoch #12, batch #  5: loss: 0.94385338 | val_loss: 1.34605061\n",
            "[>] epoch #12, batch #  7: loss: 1.13272893 | val_loss: 1.33464008\n",
            "[>] epoch #12, batch #  9: loss: 1.13144612 | val_loss: 1.35455557\n",
            "[>] epoch #12, batch # 11: loss: 0.99713278 | val_loss: 1.35946788\n",
            "[>] epoch #12, batch # 13: loss: 1.25804067 | val_loss: 1.36002216\n",
            "[>] epoch #12, batch # 15: loss: 1.23706305 | val_loss: 1.35373779\n",
            "[>] epoch #12, batch # 17: loss: 1.12346435 | val_loss: 1.33310391\n",
            "[>] epoch #12, batch # 19: loss: 1.13561523 | val_loss: 1.36609301\n",
            "[>] epoch #12, batch # 21: loss: 0.94656050 | val_loss: 1.34571378\n",
            "[>] epoch #12, batch # 23: loss: 1.00968122 | val_loss: 1.33740521\n",
            "[>] epoch #12, batch # 25: loss: 1.05056667 | val_loss: 1.35572566\n",
            "[>] epoch #12, batch # 27: loss: 1.13803637 | val_loss: 1.36490359\n",
            "[>] epoch #12, batch # 29: loss: 0.93263364 | val_loss: 1.35815981\n",
            "[>] epoch #12, batch # 31: loss: 1.04676235 | val_loss: 1.33044711\n",
            "[>] epoch #12, batch # 33: loss: 1.03718805 | val_loss: 1.32817014\n",
            "[>] epoch #12, batch # 35: loss: 1.20932710 | val_loss: 1.32375548\n",
            "[>] epoch #12, batch # 37: loss: 1.06703937 | val_loss: 1.35108022\n",
            "[>] epoch #12, batch # 39: loss: 0.84517926 | val_loss: 1.31983641\n",
            "[>] epoch #12, batch # 41: loss: 1.11904144 | val_loss: 1.34124952\n",
            "[>] epoch #12, batch # 43: loss: 1.15949976 | val_loss: 1.32815410\n",
            "[>] epoch #12, batch # 45: loss: 1.11434984 | val_loss: 1.33499178\n",
            "[>] epoch #12, batch # 47: loss: 0.95489365 | val_loss: 1.33517720\n",
            "[>] epoch #12, batch # 49: loss: 1.08952785 | val_loss: 1.33524135\n",
            "[>] epoch #12, batch # 51: loss: 1.02297235 | val_loss: 1.33653719\n",
            "[>] epoch #12, batch # 53: loss: 0.97971803 | val_loss: 1.32464711\n",
            "[>] epoch #12, batch # 55: loss: 1.20340395 | val_loss: 1.33235001\n",
            "[>] epoch #12, batch # 57: loss: 1.10427082 | val_loss: 1.35178826\n",
            "[>] epoch #12, batch # 59: loss: 1.20297205 | val_loss: 1.34177196\n",
            "[>] epoch #12, batch # 61: loss: 1.15403712 | val_loss: 1.32519780\n",
            "[>] epoch #12, batch # 63: loss: 1.10343158 | val_loss: 1.35812772\n",
            "[>] epoch #12, batch # 65: loss: 0.97383749 | val_loss: 1.33956326\n",
            "[>] epoch #12, batch # 67: loss: 1.01493955 | val_loss: 1.35156320\n",
            "[>] epoch #12, batch # 69: loss: 1.16825390 | val_loss: 1.32036992\n",
            "[>] epoch #12, batch # 71: loss: 1.23059082 | val_loss: 1.30525820\n",
            "[>] epoch #12, batch # 73: loss: 1.17124903 | val_loss: 1.32967430\n",
            "[>] epoch #12, batch # 75: loss: 1.06028318 | val_loss: 1.32444149\n",
            "[>] epoch #12, batch # 77: loss: 1.10341036 | val_loss: 1.31201132\n",
            "[>] epoch #12, batch # 79: loss: 1.27609742 | val_loss: 1.32182217\n",
            "[>] epoch #12, batch # 81: loss: 1.09300601 | val_loss: 1.30344808\n",
            "[>] epoch #12, batch # 83: loss: 0.91474307 | val_loss: 1.31039300\n",
            "[>] epoch #12, batch # 85: loss: 1.05651474 | val_loss: 1.31275824\n",
            "[>] epoch #12, batch # 87: loss: 0.98378080 | val_loss: 1.31331513\n",
            "[>] epoch #12, batch # 89: loss: 1.04563308 | val_loss: 1.28306912\n",
            "[>] epoch #12, batch # 91: loss: 1.09393084 | val_loss: 1.33159769\n",
            "[>] epoch #12, batch # 93: loss: 1.24906778 | val_loss: 1.30069180\n",
            "[>] epoch #12, batch # 95: loss: 1.13360846 | val_loss: 1.30905519\n",
            "[>] epoch #12, batch # 97: loss: 1.10434210 | val_loss: 1.30138222\n",
            "[>] epoch #12, batch # 99: loss: 0.99010336 | val_loss: 1.29506394\n",
            "[>] epoch #12, batch #101: loss: 1.24473751 | val_loss: 1.32768990\n",
            "[>] epoch #12, batch #103: loss: 1.10008550 | val_loss: 1.30186568\n",
            "[>] epoch #12, batch #105: loss: 1.01381719 | val_loss: 1.29143438\n",
            "[>] epoch #12, batch #107: loss: 1.10449469 | val_loss: 1.29841349\n",
            "[>] epoch #12, batch #109: loss: 1.13705647 | val_loss: 1.30707010\n",
            "[>] epoch #12, batch #111: loss: 0.97952485 | val_loss: 1.28322046\n",
            "[>] epoch #12, batch #113: loss: 0.93713009 | val_loss: 1.29582198\n",
            "[>] epoch #12, batch #115: loss: 0.87197107 | val_loss: 1.29806353\n",
            "[>] epoch #12, batch #117: loss: 1.14933717 | val_loss: 1.28561480\n",
            "[>] epoch #12, batch #119: loss: 1.17789745 | val_loss: 1.29339594\n",
            "[>] epoch #12, batch #121: loss: 1.09961033 | val_loss: 1.27240496\n",
            "[>] epoch #12, batch #123: loss: 1.23535597 | val_loss: 1.26777356\n",
            "[>] epoch #12, batch #125: loss: 0.98133320 | val_loss: 1.27745034\n",
            "[>] epoch #12, batch #127: loss: 0.99028367 | val_loss: 1.29038954\n",
            "[>] epoch #12, batch #129: loss: 1.21232021 | val_loss: 1.30669358\n",
            "[>] epoch #12, batch #131: loss: 1.03700078 | val_loss: 1.29232975\n",
            "[>] epoch #12, batch #133: loss: 0.96062148 | val_loss: 1.32846492\n",
            "[>] epoch #12, batch #135: loss: 1.08730328 | val_loss: 1.28328141\n",
            "[>] epoch #12, batch #137: loss: 0.91003352 | val_loss: 1.30273345\n",
            "[>] epoch #12, batch #139: loss: 1.02834034 | val_loss: 1.30241225\n",
            "[>] epoch #12, batch #141: loss: 0.85607910 | val_loss: 1.29607682\n",
            "[>] epoch #13, batch #  2: loss: 1.11888957 | val_loss: 1.28793623\n",
            "[>] epoch #13, batch #  4: loss: 1.11653996 | val_loss: 1.28120656\n",
            "[>] epoch #13, batch #  6: loss: 1.05500448 | val_loss: 1.27268113\n",
            "[>] epoch #13, batch #  8: loss: 0.99073565 | val_loss: 1.27476440\n",
            "[>] epoch #13, batch # 10: loss: 0.94203186 | val_loss: 1.29624721\n",
            "[>] epoch #13, batch # 12: loss: 0.91331482 | val_loss: 1.26449413\n",
            "[>] epoch #13, batch # 14: loss: 1.05385387 | val_loss: 1.27083466\n",
            "[>] epoch #13, batch # 16: loss: 1.04590559 | val_loss: 1.29683830\n",
            "[>] epoch #13, batch # 18: loss: 1.11807334 | val_loss: 1.26722304\n",
            "[>] epoch #13, batch # 20: loss: 1.03441024 | val_loss: 1.25625995\n",
            "[>] epoch #13, batch # 22: loss: 1.03101051 | val_loss: 1.30973025\n",
            "[>] epoch #13, batch # 24: loss: 0.93759525 | val_loss: 1.27582416\n",
            "[>] epoch #13, batch # 26: loss: 1.01559150 | val_loss: 1.25833502\n",
            "[>] epoch #13, batch # 28: loss: 0.88087970 | val_loss: 1.26206512\n",
            "[>] epoch #13, batch # 30: loss: 0.93506300 | val_loss: 1.27042635\n",
            "[>] epoch #13, batch # 32: loss: 0.98324943 | val_loss: 1.25551832\n",
            "[>] epoch #13, batch # 34: loss: 0.98718488 | val_loss: 1.25798921\n",
            "[>] epoch #13, batch # 36: loss: 0.92677820 | val_loss: 1.24374464\n",
            "[>] epoch #13, batch # 38: loss: 0.89360684 | val_loss: 1.25113234\n",
            "[>] epoch #13, batch # 40: loss: 0.91899186 | val_loss: 1.26792094\n",
            "[>] epoch #13, batch # 42: loss: 1.04522324 | val_loss: 1.25945838\n",
            "[>] epoch #13, batch # 44: loss: 0.98084837 | val_loss: 1.24689284\n",
            "[>] epoch #13, batch # 46: loss: 0.87289494 | val_loss: 1.25200604\n",
            "[>] epoch #13, batch # 48: loss: 0.90026098 | val_loss: 1.26311157\n",
            "[>] epoch #13, batch # 50: loss: 0.88562542 | val_loss: 1.29321074\n",
            "[>] epoch #13, batch # 52: loss: 1.00001287 | val_loss: 1.25013177\n",
            "[>] epoch #13, batch # 54: loss: 1.18463445 | val_loss: 1.23528716\n",
            "[>] epoch #13, batch # 56: loss: 0.97187257 | val_loss: 1.27429860\n",
            "[>] epoch #13, batch # 58: loss: 0.80303788 | val_loss: 1.24048780\n",
            "[>] epoch #13, batch # 60: loss: 0.99604177 | val_loss: 1.24914737\n",
            "[>] epoch #13, batch # 62: loss: 1.21873963 | val_loss: 1.25738124\n",
            "[>] epoch #13, batch # 64: loss: 0.94237989 | val_loss: 1.24562612\n",
            "[>] epoch #13, batch # 66: loss: 0.97200316 | val_loss: 1.23833436\n",
            "[>] epoch #13, batch # 68: loss: 1.20777571 | val_loss: 1.28048037\n",
            "[>] epoch #13, batch # 70: loss: 0.89939791 | val_loss: 1.23977170\n",
            "[>] epoch #13, batch # 72: loss: 1.04307199 | val_loss: 1.24707110\n",
            "[>] epoch #13, batch # 74: loss: 0.81323928 | val_loss: 1.22633741\n",
            "[>] epoch #13, batch # 76: loss: 1.12184703 | val_loss: 1.26280221\n",
            "[>] epoch #13, batch # 78: loss: 0.80524212 | val_loss: 1.23692211\n",
            "[>] epoch #13, batch # 80: loss: 1.05994201 | val_loss: 1.22342088\n",
            "[>] epoch #13, batch # 82: loss: 0.93656540 | val_loss: 1.23582767\n",
            "[>] epoch #13, batch # 84: loss: 1.18984747 | val_loss: 1.21213636\n",
            "[>] epoch #13, batch # 86: loss: 1.16567087 | val_loss: 1.21973378\n",
            "[>] epoch #13, batch # 88: loss: 0.81613421 | val_loss: 1.22874574\n",
            "[>] epoch #13, batch # 90: loss: 0.87945592 | val_loss: 1.22333558\n",
            "[>] epoch #13, batch # 92: loss: 0.96013892 | val_loss: 1.22308186\n",
            "[>] epoch #13, batch # 94: loss: 1.12725854 | val_loss: 1.20590147\n",
            "[>] epoch #13, batch # 96: loss: 0.86258191 | val_loss: 1.23391840\n",
            "[>] epoch #13, batch # 98: loss: 0.96643686 | val_loss: 1.21090693\n",
            "[>] epoch #13, batch #100: loss: 0.86152661 | val_loss: 1.22919569\n",
            "[>] epoch #13, batch #102: loss: 1.01094544 | val_loss: 1.24526116\n",
            "[>] epoch #13, batch #104: loss: 1.01317573 | val_loss: 1.20264491\n",
            "[>] epoch #13, batch #106: loss: 1.16160762 | val_loss: 1.20674139\n",
            "[>] epoch #13, batch #108: loss: 0.90463781 | val_loss: 1.22118998\n",
            "[>] epoch #13, batch #110: loss: 1.16003251 | val_loss: 1.22845344\n",
            "[>] epoch #13, batch #112: loss: 0.79198360 | val_loss: 1.20480275\n",
            "[>] epoch #13, batch #114: loss: 0.91087657 | val_loss: 1.24037791\n",
            "[>] epoch #13, batch #116: loss: 0.95487279 | val_loss: 1.22284296\n",
            "[>] epoch #13, batch #118: loss: 0.95663989 | val_loss: 1.20467007\n",
            "[>] epoch #13, batch #120: loss: 0.87609470 | val_loss: 1.21063205\n",
            "[>] epoch #13, batch #122: loss: 0.97676146 | val_loss: 1.21478905\n",
            "[>] epoch #13, batch #124: loss: 0.93345559 | val_loss: 1.19553829\n",
            "[>] epoch #13, batch #126: loss: 0.75855541 | val_loss: 1.19784468\n",
            "[>] epoch #13, batch #128: loss: 0.94307429 | val_loss: 1.21109634\n",
            "[>] epoch #13, batch #130: loss: 0.91357660 | val_loss: 1.22651913\n",
            "[>] epoch #13, batch #132: loss: 0.98399258 | val_loss: 1.21389692\n",
            "[>] epoch #13, batch #134: loss: 0.81363720 | val_loss: 1.22288627\n",
            "[>] epoch #13, batch #136: loss: 0.72633946 | val_loss: 1.24312407\n",
            "[>] epoch #13, batch #138: loss: 0.92335743 | val_loss: 1.22367761\n",
            "[>] epoch #13, batch #140: loss: 0.87287080 | val_loss: 1.20303058\n",
            "[>] epoch #14, batch #  1: loss: 0.90873027 | val_loss: 1.22063741\n",
            "[>] epoch #14, batch #  3: loss: 0.80696136 | val_loss: 1.23285344\n",
            "[>] epoch #14, batch #  5: loss: 0.82430547 | val_loss: 1.20473373\n",
            "[>] epoch #14, batch #  7: loss: 0.66081142 | val_loss: 1.22234620\n",
            "[>] epoch #14, batch #  9: loss: 0.88992035 | val_loss: 1.23385050\n",
            "[>] epoch #14, batch # 11: loss: 0.76060456 | val_loss: 1.21420740\n",
            "[>] epoch #14, batch # 13: loss: 0.73373276 | val_loss: 1.20532476\n",
            "[>] epoch #14, batch # 15: loss: 0.75010455 | val_loss: 1.22243540\n",
            "[>] epoch #14, batch # 17: loss: 0.82206225 | val_loss: 1.20527343\n",
            "[>] epoch #14, batch # 19: loss: 1.04959857 | val_loss: 1.21342557\n",
            "[>] epoch #14, batch # 21: loss: 0.80741346 | val_loss: 1.17569271\n",
            "[>] epoch #14, batch # 23: loss: 0.79405534 | val_loss: 1.19014688\n",
            "[>] epoch #14, batch # 25: loss: 0.68340874 | val_loss: 1.19954315\n",
            "[>] epoch #14, batch # 27: loss: 0.91255671 | val_loss: 1.19967292\n",
            "[>] epoch #14, batch # 29: loss: 0.76608300 | val_loss: 1.19475732\n",
            "[>] epoch #14, batch # 31: loss: 0.74736720 | val_loss: 1.18940180\n",
            "[>] epoch #14, batch # 33: loss: 0.81800431 | val_loss: 1.19169000\n",
            "[>] epoch #14, batch # 35: loss: 0.78820932 | val_loss: 1.17898313\n",
            "[>] epoch #14, batch # 37: loss: 0.92737728 | val_loss: 1.19281831\n",
            "[>] epoch #14, batch # 39: loss: 0.78524494 | val_loss: 1.19131789\n",
            "[>] epoch #14, batch # 41: loss: 0.75790048 | val_loss: 1.20766026\n",
            "[>] epoch #14, batch # 43: loss: 0.85398954 | val_loss: 1.17804189\n",
            "[>] epoch #14, batch # 45: loss: 0.85764486 | val_loss: 1.20091798\n",
            "[>] epoch #14, batch # 47: loss: 0.85386699 | val_loss: 1.18186350\n",
            "[>] epoch #14, batch # 49: loss: 0.96569407 | val_loss: 1.18186985\n",
            "[>] epoch #14, batch # 51: loss: 0.76291597 | val_loss: 1.17433025\n",
            "[>] epoch #14, batch # 53: loss: 0.81260872 | val_loss: 1.16213042\n",
            "[>] epoch #14, batch # 55: loss: 0.86216444 | val_loss: 1.19220073\n",
            "[>] epoch #14, batch # 57: loss: 0.96835709 | val_loss: 1.19026206\n",
            "[>] epoch #14, batch # 59: loss: 0.90519756 | val_loss: 1.18150411\n",
            "[>] epoch #14, batch # 61: loss: 0.67965841 | val_loss: 1.16499357\n",
            "[>] epoch #14, batch # 63: loss: 0.89856964 | val_loss: 1.18917725\n",
            "[>] epoch #14, batch # 65: loss: 0.93762457 | val_loss: 1.19839686\n",
            "[>] epoch #14, batch # 67: loss: 0.81346464 | val_loss: 1.17329036\n",
            "[>] epoch #14, batch # 69: loss: 0.88319868 | val_loss: 1.18015297\n",
            "[>] epoch #14, batch # 71: loss: 0.91192788 | val_loss: 1.20724111\n",
            "[>] epoch #14, batch # 73: loss: 0.98919201 | val_loss: 1.21606062\n",
            "[>] epoch #14, batch # 75: loss: 0.74857724 | val_loss: 1.18117301\n",
            "[>] epoch #14, batch # 77: loss: 1.05856919 | val_loss: 1.18348945\n",
            "[>] epoch #14, batch # 79: loss: 0.77119201 | val_loss: 1.17808673\n",
            "[>] epoch #14, batch # 81: loss: 0.90781659 | val_loss: 1.19243358\n",
            "[>] epoch #14, batch # 83: loss: 0.96486533 | val_loss: 1.19752919\n",
            "[>] epoch #14, batch # 85: loss: 0.94057548 | val_loss: 1.20152121\n",
            "[>] epoch #14, batch # 87: loss: 0.95525402 | val_loss: 1.16174883\n",
            "[>] epoch #14, batch # 89: loss: 0.87071478 | val_loss: 1.17113465\n",
            "[>] epoch #14, batch # 91: loss: 0.77468961 | val_loss: 1.19846936\n",
            "[>] epoch #14, batch # 93: loss: 0.96738577 | val_loss: 1.17609095\n",
            "[>] epoch #14, batch # 95: loss: 0.94526142 | val_loss: 1.17055589\n",
            "[>] epoch #14, batch # 97: loss: 0.81561875 | val_loss: 1.17455345\n",
            "[>] epoch #14, batch # 99: loss: 0.80067092 | val_loss: 1.16901566\n",
            "[>] epoch #14, batch #101: loss: 0.98647934 | val_loss: 1.16545501\n",
            "[>] epoch #14, batch #103: loss: 0.91904736 | val_loss: 1.17157287\n",
            "[>] epoch #14, batch #105: loss: 0.86240613 | val_loss: 1.16793310\n",
            "[>] epoch #14, batch #107: loss: 0.86456215 | val_loss: 1.14762717\n",
            "[>] epoch #14, batch #109: loss: 0.95921063 | val_loss: 1.16811584\n",
            "[>] epoch #14, batch #111: loss: 0.89648062 | val_loss: 1.15693933\n",
            "[>] epoch #14, batch #113: loss: 0.93508750 | val_loss: 1.13760337\n",
            "[>] epoch #14, batch #115: loss: 0.90937430 | val_loss: 1.16574162\n",
            "[>] epoch #14, batch #117: loss: 0.95119244 | val_loss: 1.15176060\n",
            "[>] epoch #14, batch #119: loss: 0.99446332 | val_loss: 1.18023546\n",
            "[>] epoch #14, batch #121: loss: 0.91817665 | val_loss: 1.16175246\n",
            "[>] epoch #14, batch #123: loss: 0.82889903 | val_loss: 1.11073759\n",
            "[>] epoch #14, batch #125: loss: 0.83141154 | val_loss: 1.16248734\n",
            "[>] epoch #14, batch #127: loss: 0.85778087 | val_loss: 1.14777830\n",
            "[>] epoch #14, batch #129: loss: 0.78599566 | val_loss: 1.16208236\n",
            "[>] epoch #14, batch #131: loss: 0.81592953 | val_loss: 1.16469660\n",
            "[>] epoch #14, batch #133: loss: 0.87643373 | val_loss: 1.14887247\n",
            "[>] epoch #14, batch #135: loss: 0.88040411 | val_loss: 1.15105725\n",
            "[>] epoch #14, batch #137: loss: 0.76352262 | val_loss: 1.15141442\n",
            "[>] epoch #14, batch #139: loss: 1.02551615 | val_loss: 1.15016006\n",
            "[>] epoch #14, batch #141: loss: 0.71368349 | val_loss: 1.14120114\n",
            "[>] epoch #15, batch #  2: loss: 0.78811604 | val_loss: 1.14806452\n",
            "[>] epoch #15, batch #  4: loss: 0.91165674 | val_loss: 1.13682647\n",
            "[>] epoch #15, batch #  6: loss: 0.70553362 | val_loss: 1.13327365\n",
            "[>] epoch #15, batch #  8: loss: 0.69226241 | val_loss: 1.14596820\n",
            "[>] epoch #15, batch # 10: loss: 0.95223922 | val_loss: 1.14279405\n",
            "[>] epoch #15, batch # 12: loss: 0.84208441 | val_loss: 1.14024652\n",
            "[>] epoch #15, batch # 14: loss: 0.72237474 | val_loss: 1.17195490\n",
            "[>] epoch #15, batch # 16: loss: 0.74050117 | val_loss: 1.15444706\n",
            "[>] epoch #15, batch # 18: loss: 0.74587429 | val_loss: 1.13757232\n",
            "[>] epoch #15, batch # 20: loss: 0.77598733 | val_loss: 1.13770400\n",
            "[>] epoch #15, batch # 22: loss: 0.79122204 | val_loss: 1.14800147\n",
            "[>] epoch #15, batch # 24: loss: 0.74176180 | val_loss: 1.12062120\n",
            "[>] epoch #15, batch # 26: loss: 0.69942969 | val_loss: 1.14517987\n",
            "[>] epoch #15, batch # 28: loss: 0.82787514 | val_loss: 1.16708553\n",
            "[>] epoch #15, batch # 30: loss: 0.80781859 | val_loss: 1.15166511\n",
            "[>] epoch #15, batch # 32: loss: 0.93748361 | val_loss: 1.13641948\n",
            "[>] epoch #15, batch # 34: loss: 0.88399249 | val_loss: 1.14462337\n",
            "[>] epoch #15, batch # 36: loss: 0.76220131 | val_loss: 1.14375953\n",
            "[>] epoch #15, batch # 38: loss: 0.76086426 | val_loss: 1.12932888\n",
            "[>] epoch #15, batch # 40: loss: 0.94640923 | val_loss: 1.13164446\n",
            "[>] epoch #15, batch # 42: loss: 0.87525171 | val_loss: 1.13099902\n",
            "[>] epoch #15, batch # 44: loss: 0.81012338 | val_loss: 1.14493932\n",
            "[>] epoch #15, batch # 46: loss: 0.75744700 | val_loss: 1.12534141\n",
            "[>] epoch #15, batch # 48: loss: 0.78129089 | val_loss: 1.13722494\n",
            "[>] epoch #15, batch # 50: loss: 0.75083184 | val_loss: 1.11619166\n",
            "[>] epoch #15, batch # 52: loss: 0.67625433 | val_loss: 1.12921665\n",
            "[>] epoch #15, batch # 54: loss: 0.85639733 | val_loss: 1.13013095\n",
            "[>] epoch #15, batch # 56: loss: 0.60647589 | val_loss: 1.11250800\n",
            "[>] epoch #15, batch # 58: loss: 0.80147892 | val_loss: 1.14229651\n",
            "[>] epoch #15, batch # 60: loss: 0.72205758 | val_loss: 1.12371270\n",
            "[>] epoch #15, batch # 62: loss: 0.76744652 | val_loss: 1.12890797\n",
            "[>] epoch #15, batch # 64: loss: 0.82717943 | val_loss: 1.13701711\n",
            "[>] epoch #15, batch # 66: loss: 0.77339762 | val_loss: 1.12558491\n",
            "[>] epoch #15, batch # 68: loss: 0.66973841 | val_loss: 1.10950147\n",
            "[>] epoch #15, batch # 70: loss: 0.82697839 | val_loss: 1.10459148\n",
            "[>] epoch #15, batch # 72: loss: 0.62090790 | val_loss: 1.11758596\n",
            "[>] epoch #15, batch # 74: loss: 0.82097274 | val_loss: 1.12735117\n",
            "[>] epoch #15, batch # 76: loss: 0.82761753 | val_loss: 1.11293135\n",
            "[>] epoch #15, batch # 78: loss: 0.59056836 | val_loss: 1.12174173\n",
            "[>] epoch #15, batch # 80: loss: 0.72070646 | val_loss: 1.11970744\n",
            "[>] epoch #15, batch # 82: loss: 0.82113123 | val_loss: 1.12323761\n",
            "[>] epoch #15, batch # 84: loss: 0.96161675 | val_loss: 1.13856078\n",
            "[>] epoch #15, batch # 86: loss: 0.81034726 | val_loss: 1.11366902\n",
            "[>] epoch #15, batch # 88: loss: 0.99997514 | val_loss: 1.12047314\n",
            "[>] epoch #15, batch # 90: loss: 0.70215601 | val_loss: 1.13918218\n",
            "[>] epoch #15, batch # 92: loss: 0.87407386 | val_loss: 1.10434978\n",
            "[>] epoch #15, batch # 94: loss: 0.74087983 | val_loss: 1.10205561\n",
            "[>] epoch #15, batch # 96: loss: 0.89074272 | val_loss: 1.10829017\n",
            "[>] epoch #15, batch # 98: loss: 0.82991511 | val_loss: 1.10882512\n",
            "[>] epoch #15, batch #100: loss: 0.86916256 | val_loss: 1.08487970\n",
            "[>] epoch #15, batch #102: loss: 0.73563999 | val_loss: 1.11079072\n",
            "[>] epoch #15, batch #104: loss: 0.71977741 | val_loss: 1.10541330\n",
            "[>] epoch #15, batch #106: loss: 0.79795265 | val_loss: 1.11172845\n",
            "[>] epoch #15, batch #108: loss: 0.85172856 | val_loss: 1.11601505\n",
            "[>] epoch #15, batch #110: loss: 0.90168494 | val_loss: 1.10683782\n",
            "[>] epoch #15, batch #112: loss: 0.85101473 | val_loss: 1.11303982\n",
            "[>] epoch #15, batch #114: loss: 0.70423079 | val_loss: 1.11561669\n",
            "[>] epoch #15, batch #116: loss: 0.88216639 | val_loss: 1.10797623\n",
            "[>] epoch #15, batch #118: loss: 0.75236261 | val_loss: 1.09537829\n",
            "[>] epoch #15, batch #120: loss: 0.92696965 | val_loss: 1.08694972\n",
            "[>] epoch #15, batch #122: loss: 0.84252441 | val_loss: 1.10985641\n",
            "[>] epoch #15, batch #124: loss: 0.81111145 | val_loss: 1.11873713\n",
            "[>] epoch #15, batch #126: loss: 0.81242573 | val_loss: 1.10944824\n",
            "[>] epoch #15, batch #128: loss: 0.91981691 | val_loss: 1.11076961\n",
            "[>] epoch #15, batch #130: loss: 0.98225498 | val_loss: 1.12568609\n",
            "[>] epoch #15, batch #132: loss: 0.94967562 | val_loss: 1.09906025\n",
            "[>] epoch #15, batch #134: loss: 0.72496152 | val_loss: 1.10866657\n",
            "[>] epoch #15, batch #136: loss: 0.84004396 | val_loss: 1.11440819\n",
            "[>] epoch #15, batch #138: loss: 0.69166887 | val_loss: 1.10422079\n",
            "[>] epoch #15, batch #140: loss: 0.71457827 | val_loss: 1.15111307\n",
            "[>] epoch #16, batch #  1: loss: 0.80210644 | val_loss: 1.10683490\n",
            "[>] epoch #16, batch #  3: loss: 0.69215631 | val_loss: 1.11396983\n",
            "[>] epoch #16, batch #  5: loss: 0.79866761 | val_loss: 1.11639194\n",
            "[>] epoch #16, batch #  7: loss: 0.81348127 | val_loss: 1.11945455\n",
            "[>] epoch #16, batch #  9: loss: 0.67293930 | val_loss: 1.10184194\n",
            "[>] epoch #16, batch # 11: loss: 0.78287834 | val_loss: 1.10314465\n",
            "[>] epoch #16, batch # 13: loss: 0.62834734 | val_loss: 1.11451213\n",
            "[>] epoch #16, batch # 15: loss: 0.62801886 | val_loss: 1.10582478\n",
            "[>] epoch #16, batch # 17: loss: 0.72864044 | val_loss: 1.10524124\n",
            "[>] epoch #16, batch # 19: loss: 0.76969868 | val_loss: 1.09907381\n",
            "[>] epoch #16, batch # 21: loss: 0.69502640 | val_loss: 1.10590219\n",
            "[>] epoch #16, batch # 23: loss: 0.63301867 | val_loss: 1.08196704\n",
            "[>] epoch #16, batch # 25: loss: 0.68438923 | val_loss: 1.08714473\n",
            "[>] epoch #16, batch # 27: loss: 0.60286450 | val_loss: 1.09167692\n",
            "[>] epoch #16, batch # 29: loss: 0.89273244 | val_loss: 1.09825210\n",
            "[>] epoch #16, batch # 31: loss: 0.65678024 | val_loss: 1.07673048\n",
            "[>] epoch #16, batch # 33: loss: 0.74770242 | val_loss: 1.10784807\n",
            "[>] epoch #16, batch # 35: loss: 0.73740345 | val_loss: 1.09920155\n",
            "[>] epoch #16, batch # 37: loss: 0.78773284 | val_loss: 1.09851198\n",
            "[>] epoch #16, batch # 39: loss: 0.85571259 | val_loss: 1.14036395\n",
            "[>] epoch #16, batch # 41: loss: 0.79152185 | val_loss: 1.12671035\n",
            "[>] epoch #16, batch # 43: loss: 0.73664337 | val_loss: 1.06379057\n",
            "[>] epoch #16, batch # 45: loss: 0.70723224 | val_loss: 1.09847724\n",
            "[>] epoch #16, batch # 47: loss: 0.90205741 | val_loss: 1.06267636\n",
            "[>] epoch #16, batch # 49: loss: 0.81128341 | val_loss: 1.09464078\n",
            "[>] epoch #16, batch # 51: loss: 0.60233170 | val_loss: 1.09544444\n",
            "[>] epoch #16, batch # 53: loss: 0.69136626 | val_loss: 1.08451281\n",
            "[>] epoch #16, batch # 55: loss: 0.70993245 | val_loss: 1.06533637\n",
            "[>] epoch #16, batch # 57: loss: 0.83048189 | val_loss: 1.10147197\n",
            "[>] epoch #16, batch # 59: loss: 0.71247762 | val_loss: 1.09158018\n",
            "[>] epoch #16, batch # 61: loss: 0.73950928 | val_loss: 1.11968982\n",
            "[>] epoch #16, batch # 63: loss: 0.81218934 | val_loss: 1.09630249\n",
            "[>] epoch #16, batch # 65: loss: 0.62795633 | val_loss: 1.10119307\n",
            "[>] epoch #16, batch # 67: loss: 0.67054063 | val_loss: 1.08896681\n",
            "[>] epoch #16, batch # 69: loss: 0.71100265 | val_loss: 1.10509171\n",
            "[>] epoch #16, batch # 71: loss: 0.75383782 | val_loss: 1.10784877\n",
            "[>] epoch #16, batch # 73: loss: 0.87552422 | val_loss: 1.09261540\n",
            "[>] epoch #16, batch # 75: loss: 0.57672250 | val_loss: 1.07325315\n",
            "[>] epoch #16, batch # 77: loss: 0.81259459 | val_loss: 1.06974416\n",
            "[>] epoch #16, batch # 79: loss: 0.91024286 | val_loss: 1.08268050\n",
            "[>] epoch #16, batch # 81: loss: 0.81829190 | val_loss: 1.09441366\n",
            "[>] epoch #16, batch # 83: loss: 0.70929289 | val_loss: 1.09129362\n",
            "[>] epoch #16, batch # 85: loss: 0.78844756 | val_loss: 1.08973023\n",
            "[>] epoch #16, batch # 87: loss: 0.63831836 | val_loss: 1.09307196\n",
            "[>] epoch #16, batch # 89: loss: 0.63522148 | val_loss: 1.10094277\n",
            "[>] epoch #16, batch # 91: loss: 0.88874066 | val_loss: 1.07948295\n",
            "[>] epoch #16, batch # 93: loss: 0.75224513 | val_loss: 1.05667361\n",
            "[>] epoch #16, batch # 95: loss: 0.82984734 | val_loss: 1.05587009\n",
            "[>] epoch #16, batch # 97: loss: 0.76731616 | val_loss: 1.09340004\n",
            "[>] epoch #16, batch # 99: loss: 0.76447535 | val_loss: 1.05116240\n",
            "[>] epoch #16, batch #101: loss: 0.73702079 | val_loss: 1.06342080\n",
            "[>] epoch #16, batch #103: loss: 0.78424668 | val_loss: 1.06037935\n",
            "[>] epoch #16, batch #105: loss: 0.77996296 | val_loss: 1.06658780\n",
            "[>] epoch #16, batch #107: loss: 0.78818381 | val_loss: 1.04670284\n",
            "[>] epoch #16, batch #109: loss: 0.72282726 | val_loss: 1.07289920\n",
            "[>] epoch #16, batch #111: loss: 0.76303983 | val_loss: 1.07444499\n",
            "[>] epoch #16, batch #113: loss: 0.66874897 | val_loss: 1.05957269\n",
            "[>] epoch #16, batch #115: loss: 0.77507168 | val_loss: 1.07426839\n",
            "[>] epoch #16, batch #117: loss: 0.68271387 | val_loss: 1.05382526\n",
            "[>] epoch #16, batch #119: loss: 0.73676217 | val_loss: 1.06472792\n",
            "[>] epoch #16, batch #121: loss: 0.71841085 | val_loss: 1.07445694\n",
            "[>] epoch #16, batch #123: loss: 0.84327441 | val_loss: 1.08095399\n",
            "[>] epoch #16, batch #125: loss: 0.74590868 | val_loss: 1.04390112\n",
            "[>] epoch #16, batch #127: loss: 0.73965847 | val_loss: 1.03958249\n",
            "[>] epoch #16, batch #129: loss: 0.82823485 | val_loss: 1.06651572\n",
            "[>] epoch #16, batch #131: loss: 0.82919991 | val_loss: 1.04617172\n",
            "[>] epoch #16, batch #133: loss: 0.69941711 | val_loss: 1.05190593\n",
            "[>] epoch #16, batch #135: loss: 0.73222411 | val_loss: 1.03639302\n",
            "[>] epoch #16, batch #137: loss: 0.57803243 | val_loss: 1.05541094\n",
            "[>] epoch #16, batch #139: loss: 0.78697979 | val_loss: 1.03828933\n",
            "[>] epoch #16, batch #141: loss: 0.93531823 | val_loss: 1.08164969\n",
            "[>] epoch #17, batch #  2: loss: 0.59071052 | val_loss: 1.05646359\n",
            "[>] epoch #17, batch #  4: loss: 0.64490628 | val_loss: 1.08003579\n",
            "[>] epoch #17, batch #  6: loss: 0.68449348 | val_loss: 1.05856498\n",
            "[>] epoch #17, batch #  8: loss: 0.70808780 | val_loss: 1.07250823\n",
            "[>] epoch #17, batch # 10: loss: 0.63142890 | val_loss: 1.06779196\n",
            "[>] epoch #17, batch # 12: loss: 0.80124289 | val_loss: 1.05763815\n",
            "[>] epoch #17, batch # 14: loss: 0.69365507 | val_loss: 1.08035206\n",
            "[>] epoch #17, batch # 16: loss: 0.53181869 | val_loss: 1.05001782\n",
            "[>] epoch #17, batch # 18: loss: 0.66664100 | val_loss: 1.06132297\n",
            "[>] epoch #17, batch # 20: loss: 0.54964095 | val_loss: 1.05142321\n",
            "[>] epoch #17, batch # 22: loss: 0.53898841 | val_loss: 1.04765428\n",
            "[>] epoch #17, batch # 24: loss: 0.78100455 | val_loss: 1.07988592\n",
            "[>] epoch #17, batch # 26: loss: 0.63995701 | val_loss: 1.02669344\n",
            "[>] epoch #17, batch # 28: loss: 0.70748729 | val_loss: 1.06899029\n",
            "[>] epoch #17, batch # 30: loss: 0.64600062 | val_loss: 1.08218090\n",
            "[>] epoch #17, batch # 32: loss: 0.64484918 | val_loss: 1.07888588\n",
            "[>] epoch #17, batch # 34: loss: 0.61003375 | val_loss: 1.07367422\n",
            "[>] epoch #17, batch # 36: loss: 0.74195468 | val_loss: 1.02040919\n",
            "[>] epoch #17, batch # 38: loss: 0.75616461 | val_loss: 1.04172570\n",
            "[>] epoch #17, batch # 40: loss: 0.62129402 | val_loss: 1.05695752\n",
            "[>] epoch #17, batch # 42: loss: 0.65199387 | val_loss: 1.07151208\n",
            "[>] epoch #17, batch # 44: loss: 0.53309357 | val_loss: 1.04001760\n",
            "[>] epoch #17, batch # 46: loss: 0.70630306 | val_loss: 1.07121089\n",
            "[>] epoch #17, batch # 48: loss: 0.70547777 | val_loss: 1.06146484\n",
            "[>] epoch #17, batch # 50: loss: 0.63256806 | val_loss: 1.04472641\n",
            "[>] epoch #17, batch # 52: loss: 0.71340001 | val_loss: 1.02253323\n",
            "[>] epoch #17, batch # 54: loss: 0.64692855 | val_loss: 1.04666951\n",
            "[>] epoch #17, batch # 56: loss: 0.62361526 | val_loss: 1.04657344\n",
            "[>] epoch #17, batch # 58: loss: 0.62685996 | val_loss: 1.05137608\n",
            "[>] epoch #17, batch # 60: loss: 0.82772082 | val_loss: 1.06355403\n",
            "[>] epoch #17, batch # 62: loss: 0.74219066 | val_loss: 1.05425803\n",
            "[>] epoch #17, batch # 64: loss: 0.64617479 | val_loss: 1.03281294\n",
            "[>] epoch #17, batch # 66: loss: 0.62896109 | val_loss: 1.05142470\n",
            "[>] epoch #17, batch # 68: loss: 0.61910141 | val_loss: 1.05051340\n",
            "[>] epoch #17, batch # 70: loss: 0.65030903 | val_loss: 1.05755988\n",
            "[>] epoch #17, batch # 72: loss: 0.66594255 | val_loss: 1.05986614\n",
            "[>] epoch #17, batch # 74: loss: 0.61973256 | val_loss: 1.02800884\n",
            "[>] epoch #17, batch # 76: loss: 0.81613559 | val_loss: 1.05780407\n",
            "[>] epoch #17, batch # 78: loss: 0.75712490 | val_loss: 1.04478848\n",
            "[>] epoch #17, batch # 80: loss: 0.55913079 | val_loss: 1.03592713\n",
            "[>] epoch #17, batch # 82: loss: 0.53810996 | val_loss: 1.07966307\n",
            "[>] epoch #17, batch # 84: loss: 0.55216032 | val_loss: 1.03900654\n",
            "[>] epoch #17, batch # 86: loss: 0.74908262 | val_loss: 1.04499081\n",
            "[>] epoch #17, batch # 88: loss: 0.62380958 | val_loss: 1.03681878\n",
            "[>] epoch #17, batch # 90: loss: 0.70152384 | val_loss: 1.04545602\n",
            "[>] epoch #17, batch # 92: loss: 0.74673927 | val_loss: 1.03047955\n",
            "[>] epoch #17, batch # 94: loss: 0.64917976 | val_loss: 1.05710453\n",
            "[>] epoch #17, batch # 96: loss: 0.75906128 | val_loss: 1.02371461\n",
            "[>] epoch #17, batch # 98: loss: 0.76761591 | val_loss: 1.04419800\n",
            "[>] epoch #17, batch #100: loss: 0.81270581 | val_loss: 1.06925578\n",
            "[>] epoch #17, batch #102: loss: 0.71952683 | val_loss: 1.05148436\n",
            "[>] epoch #17, batch #104: loss: 0.59126961 | val_loss: 1.05495348\n",
            "[>] epoch #17, batch #106: loss: 0.59021974 | val_loss: 1.06289527\n",
            "[>] epoch #17, batch #108: loss: 0.75692010 | val_loss: 1.06293812\n",
            "[>] epoch #17, batch #110: loss: 0.62359798 | val_loss: 1.05983947\n",
            "[>] epoch #17, batch #112: loss: 0.72561628 | val_loss: 1.07270879\n",
            "[>] epoch #17, batch #114: loss: 0.65837622 | val_loss: 1.02507512\n",
            "[>] epoch #17, batch #116: loss: 0.80515611 | val_loss: 1.04760395\n",
            "[>] epoch #17, batch #118: loss: 0.70427465 | val_loss: 1.00399343\n",
            "[>] epoch #17, batch #120: loss: 0.63866001 | val_loss: 1.04161669\n",
            "[>] epoch #17, batch #122: loss: 0.74350363 | val_loss: 1.06413674\n",
            "[>] epoch #17, batch #124: loss: 0.81089115 | val_loss: 1.02955220\n",
            "[>] epoch #17, batch #126: loss: 0.66805089 | val_loss: 1.04275662\n",
            "[>] epoch #17, batch #128: loss: 0.76479620 | val_loss: 1.03540120\n",
            "[>] epoch #17, batch #130: loss: 0.61944884 | val_loss: 1.05479467\n",
            "[>] epoch #17, batch #132: loss: 0.80240560 | val_loss: 1.05351929\n",
            "[>] epoch #17, batch #134: loss: 0.73184031 | val_loss: 1.02607373\n",
            "[>] epoch #17, batch #136: loss: 0.71441424 | val_loss: 1.03247352\n",
            "[>] epoch #17, batch #138: loss: 0.59911865 | val_loss: 1.02579580\n",
            "[>] epoch #17, batch #140: loss: 0.67481381 | val_loss: 1.04104717\n",
            "[>] epoch #18, batch #  1: loss: 0.66531515 | val_loss: 1.03659010\n",
            "[>] epoch #18, batch #  3: loss: 0.68521696 | val_loss: 1.02667373\n",
            "[>] epoch #18, batch #  5: loss: 0.53086841 | val_loss: 1.04862029\n",
            "[>] epoch #18, batch #  7: loss: 0.65876800 | val_loss: 1.02460214\n",
            "[>] epoch #18, batch #  9: loss: 0.55548847 | val_loss: 1.00837032\n",
            "[>] epoch #18, batch # 11: loss: 0.52199221 | val_loss: 1.04258010\n",
            "[>] epoch #18, batch # 13: loss: 0.55931121 | val_loss: 1.05605071\n",
            "[>] epoch #18, batch # 15: loss: 0.57543457 | val_loss: 1.03832719\n",
            "[>] epoch #18, batch # 17: loss: 0.55670142 | val_loss: 1.02537767\n",
            "[>] epoch #18, batch # 19: loss: 0.72976124 | val_loss: 1.03406816\n",
            "[>] epoch #18, batch # 21: loss: 0.56481403 | val_loss: 1.00527249\n",
            "[>] epoch #18, batch # 23: loss: 0.48264807 | val_loss: 1.03017617\n",
            "[>] epoch #18, batch # 25: loss: 0.47179180 | val_loss: 1.06696872\n",
            "[>] epoch #18, batch # 27: loss: 0.58107197 | val_loss: 1.03692788\n",
            "[>] epoch #18, batch # 29: loss: 0.62334543 | val_loss: 1.04153534\n",
            "[>] epoch #18, batch # 31: loss: 0.66989332 | val_loss: 1.01291336\n",
            "[>] epoch #18, batch # 33: loss: 0.61688197 | val_loss: 1.03171437\n",
            "[>] epoch #18, batch # 35: loss: 0.73845237 | val_loss: 1.06277964\n",
            "[>] epoch #18, batch # 37: loss: 0.59796548 | val_loss: 1.05273534\n",
            "[>] epoch #18, batch # 39: loss: 0.50675553 | val_loss: 1.05835458\n",
            "[>] epoch #18, batch # 41: loss: 0.74594879 | val_loss: 1.05512958\n",
            "[>] epoch #18, batch # 43: loss: 0.50848639 | val_loss: 1.05569810\n",
            "[>] epoch #18, batch # 45: loss: 0.70510763 | val_loss: 1.04475283\n",
            "[>] epoch #18, batch # 47: loss: 0.56226003 | val_loss: 1.05169047\n",
            "[>] epoch #18, batch # 49: loss: 0.66895914 | val_loss: 1.01143693\n",
            "[>] epoch #18, batch # 51: loss: 0.54377496 | val_loss: 1.03521519\n",
            "[>] epoch #18, batch # 53: loss: 0.63286644 | val_loss: 1.03797765\n",
            "[>] epoch #18, batch # 55: loss: 0.64549291 | val_loss: 1.04072221\n",
            "[>] epoch #18, batch # 57: loss: 0.61097819 | val_loss: 1.05775203\n",
            "[>] epoch #18, batch # 59: loss: 0.50655949 | val_loss: 1.02570107\n",
            "[>] epoch #18, batch # 61: loss: 0.67789334 | val_loss: 1.02426243\n",
            "[>] epoch #18, batch # 63: loss: 0.69166875 | val_loss: 1.03096447\n",
            "[>] epoch #18, batch # 65: loss: 0.62149531 | val_loss: 1.03719978\n",
            "[>] epoch #18, batch # 67: loss: 0.58592498 | val_loss: 1.00954648\n",
            "[>] epoch #18, batch # 69: loss: 0.58140719 | val_loss: 1.00795516\n",
            "[>] epoch #18, batch # 71: loss: 0.72026604 | val_loss: 1.05768516\n",
            "[>] epoch #18, batch # 73: loss: 0.61221927 | val_loss: 1.05251972\n",
            "[>] epoch #18, batch # 75: loss: 0.62234539 | val_loss: 1.04164828\n",
            "[>] epoch #18, batch # 77: loss: 0.61563605 | val_loss: 1.01833066\n",
            "[>] epoch #18, batch # 79: loss: 0.67197567 | val_loss: 1.03765793\n",
            "[>] epoch #18, batch # 81: loss: 0.63024861 | val_loss: 1.01283608\n",
            "[>] epoch #18, batch # 83: loss: 0.60571730 | val_loss: 1.01498904\n",
            "[>] epoch #18, batch # 85: loss: 0.45152646 | val_loss: 1.01293452\n",
            "[>] epoch #18, batch # 87: loss: 0.54323792 | val_loss: 1.01499552\n",
            "[>] epoch #18, batch # 89: loss: 0.53675014 | val_loss: 1.02699673\n",
            "[>] epoch #18, batch # 91: loss: 0.69562507 | val_loss: 1.01640907\n",
            "[>] epoch #18, batch # 93: loss: 0.62011701 | val_loss: 0.97896917\n",
            "[>] epoch #18, batch # 95: loss: 0.55661237 | val_loss: 1.01498348\n",
            "[>] epoch #18, batch # 97: loss: 0.66605812 | val_loss: 1.02203433\n",
            "[>] epoch #18, batch # 99: loss: 0.69137847 | val_loss: 1.03091000\n",
            "[>] epoch #18, batch #101: loss: 0.52967149 | val_loss: 1.00836858\n",
            "[>] epoch #18, batch #103: loss: 0.62929112 | val_loss: 1.02477313\n",
            "[>] epoch #18, batch #105: loss: 0.58838719 | val_loss: 1.00216399\n",
            "[>] epoch #18, batch #107: loss: 0.76011664 | val_loss: 1.01919433\n",
            "[>] epoch #18, batch #109: loss: 0.60390621 | val_loss: 0.98766002\n",
            "[>] epoch #18, batch #111: loss: 0.60852665 | val_loss: 1.01544400\n",
            "[>] epoch #18, batch #113: loss: 0.66447246 | val_loss: 1.03086595\n",
            "[>] epoch #18, batch #115: loss: 0.63566852 | val_loss: 1.01046554\n",
            "[>] epoch #18, batch #117: loss: 0.76724714 | val_loss: 0.99558928\n",
            "[>] epoch #18, batch #119: loss: 0.52862531 | val_loss: 1.02287378\n",
            "[>] epoch #18, batch #121: loss: 0.56936777 | val_loss: 1.00280390\n",
            "[>] epoch #18, batch #123: loss: 0.66652369 | val_loss: 1.03353484\n",
            "[>] epoch #18, batch #125: loss: 0.46948686 | val_loss: 1.02621822\n",
            "[>] epoch #18, batch #127: loss: 0.50123215 | val_loss: 1.03845698\n",
            "[>] epoch #18, batch #129: loss: 0.55984265 | val_loss: 1.02200936\n",
            "[>] epoch #18, batch #131: loss: 0.52303851 | val_loss: 1.01475618\n",
            "[>] epoch #18, batch #133: loss: 0.52052021 | val_loss: 1.04177773\n",
            "[>] epoch #18, batch #135: loss: 0.46948540 | val_loss: 1.01183817\n",
            "[>] epoch #18, batch #137: loss: 0.60006702 | val_loss: 1.01830489\n",
            "[>] epoch #18, batch #139: loss: 0.62686342 | val_loss: 1.00937239\n",
            "[>] epoch #18, batch #141: loss: 1.37310314 | val_loss: 1.04113963\n",
            "[>] epoch #19, batch #  2: loss: 0.55337477 | val_loss: 1.01121546\n",
            "[>] epoch #19, batch #  4: loss: 0.52139431 | val_loss: 1.02039676\n",
            "[>] epoch #19, batch #  6: loss: 0.55132848 | val_loss: 1.01635762\n",
            "[>] epoch #19, batch #  8: loss: 0.36386055 | val_loss: 1.03455478\n",
            "[>] epoch #19, batch # 10: loss: 0.57579786 | val_loss: 0.99581109\n",
            "[>] epoch #19, batch # 12: loss: 0.51028788 | val_loss: 1.01382615\n",
            "[>] epoch #19, batch # 14: loss: 0.61461508 | val_loss: 1.02424208\n",
            "[>] epoch #19, batch # 16: loss: 0.50165331 | val_loss: 1.00387196\n",
            "[>] epoch #19, batch # 18: loss: 0.57244712 | val_loss: 1.01703015\n",
            "[>] epoch #19, batch # 20: loss: 0.57696790 | val_loss: 1.02669613\n",
            "[>] epoch #19, batch # 22: loss: 0.48627651 | val_loss: 1.02904666\n",
            "[>] epoch #19, batch # 24: loss: 0.54499978 | val_loss: 1.04534834\n",
            "[>] epoch #19, batch # 26: loss: 0.68151164 | val_loss: 1.02326005\n",
            "[>] epoch #19, batch # 28: loss: 0.55971777 | val_loss: 1.01576362\n",
            "[>] epoch #19, batch # 30: loss: 0.56168985 | val_loss: 1.04183376\n",
            "[>] epoch #19, batch # 32: loss: 0.55892348 | val_loss: 1.02755740\n",
            "[>] epoch #19, batch # 34: loss: 0.65021002 | val_loss: 1.01764924\n",
            "[>] epoch #19, batch # 36: loss: 0.58846712 | val_loss: 1.03798535\n",
            "[>] epoch #19, batch # 38: loss: 0.83070880 | val_loss: 1.01365137\n",
            "[>] epoch #19, batch # 40: loss: 0.48286965 | val_loss: 1.03296302\n",
            "[>] epoch #19, batch # 42: loss: 0.54686934 | val_loss: 1.06724740\n",
            "[>] epoch #19, batch # 44: loss: 0.45400935 | val_loss: 1.03336366\n",
            "[>] epoch #19, batch # 46: loss: 0.51510936 | val_loss: 1.01006021\n",
            "[>] epoch #19, batch # 48: loss: 0.54627210 | val_loss: 1.02777922\n",
            "[>] epoch #19, batch # 50: loss: 0.65850693 | val_loss: 1.05033484\n",
            "[>] epoch #19, batch # 52: loss: 0.62187970 | val_loss: 1.02213542\n",
            "[>] epoch #19, batch # 54: loss: 0.59244668 | val_loss: 1.03852446\n",
            "[>] epoch #19, batch # 56: loss: 0.66936773 | val_loss: 1.07069519\n",
            "[>] epoch #19, batch # 58: loss: 0.54086977 | val_loss: 1.02939334\n",
            "[>] epoch #19, batch # 60: loss: 0.55318552 | val_loss: 1.00013600\n",
            "[>] epoch #19, batch # 62: loss: 0.61207306 | val_loss: 1.06374348\n",
            "[>] epoch #19, batch # 64: loss: 0.63885635 | val_loss: 1.02055494\n",
            "[>] epoch #19, batch # 66: loss: 0.51008290 | val_loss: 1.00354001\n",
            "[>] epoch #19, batch # 68: loss: 0.52425504 | val_loss: 1.00369937\n",
            "[>] epoch #19, batch # 70: loss: 0.60050678 | val_loss: 1.01923160\n",
            "[>] epoch #19, batch # 72: loss: 0.60116065 | val_loss: 1.00583291\n",
            "[>] epoch #19, batch # 74: loss: 0.57060939 | val_loss: 1.05004976\n",
            "[>] epoch #19, batch # 76: loss: 0.52705246 | val_loss: 1.01487008\n",
            "[>] epoch #19, batch # 78: loss: 0.49872562 | val_loss: 1.01440434\n",
            "[>] epoch #19, batch # 80: loss: 0.60219181 | val_loss: 1.03547809\n",
            "[>] epoch #19, batch # 82: loss: 0.53510612 | val_loss: 1.00199786\n",
            "[>] epoch #19, batch # 84: loss: 0.61582297 | val_loss: 0.99216234\n",
            "[>] epoch #19, batch # 86: loss: 0.53427958 | val_loss: 1.01875821\n",
            "[>] epoch #19, batch # 88: loss: 0.59438896 | val_loss: 1.04021511\n",
            "[>] epoch #19, batch # 90: loss: 0.42769942 | val_loss: 1.01248320\n",
            "[>] epoch #19, batch # 92: loss: 0.60205138 | val_loss: 1.00643518\n",
            "[>] epoch #19, batch # 94: loss: 0.46165258 | val_loss: 0.98275914\n",
            "[>] epoch #19, batch # 96: loss: 0.57726628 | val_loss: 1.01713529\n",
            "[>] epoch #19, batch # 98: loss: 0.53524560 | val_loss: 0.98374419\n",
            "[>] epoch #19, batch #100: loss: 0.56430012 | val_loss: 0.98761282\n",
            "[>] epoch #19, batch #102: loss: 0.48414934 | val_loss: 0.98972934\n",
            "[>] epoch #19, batch #104: loss: 0.59191734 | val_loss: 0.97820775\n",
            "[>] epoch #19, batch #106: loss: 0.45974457 | val_loss: 0.97845265\n",
            "[>] epoch #19, batch #108: loss: 0.69202125 | val_loss: 0.96820199\n",
            "[>] epoch #19, batch #110: loss: 0.45586649 | val_loss: 0.98447061\n",
            "[>] epoch #19, batch #112: loss: 0.62698686 | val_loss: 0.99853089\n",
            "[>] epoch #19, batch #114: loss: 0.49129140 | val_loss: 0.97879242\n",
            "[>] epoch #19, batch #116: loss: 0.49194658 | val_loss: 0.98567893\n",
            "[>] epoch #19, batch #118: loss: 0.59233445 | val_loss: 0.97142983\n",
            "[>] epoch #19, batch #120: loss: 0.51322693 | val_loss: 1.00174738\n",
            "[>] epoch #19, batch #122: loss: 0.52470255 | val_loss: 0.99418058\n",
            "[>] epoch #19, batch #124: loss: 0.58795267 | val_loss: 0.97619379\n",
            "[>] epoch #19, batch #126: loss: 0.55676377 | val_loss: 0.99635904\n",
            "[>] epoch #19, batch #128: loss: 0.57702678 | val_loss: 0.97927587\n",
            "[>] epoch #19, batch #130: loss: 0.48294151 | val_loss: 0.99565242\n",
            "[>] epoch #19, batch #132: loss: 0.48761436 | val_loss: 1.01458550\n",
            "[>] epoch #19, batch #134: loss: 0.58905047 | val_loss: 0.96947526\n",
            "[>] epoch #19, batch #136: loss: 0.60554582 | val_loss: 1.01161926\n",
            "[>] epoch #19, batch #138: loss: 0.55698633 | val_loss: 0.98198577\n",
            "[>] epoch #19, batch #140: loss: 0.68748420 | val_loss: 0.99146456\n",
            "[>] epoch #20, batch #  1: loss: 0.58617741 | val_loss: 0.99138765\n",
            "[>] epoch #20, batch #  3: loss: 0.49056238 | val_loss: 0.97573238\n",
            "[>] epoch #20, batch #  5: loss: 0.59744376 | val_loss: 0.98261740\n",
            "[>] epoch #20, batch #  7: loss: 0.52487838 | val_loss: 1.01077193\n",
            "[>] epoch #20, batch #  9: loss: 0.39369056 | val_loss: 0.98678984\n",
            "[>] epoch #20, batch # 11: loss: 0.51370031 | val_loss: 0.99120141\n",
            "[>] epoch #20, batch # 13: loss: 0.55142748 | val_loss: 0.99472517\n",
            "[>] epoch #20, batch # 15: loss: 0.68440592 | val_loss: 1.01830178\n",
            "[>] epoch #20, batch # 17: loss: 0.63468683 | val_loss: 1.00715810\n",
            "[>] epoch #20, batch # 19: loss: 0.55914521 | val_loss: 1.00612976\n",
            "[>] epoch #20, batch # 21: loss: 0.53318632 | val_loss: 0.96984574\n",
            "[>] epoch #20, batch # 23: loss: 0.69022596 | val_loss: 1.00642633\n",
            "[>] epoch #20, batch # 25: loss: 0.53063679 | val_loss: 1.04116647\n",
            "[>] epoch #20, batch # 27: loss: 0.38297620 | val_loss: 1.03720513\n",
            "[>] epoch #20, batch # 29: loss: 0.54892135 | val_loss: 1.00916697\n",
            "[>] epoch #20, batch # 31: loss: 0.46309572 | val_loss: 1.00698930\n",
            "[>] epoch #20, batch # 33: loss: 0.49059194 | val_loss: 0.97634230\n",
            "[>] epoch #20, batch # 35: loss: 0.43938246 | val_loss: 0.98389698\n",
            "[>] epoch #20, batch # 37: loss: 0.49494910 | val_loss: 0.98439641\n",
            "[>] epoch #20, batch # 39: loss: 0.53447968 | val_loss: 1.02853860\n",
            "[>] epoch #20, batch # 41: loss: 0.60839307 | val_loss: 0.98924856\n",
            "[>] epoch #20, batch # 43: loss: 0.56853348 | val_loss: 0.98905037\n",
            "[>] epoch #20, batch # 45: loss: 0.47429359 | val_loss: 0.99229282\n",
            "[>] epoch #20, batch # 47: loss: 0.62330753 | val_loss: 1.00191477\n",
            "[>] epoch #20, batch # 49: loss: 0.51339382 | val_loss: 1.01183060\n",
            "[>] epoch #20, batch # 51: loss: 0.45824748 | val_loss: 1.01842761\n",
            "[>] epoch #20, batch # 53: loss: 0.52925563 | val_loss: 0.99117035\n",
            "[>] epoch #20, batch # 55: loss: 0.47480100 | val_loss: 0.99832122\n",
            "[>] epoch #20, batch # 57: loss: 0.51591963 | val_loss: 0.99456054\n",
            "[>] epoch #20, batch # 59: loss: 0.48939553 | val_loss: 0.99865141\n",
            "[>] epoch #20, batch # 61: loss: 0.39541680 | val_loss: 0.98007326\n",
            "[>] epoch #20, batch # 63: loss: 0.52955419 | val_loss: 0.98646270\n",
            "[>] epoch #20, batch # 65: loss: 0.48455602 | val_loss: 1.00188585\n",
            "[>] epoch #20, batch # 67: loss: 0.43158820 | val_loss: 0.98284584\n",
            "[>] epoch #20, batch # 69: loss: 0.49203062 | val_loss: 0.98486867\n",
            "[>] epoch #20, batch # 71: loss: 0.44932249 | val_loss: 0.98824010\n",
            "[>] epoch #20, batch # 73: loss: 0.49722144 | val_loss: 0.98067235\n",
            "[>] epoch #20, batch # 75: loss: 0.45692468 | val_loss: 0.98536452\n",
            "[>] epoch #20, batch # 77: loss: 0.45327342 | val_loss: 0.99726239\n",
            "[>] epoch #20, batch # 79: loss: 0.66453314 | val_loss: 0.96470944\n",
            "[>] epoch #20, batch # 81: loss: 0.54182976 | val_loss: 0.95401437\n",
            "[>] epoch #20, batch # 83: loss: 0.44612360 | val_loss: 0.98951162\n",
            "[>] epoch #20, batch # 85: loss: 0.54763395 | val_loss: 0.98574779\n",
            "[>] epoch #20, batch # 87: loss: 0.58585048 | val_loss: 0.96935964\n",
            "[>] epoch #20, batch # 89: loss: 0.52354687 | val_loss: 0.97390331\n",
            "[>] epoch #20, batch # 91: loss: 0.56467503 | val_loss: 0.97730528\n",
            "[>] epoch #20, batch # 93: loss: 0.54209936 | val_loss: 1.00113949\n",
            "[>] epoch #20, batch # 95: loss: 0.52211344 | val_loss: 0.98487197\n",
            "[>] epoch #20, batch # 97: loss: 0.51335871 | val_loss: 0.97900180\n",
            "[>] epoch #20, batch # 99: loss: 0.59230995 | val_loss: 0.99657114\n",
            "[>] epoch #20, batch #101: loss: 0.48142424 | val_loss: 0.97614908\n",
            "[>] epoch #20, batch #103: loss: 0.52931625 | val_loss: 0.98344416\n",
            "[>] epoch #20, batch #105: loss: 0.65239954 | val_loss: 0.98775176\n",
            "[>] epoch #20, batch #107: loss: 0.56535238 | val_loss: 0.97357729\n",
            "[>] epoch #20, batch #109: loss: 0.54820400 | val_loss: 0.98284063\n",
            "[>] epoch #20, batch #111: loss: 0.51563889 | val_loss: 1.00616894\n",
            "[>] epoch #20, batch #113: loss: 0.61487025 | val_loss: 0.98626350\n",
            "[>] epoch #20, batch #115: loss: 0.49250337 | val_loss: 1.00828739\n",
            "[>] epoch #20, batch #117: loss: 0.43083516 | val_loss: 0.97847349\n",
            "[>] epoch #20, batch #119: loss: 0.57542783 | val_loss: 1.00314262\n",
            "[>] epoch #20, batch #121: loss: 0.54935884 | val_loss: 0.97590501\n",
            "[>] epoch #20, batch #123: loss: 0.51793700 | val_loss: 0.98447732\n",
            "[>] epoch #20, batch #125: loss: 0.58643723 | val_loss: 1.02441957\n",
            "[>] epoch #20, batch #127: loss: 0.43081912 | val_loss: 0.97079969\n",
            "[>] epoch #20, batch #129: loss: 0.47278118 | val_loss: 1.00395546\n",
            "[>] epoch #20, batch #131: loss: 0.57878590 | val_loss: 0.96790765\n",
            "[>] epoch #20, batch #133: loss: 0.55344611 | val_loss: 0.96530607\n",
            "[>] epoch #20, batch #135: loss: 0.67351633 | val_loss: 0.97093958\n",
            "[>] epoch #20, batch #137: loss: 0.56582552 | val_loss: 0.98663085\n",
            "[>] epoch #20, batch #139: loss: 0.46935394 | val_loss: 1.00292316\n",
            "[>] epoch #20, batch #141: loss: 0.46377748 | val_loss: 0.99052900\n",
            "[>] epoch #21, batch #  2: loss: 0.55590588 | val_loss: 0.98641094\n",
            "[>] epoch #21, batch #  4: loss: 0.47988439 | val_loss: 0.97140861\n",
            "[>] epoch #21, batch #  6: loss: 0.60817426 | val_loss: 1.01838358\n",
            "[>] epoch #21, batch #  8: loss: 0.55589199 | val_loss: 0.98138414\n",
            "[>] epoch #21, batch # 10: loss: 0.46116138 | val_loss: 0.98875397\n",
            "[>] epoch #21, batch # 12: loss: 0.47192210 | val_loss: 0.96843747\n",
            "[>] epoch #21, batch # 14: loss: 0.49397019 | val_loss: 0.97904579\n",
            "[>] epoch #21, batch # 16: loss: 0.47908440 | val_loss: 0.97616004\n",
            "[>] epoch #21, batch # 18: loss: 0.39161444 | val_loss: 0.98499035\n",
            "[>] epoch #21, batch # 20: loss: 0.41638944 | val_loss: 0.97120110\n",
            "[>] epoch #21, batch # 22: loss: 0.45376727 | val_loss: 0.99649420\n",
            "[>] epoch #21, batch # 24: loss: 0.54395431 | val_loss: 0.97982057\n",
            "[>] epoch #21, batch # 26: loss: 0.51963544 | val_loss: 0.99299727\n",
            "[>] epoch #21, batch # 28: loss: 0.42343792 | val_loss: 0.98069350\n",
            "[>] epoch #21, batch # 30: loss: 0.40513629 | val_loss: 0.99097873\n",
            "[>] epoch #21, batch # 32: loss: 0.44185859 | val_loss: 0.95391539\n",
            "[>] epoch #21, batch # 34: loss: 0.50695270 | val_loss: 1.00988092\n",
            "[>] epoch #21, batch # 36: loss: 0.57752198 | val_loss: 0.97913839\n",
            "[>] epoch #21, batch # 38: loss: 0.42073905 | val_loss: 0.99907431\n",
            "[>] epoch #21, batch # 40: loss: 0.50180912 | val_loss: 0.99211001\n",
            "[>] epoch #21, batch # 42: loss: 0.52330410 | val_loss: 0.95046372\n",
            "[>] epoch #21, batch # 44: loss: 0.59024721 | val_loss: 0.97636364\n",
            "[>] epoch #21, batch # 46: loss: 0.45432508 | val_loss: 0.99312564\n",
            "[>] epoch #21, batch # 48: loss: 0.49319577 | val_loss: 0.98404005\n",
            "[>] epoch #21, batch # 50: loss: 0.38316402 | val_loss: 0.93490463\n",
            "[>] epoch #21, batch # 52: loss: 0.46924508 | val_loss: 0.98864902\n",
            "[>] epoch #21, batch # 54: loss: 0.58554637 | val_loss: 0.98181543\n",
            "[>] epoch #21, batch # 56: loss: 0.52383190 | val_loss: 0.95918311\n",
            "[>] epoch #21, batch # 58: loss: 0.58092886 | val_loss: 0.97702453\n",
            "[>] epoch #21, batch # 60: loss: 0.52221501 | val_loss: 0.97734827\n",
            "[>] epoch #21, batch # 62: loss: 0.43295258 | val_loss: 0.98412635\n",
            "[>] epoch #21, batch # 64: loss: 0.42178291 | val_loss: 0.98266019\n",
            "[>] epoch #21, batch # 66: loss: 0.33939317 | val_loss: 0.97247524\n",
            "[>] epoch #21, batch # 68: loss: 0.44428015 | val_loss: 0.99450887\n",
            "[>] epoch #21, batch # 70: loss: 0.52251267 | val_loss: 0.98280881\n",
            "[>] epoch #21, batch # 72: loss: 0.60098475 | val_loss: 0.98894466\n",
            "[>] epoch #21, batch # 74: loss: 0.43894079 | val_loss: 0.99498965\n",
            "[>] epoch #21, batch # 76: loss: 0.44195396 | val_loss: 0.98322193\n",
            "[>] epoch #21, batch # 78: loss: 0.54212880 | val_loss: 0.99793891\n",
            "[>] epoch #21, batch # 80: loss: 0.45772162 | val_loss: 0.94730877\n",
            "[>] epoch #21, batch # 82: loss: 0.49782214 | val_loss: 0.98793436\n",
            "[>] epoch #21, batch # 84: loss: 0.53924453 | val_loss: 1.00261441\n",
            "[>] epoch #21, batch # 86: loss: 0.54370433 | val_loss: 0.98025934\n",
            "[>] epoch #21, batch # 88: loss: 0.48320547 | val_loss: 0.99091844\n",
            "[>] epoch #21, batch # 90: loss: 0.44544679 | val_loss: 0.97299117\n",
            "[>] epoch #21, batch # 92: loss: 0.48138967 | val_loss: 0.98240155\n",
            "[>] epoch #21, batch # 94: loss: 0.54207700 | val_loss: 1.00152822\n",
            "[>] epoch #21, batch # 96: loss: 0.52300936 | val_loss: 0.98632171\n",
            "[>] epoch #21, batch # 98: loss: 0.49472907 | val_loss: 0.96928927\n",
            "[>] epoch #21, batch #100: loss: 0.59539247 | val_loss: 0.98621005\n",
            "[>] epoch #21, batch #102: loss: 0.53959060 | val_loss: 0.98901341\n",
            "[>] epoch #21, batch #104: loss: 0.35970494 | val_loss: 0.95654998\n",
            "[>] epoch #21, batch #106: loss: 0.43459320 | val_loss: 0.96939887\n",
            "[>] epoch #21, batch #108: loss: 0.51450831 | val_loss: 0.99382664\n",
            "[>] epoch #21, batch #110: loss: 0.64717090 | val_loss: 0.94028316\n",
            "[>] epoch #21, batch #112: loss: 0.44517156 | val_loss: 0.97513336\n",
            "[>] epoch #21, batch #114: loss: 0.46789217 | val_loss: 0.94283903\n",
            "[>] epoch #21, batch #116: loss: 0.48140216 | val_loss: 0.97065377\n",
            "[>] epoch #21, batch #118: loss: 0.49120951 | val_loss: 0.98730672\n",
            "[>] epoch #21, batch #120: loss: 0.54516870 | val_loss: 0.97946260\n",
            "[>] epoch #21, batch #122: loss: 0.46962368 | val_loss: 1.01737630\n",
            "[>] epoch #21, batch #124: loss: 0.56366277 | val_loss: 0.97678283\n",
            "[>] epoch #21, batch #126: loss: 0.48086751 | val_loss: 0.96390516\n",
            "[>] epoch #21, batch #128: loss: 0.46080753 | val_loss: 0.99768711\n",
            "[>] epoch #21, batch #130: loss: 0.49870282 | val_loss: 0.96747025\n",
            "[>] epoch #21, batch #132: loss: 0.42600518 | val_loss: 0.96417282\n",
            "[>] epoch #21, batch #134: loss: 0.48259181 | val_loss: 0.99939583\n",
            "[>] epoch #21, batch #136: loss: 0.50973862 | val_loss: 0.98343183\n",
            "[>] epoch #21, batch #138: loss: 0.61699456 | val_loss: 0.97208239\n",
            "[>] epoch #21, batch #140: loss: 0.47778392 | val_loss: 0.97442282\n",
            "[>] epoch #22, batch #  1: loss: 0.44931281 | val_loss: 0.98898691\n",
            "[>] epoch #22, batch #  3: loss: 0.36337647 | val_loss: 0.97242404\n",
            "[>] epoch #22, batch #  5: loss: 0.37558293 | val_loss: 0.95440024\n",
            "[>] epoch #22, batch #  7: loss: 0.42717010 | val_loss: 0.99040018\n",
            "[>] epoch #22, batch #  9: loss: 0.44206011 | val_loss: 0.97510227\n",
            "[>] epoch #22, batch # 11: loss: 0.45998475 | val_loss: 0.97783102\n",
            "[>] epoch #22, batch # 13: loss: 0.42326441 | val_loss: 0.98511507\n",
            "[>] epoch #22, batch # 15: loss: 0.47108901 | val_loss: 0.97935837\n",
            "[>] epoch #22, batch # 17: loss: 0.31602401 | val_loss: 0.98512300\n",
            "[>] epoch #22, batch # 19: loss: 0.51393116 | val_loss: 0.97867780\n",
            "[>] epoch #22, batch # 21: loss: 0.38270339 | val_loss: 0.94364860\n",
            "[>] epoch #22, batch # 23: loss: 0.38837299 | val_loss: 0.96933546\n",
            "[>] epoch #22, batch # 25: loss: 0.48858133 | val_loss: 0.98544927\n",
            "[>] epoch #22, batch # 27: loss: 0.44397825 | val_loss: 0.96598156\n",
            "[>] epoch #22, batch # 29: loss: 0.55223012 | val_loss: 0.97522278\n",
            "[>] epoch #22, batch # 31: loss: 0.54914171 | val_loss: 0.98226650\n",
            "[>] epoch #22, batch # 33: loss: 0.49693000 | val_loss: 0.95661133\n",
            "[>] epoch #22, batch # 35: loss: 0.39003038 | val_loss: 0.97277017\n",
            "[>] epoch #22, batch # 37: loss: 0.53361619 | val_loss: 0.98796637\n",
            "[>] epoch #22, batch # 39: loss: 0.42554468 | val_loss: 0.99719317\n",
            "[>] epoch #22, batch # 41: loss: 0.44210166 | val_loss: 0.98472521\n",
            "[>] epoch #22, batch # 43: loss: 0.37479770 | val_loss: 0.99669393\n",
            "[>] epoch #22, batch # 45: loss: 0.38039601 | val_loss: 1.00971265\n",
            "[>] epoch #22, batch # 47: loss: 0.43132368 | val_loss: 0.97491097\n",
            "[>] epoch #22, batch # 49: loss: 0.48430470 | val_loss: 0.96823877\n",
            "[>] epoch #22, batch # 51: loss: 0.48208749 | val_loss: 0.99359127\n",
            "[>] epoch #22, batch # 53: loss: 0.51228166 | val_loss: 0.97135336\n",
            "[>] epoch #22, batch # 55: loss: 0.47773415 | val_loss: 0.96631979\n",
            "[>] epoch #22, batch # 57: loss: 0.45756900 | val_loss: 1.00672484\n",
            "[>] epoch #22, batch # 59: loss: 0.48084950 | val_loss: 0.96398286\n",
            "[>] epoch #22, batch # 61: loss: 0.50341600 | val_loss: 0.97864329\n",
            "[>] epoch #22, batch # 63: loss: 0.49055177 | val_loss: 0.96921195\n",
            "[>] epoch #22, batch # 65: loss: 0.35614768 | val_loss: 0.98337207\n",
            "[>] epoch #22, batch # 67: loss: 0.47534159 | val_loss: 0.95465110\n",
            "[>] epoch #22, batch # 69: loss: 0.42460656 | val_loss: 0.97517746\n",
            "[>] epoch #22, batch # 71: loss: 0.40447462 | val_loss: 0.96681258\n",
            "[>] epoch #22, batch # 73: loss: 0.46619809 | val_loss: 1.00841696\n",
            "[>] epoch #22, batch # 75: loss: 0.37619612 | val_loss: 0.95636310\n",
            "[>] epoch #22, batch # 77: loss: 0.44248322 | val_loss: 0.99063871\n",
            "[>] epoch #22, batch # 79: loss: 0.43615869 | val_loss: 0.96140215\n",
            "[>] epoch #22, batch # 81: loss: 0.34754160 | val_loss: 0.94978575\n",
            "[>] epoch #22, batch # 83: loss: 0.56596732 | val_loss: 0.98651160\n",
            "[>] epoch #22, batch # 85: loss: 0.48342350 | val_loss: 0.97364652\n",
            "[>] epoch #22, batch # 87: loss: 0.44166994 | val_loss: 0.95243467\n",
            "[>] epoch #22, batch # 89: loss: 0.37776920 | val_loss: 0.95274557\n",
            "[>] epoch #22, batch # 91: loss: 0.44727167 | val_loss: 0.96844418\n",
            "[>] epoch #22, batch # 93: loss: 0.45785427 | val_loss: 0.94171116\n",
            "[>] epoch #22, batch # 95: loss: 0.46681702 | val_loss: 0.97597350\n",
            "[>] epoch #22, batch # 97: loss: 0.51139617 | val_loss: 0.94499622\n",
            "[>] epoch #22, batch # 99: loss: 0.58377528 | val_loss: 0.95065271\n",
            "[>] epoch #22, batch #101: loss: 0.52161092 | val_loss: 0.95030479\n",
            "[>] epoch #22, batch #103: loss: 0.53784418 | val_loss: 0.94177976\n",
            "[>] epoch #22, batch #105: loss: 0.39873436 | val_loss: 0.95789456\n",
            "[>] epoch #22, batch #107: loss: 0.43958461 | val_loss: 0.96028452\n",
            "[>] epoch #22, batch #109: loss: 0.39436102 | val_loss: 0.94748254\n",
            "[>] epoch #22, batch #111: loss: 0.46736449 | val_loss: 0.98260018\n",
            "[>] epoch #22, batch #113: loss: 0.37209344 | val_loss: 0.97876538\n",
            "[>] epoch #22, batch #115: loss: 0.58731061 | val_loss: 0.97474743\n",
            "[>] epoch #22, batch #117: loss: 0.39351755 | val_loss: 0.96579888\n",
            "[>] epoch #22, batch #119: loss: 0.42339754 | val_loss: 0.92775229\n",
            "[>] epoch #22, batch #121: loss: 0.40555298 | val_loss: 0.96457171\n",
            "[>] epoch #22, batch #123: loss: 0.52463996 | val_loss: 0.97468986\n",
            "[>] epoch #22, batch #125: loss: 0.43205085 | val_loss: 0.96398570\n",
            "[>] epoch #22, batch #127: loss: 0.56073427 | val_loss: 0.99231790\n",
            "[>] epoch #22, batch #129: loss: 0.50189352 | val_loss: 0.96507431\n",
            "[>] epoch #22, batch #131: loss: 0.59743708 | val_loss: 0.99224655\n",
            "[>] epoch #22, batch #133: loss: 0.37293565 | val_loss: 0.95237993\n",
            "[>] epoch #22, batch #135: loss: 0.36282659 | val_loss: 0.98865460\n",
            "[>] epoch #22, batch #137: loss: 0.64159060 | val_loss: 0.95993142\n",
            "[>] epoch #22, batch #139: loss: 0.38669714 | val_loss: 0.97283198\n",
            "[>] epoch #22, batch #141: loss: 0.92296094 | val_loss: 0.97137436\n",
            "[>] epoch #23, batch #  2: loss: 0.38340652 | val_loss: 0.97521501\n",
            "[>] epoch #23, batch #  4: loss: 0.45665053 | val_loss: 0.97465651\n",
            "[>] epoch #23, batch #  6: loss: 0.51223242 | val_loss: 0.95236712\n",
            "[>] epoch #23, batch #  8: loss: 0.37049335 | val_loss: 0.95094784\n",
            "[>] epoch #23, batch # 10: loss: 0.41932598 | val_loss: 0.98165816\n",
            "[>] epoch #23, batch # 12: loss: 0.46921322 | val_loss: 0.97079859\n",
            "[>] epoch #23, batch # 14: loss: 0.35736328 | val_loss: 0.99149792\n",
            "[>] epoch #23, batch # 16: loss: 0.39627317 | val_loss: 0.95745765\n",
            "[>] epoch #23, batch # 18: loss: 0.40228820 | val_loss: 0.97482237\n",
            "[>] epoch #23, batch # 20: loss: 0.34194550 | val_loss: 0.97506315\n",
            "[>] epoch #23, batch # 22: loss: 0.43052033 | val_loss: 0.98587045\n",
            "[>] epoch #23, batch # 24: loss: 0.44483799 | val_loss: 1.01783041\n",
            "[>] epoch #23, batch # 26: loss: 0.46317473 | val_loss: 1.02469657\n",
            "[>] epoch #23, batch # 28: loss: 0.45907208 | val_loss: 0.98215806\n",
            "[>] epoch #23, batch # 30: loss: 0.42033753 | val_loss: 0.96378709\n",
            "[>] epoch #23, batch # 32: loss: 0.44442835 | val_loss: 0.96648938\n",
            "[>] epoch #23, batch # 34: loss: 0.42973110 | val_loss: 0.97134519\n",
            "[>] epoch #23, batch # 36: loss: 0.38197222 | val_loss: 0.95980574\n",
            "[>] epoch #23, batch # 38: loss: 0.42030519 | val_loss: 0.96664269\n",
            "[>] epoch #23, batch # 40: loss: 0.39950496 | val_loss: 0.97313007\n",
            "[>] epoch #23, batch # 42: loss: 0.37536246 | val_loss: 0.94439935\n",
            "[>] epoch #23, batch # 44: loss: 0.36552563 | val_loss: 0.96538637\n",
            "[>] epoch #23, batch # 46: loss: 0.41368860 | val_loss: 0.97148649\n",
            "[>] epoch #23, batch # 48: loss: 0.47466326 | val_loss: 0.98981025\n",
            "[>] epoch #23, batch # 50: loss: 0.51777041 | val_loss: 0.96034171\n",
            "[>] epoch #23, batch # 52: loss: 0.42599198 | val_loss: 0.95576178\n",
            "[>] epoch #23, batch # 54: loss: 0.39197704 | val_loss: 0.93165826\n",
            "[>] epoch #23, batch # 56: loss: 0.30176935 | val_loss: 0.94226529\n",
            "[>] epoch #23, batch # 58: loss: 0.39893314 | val_loss: 0.96477193\n",
            "[>] epoch #23, batch # 60: loss: 0.36643276 | val_loss: 0.97927841\n",
            "[>] epoch #23, batch # 62: loss: 0.35868216 | val_loss: 0.93826705\n",
            "[>] epoch #23, batch # 64: loss: 0.34178680 | val_loss: 0.93922762\n",
            "[>] epoch #23, batch # 66: loss: 0.45747939 | val_loss: 0.96959052\n",
            "[>] epoch #23, batch # 68: loss: 0.37890849 | val_loss: 0.94416423\n",
            "[>] epoch #23, batch # 70: loss: 0.46250382 | val_loss: 0.96680463\n",
            "[>] epoch #23, batch # 72: loss: 0.37223569 | val_loss: 0.95016588\n",
            "[>] epoch #23, batch # 74: loss: 0.42188331 | val_loss: 0.93549013\n",
            "[>] epoch #23, batch # 76: loss: 0.35255802 | val_loss: 0.96226893\n",
            "[>] epoch #23, batch # 78: loss: 0.32391572 | val_loss: 0.97122556\n",
            "[>] epoch #23, batch # 80: loss: 0.37691277 | val_loss: 0.94622446\n",
            "[>] epoch #23, batch # 82: loss: 0.45202351 | val_loss: 0.93651480\n",
            "[>] epoch #23, batch # 84: loss: 0.42864460 | val_loss: 0.94471751\n",
            "[>] epoch #23, batch # 86: loss: 0.48722681 | val_loss: 0.97104405\n",
            "[>] epoch #23, batch # 88: loss: 0.48378116 | val_loss: 0.93837677\n",
            "[>] epoch #23, batch # 90: loss: 0.38352472 | val_loss: 0.95137737\n",
            "[>] epoch #23, batch # 92: loss: 0.33577880 | val_loss: 0.94136117\n",
            "[>] epoch #23, batch # 94: loss: 0.34965226 | val_loss: 0.95998278\n",
            "[>] epoch #23, batch # 96: loss: 0.36714545 | val_loss: 0.92985672\n",
            "[>] epoch #23, batch # 98: loss: 0.36545846 | val_loss: 0.93101712\n",
            "[>] epoch #23, batch #100: loss: 0.37717527 | val_loss: 0.97322743\n",
            "[>] epoch #23, batch #102: loss: 0.36059549 | val_loss: 0.96910189\n",
            "[>] epoch #23, batch #104: loss: 0.41370097 | val_loss: 0.94034811\n",
            "[>] epoch #23, batch #106: loss: 0.43820816 | val_loss: 0.94613818\n",
            "[>] epoch #23, batch #108: loss: 0.51409876 | val_loss: 0.93842646\n",
            "[>] epoch #23, batch #110: loss: 0.46382892 | val_loss: 0.95736393\n",
            "[>] epoch #23, batch #112: loss: 0.51711839 | val_loss: 0.96491683\n",
            "[>] epoch #23, batch #114: loss: 0.41417232 | val_loss: 0.94201656\n",
            "[>] epoch #23, batch #116: loss: 0.50654960 | val_loss: 0.93856195\n",
            "[>] epoch #23, batch #118: loss: 0.35424811 | val_loss: 0.94976968\n",
            "[>] epoch #23, batch #120: loss: 0.42555007 | val_loss: 0.94283356\n",
            "[>] epoch #23, batch #122: loss: 0.52611721 | val_loss: 0.97119062\n",
            "[>] epoch #23, batch #124: loss: 0.45421886 | val_loss: 0.92080265\n",
            "[>] epoch #23, batch #126: loss: 0.35580340 | val_loss: 0.96067333\n",
            "[>] epoch #23, batch #128: loss: 0.42735630 | val_loss: 0.95554805\n",
            "[>] epoch #23, batch #130: loss: 0.52339292 | val_loss: 0.94707085\n",
            "[>] epoch #23, batch #132: loss: 0.41816464 | val_loss: 0.93488368\n",
            "[>] epoch #23, batch #134: loss: 0.40021938 | val_loss: 0.95408113\n",
            "[>] epoch #23, batch #136: loss: 0.43616909 | val_loss: 0.92904024\n",
            "[>] epoch #23, batch #138: loss: 0.45250723 | val_loss: 0.94930006\n",
            "[>] epoch #23, batch #140: loss: 0.54430842 | val_loss: 0.95766183\n",
            "[>] epoch #24, batch #  1: loss: 0.37085614 | val_loss: 0.94777932\n",
            "[>] epoch #24, batch #  3: loss: 0.34316236 | val_loss: 0.95452374\n",
            "[>] epoch #24, batch #  5: loss: 0.46580917 | val_loss: 0.95084099\n",
            "[>] epoch #24, batch #  7: loss: 0.34842765 | val_loss: 0.93476617\n",
            "[>] epoch #24, batch #  9: loss: 0.45944071 | val_loss: 0.97020378\n",
            "[>] epoch #24, batch # 11: loss: 0.28423533 | val_loss: 0.94371682\n",
            "[>] epoch #24, batch # 13: loss: 0.39041868 | val_loss: 0.96011577\n",
            "[>] epoch #24, batch # 15: loss: 0.45568961 | val_loss: 0.96376001\n",
            "[>] epoch #24, batch # 17: loss: 0.33322042 | val_loss: 0.96153315\n",
            "[>] epoch #24, batch # 19: loss: 0.30956727 | val_loss: 0.94932521\n",
            "[>] epoch #24, batch # 21: loss: 0.41497633 | val_loss: 0.94092039\n",
            "[>] epoch #24, batch # 23: loss: 0.36067769 | val_loss: 0.97603863\n",
            "[>] epoch #24, batch # 25: loss: 0.31538481 | val_loss: 0.97453859\n",
            "[>] epoch #24, batch # 27: loss: 0.32267407 | val_loss: 0.95637839\n",
            "[>] epoch #24, batch # 29: loss: 0.34965181 | val_loss: 0.95656397\n",
            "[>] epoch #24, batch # 31: loss: 0.39301118 | val_loss: 0.98326336\n",
            "[>] epoch #24, batch # 33: loss: 0.37128857 | val_loss: 0.93795417\n",
            "[>] epoch #24, batch # 35: loss: 0.38678879 | val_loss: 0.95747398\n",
            "[>] epoch #24, batch # 37: loss: 0.41829491 | val_loss: 0.93390517\n",
            "[>] epoch #24, batch # 39: loss: 0.39255163 | val_loss: 0.94254737\n",
            "[>] epoch #24, batch # 41: loss: 0.30472973 | val_loss: 0.94634113\n",
            "[>] epoch #24, batch # 43: loss: 0.30650356 | val_loss: 0.94845479\n",
            "[>] epoch #24, batch # 45: loss: 0.42002162 | val_loss: 0.96194841\n",
            "[>] epoch #24, batch # 47: loss: 0.42288432 | val_loss: 0.93628362\n",
            "[>] epoch #24, batch # 49: loss: 0.38581422 | val_loss: 0.93615112\n",
            "[>] epoch #24, batch # 51: loss: 0.48747757 | val_loss: 0.96290765\n",
            "[>] epoch #24, batch # 53: loss: 0.34412816 | val_loss: 0.94925049\n",
            "[>] epoch #24, batch # 55: loss: 0.36204720 | val_loss: 0.96075452\n",
            "[>] epoch #24, batch # 57: loss: 0.41684449 | val_loss: 0.95436446\n",
            "[>] epoch #24, batch # 59: loss: 0.36759862 | val_loss: 0.91870517\n",
            "[>] epoch #24, batch # 61: loss: 0.35317585 | val_loss: 0.97146725\n",
            "[>] epoch #24, batch # 63: loss: 0.35094425 | val_loss: 0.96619599\n",
            "[>] epoch #24, batch # 65: loss: 0.44211829 | val_loss: 0.95271812\n",
            "[>] epoch #24, batch # 67: loss: 0.34639397 | val_loss: 0.96481816\n",
            "[>] epoch #24, batch # 69: loss: 0.51510864 | val_loss: 0.97423084\n",
            "[>] epoch #24, batch # 71: loss: 0.51544565 | val_loss: 0.93182182\n",
            "[>] epoch #24, batch # 73: loss: 0.45120963 | val_loss: 0.98495028\n",
            "[>] epoch #24, batch # 75: loss: 0.41886485 | val_loss: 0.94378565\n",
            "[>] epoch #24, batch # 77: loss: 0.32195732 | val_loss: 1.00221662\n",
            "[>] epoch #24, batch # 79: loss: 0.47525465 | val_loss: 0.94679415\n",
            "[>] epoch #24, batch # 81: loss: 0.34524146 | val_loss: 0.94309390\n",
            "[>] epoch #24, batch # 83: loss: 0.27526265 | val_loss: 0.94033300\n",
            "[>] epoch #24, batch # 85: loss: 0.26973438 | val_loss: 0.96959039\n",
            "[>] epoch #24, batch # 87: loss: 0.36845514 | val_loss: 0.94888868\n",
            "[>] epoch #24, batch # 89: loss: 0.51312637 | val_loss: 0.98122090\n",
            "[>] epoch #24, batch # 91: loss: 0.34708902 | val_loss: 0.98197838\n",
            "[>] epoch #24, batch # 93: loss: 0.50223732 | val_loss: 0.94466536\n",
            "[>] epoch #24, batch # 95: loss: 0.34772283 | val_loss: 0.95870713\n",
            "[>] epoch #24, batch # 97: loss: 0.38905329 | val_loss: 0.95251728\n",
            "[>] epoch #24, batch # 99: loss: 0.53246540 | val_loss: 0.92670524\n",
            "[>] epoch #24, batch #101: loss: 0.46309942 | val_loss: 0.94640638\n",
            "[>] epoch #24, batch #103: loss: 0.37598315 | val_loss: 0.93750829\n",
            "[>] epoch #24, batch #105: loss: 0.46967617 | val_loss: 0.95814682\n",
            "[>] epoch #24, batch #107: loss: 0.39892787 | val_loss: 0.95464871\n",
            "[>] epoch #24, batch #109: loss: 0.46035442 | val_loss: 0.95153953\n",
            "[>] epoch #24, batch #111: loss: 0.40982404 | val_loss: 0.96264592\n",
            "[>] epoch #24, batch #113: loss: 0.36405313 | val_loss: 0.94262721\n",
            "[>] epoch #24, batch #115: loss: 0.32707691 | val_loss: 0.95097083\n",
            "[>] epoch #24, batch #117: loss: 0.40847766 | val_loss: 0.93048963\n",
            "[>] epoch #24, batch #119: loss: 0.39361805 | val_loss: 0.96029738\n",
            "[>] epoch #24, batch #121: loss: 0.39496452 | val_loss: 0.93425350\n",
            "[>] epoch #24, batch #123: loss: 0.43691859 | val_loss: 0.94243565\n",
            "[>] epoch #24, batch #125: loss: 0.40863010 | val_loss: 0.95450437\n",
            "[>] epoch #24, batch #127: loss: 0.42920944 | val_loss: 0.93068402\n",
            "[>] epoch #24, batch #129: loss: 0.40308267 | val_loss: 0.94324265\n",
            "[>] epoch #24, batch #131: loss: 0.39888895 | val_loss: 0.91781491\n",
            "[>] epoch #24, batch #133: loss: 0.37471303 | val_loss: 0.95322628\n",
            "[>] epoch #24, batch #135: loss: 0.40199462 | val_loss: 0.92773224\n",
            "[>] epoch #24, batch #137: loss: 0.37652460 | val_loss: 0.91617835\n",
            "[>] epoch #24, batch #139: loss: 0.40019837 | val_loss: 0.91200666\n",
            "[>] epoch #24, batch #141: loss: 0.45483431 | val_loss: 0.94754501\n",
            "[>] epoch #25, batch #  2: loss: 0.32718909 | val_loss: 0.92751728\n",
            "[>] epoch #25, batch #  4: loss: 0.40331453 | val_loss: 0.92674185\n",
            "[>] epoch #25, batch #  6: loss: 0.32469943 | val_loss: 0.94844951\n",
            "[>] epoch #25, batch #  8: loss: 0.29421088 | val_loss: 0.94944096\n",
            "[>] epoch #25, batch # 10: loss: 0.34406760 | val_loss: 0.95308681\n",
            "[>] epoch #25, batch # 12: loss: 0.32109621 | val_loss: 0.95363827\n",
            "[>] epoch #25, batch # 14: loss: 0.36964181 | val_loss: 0.97119022\n",
            "[>] epoch #25, batch # 16: loss: 0.37398946 | val_loss: 0.95074222\n",
            "[>] epoch #25, batch # 18: loss: 0.32529300 | val_loss: 0.95656793\n",
            "[>] epoch #25, batch # 20: loss: 0.34592548 | val_loss: 0.94536373\n",
            "[>] epoch #25, batch # 22: loss: 0.40562806 | val_loss: 0.95119473\n",
            "[>] epoch #25, batch # 24: loss: 0.35332340 | val_loss: 0.96409836\n",
            "[>] epoch #25, batch # 26: loss: 0.27421066 | val_loss: 0.93854956\n",
            "[>] epoch #25, batch # 28: loss: 0.31865761 | val_loss: 0.93266763\n",
            "[>] epoch #25, batch # 30: loss: 0.37464991 | val_loss: 0.92258250\n",
            "[>] epoch #25, batch # 32: loss: 0.26133975 | val_loss: 0.93236467\n",
            "[>] epoch #25, batch # 34: loss: 0.40099543 | val_loss: 0.94295052\n",
            "[>] epoch #25, batch # 36: loss: 0.38189879 | val_loss: 0.95106438\n",
            "[>] epoch #25, batch # 38: loss: 0.37624308 | val_loss: 0.93065097\n",
            "[>] epoch #25, batch # 40: loss: 0.36381373 | val_loss: 0.94677304\n",
            "[>] epoch #25, batch # 42: loss: 0.30313575 | val_loss: 0.92544211\n",
            "[>] epoch #25, batch # 44: loss: 0.30076572 | val_loss: 0.96768166\n",
            "[>] epoch #25, batch # 46: loss: 0.35668486 | val_loss: 0.93213779\n",
            "[>] epoch #25, batch # 48: loss: 0.49467260 | val_loss: 0.95410220\n",
            "[>] epoch #25, batch # 50: loss: 0.39942142 | val_loss: 0.95381379\n",
            "[>] epoch #25, batch # 52: loss: 0.34971482 | val_loss: 0.95152233\n",
            "[>] epoch #25, batch # 54: loss: 0.35265252 | val_loss: 0.93676980\n",
            "[>] epoch #25, batch # 56: loss: 0.44738200 | val_loss: 0.92316041\n",
            "[>] epoch #25, batch # 58: loss: 0.30137420 | val_loss: 0.95074168\n",
            "[>] epoch #25, batch # 60: loss: 0.43269804 | val_loss: 0.94335593\n",
            "[>] epoch #25, batch # 62: loss: 0.38581082 | val_loss: 0.94013560\n",
            "[>] epoch #25, batch # 64: loss: 0.33851528 | val_loss: 0.92177922\n",
            "[>] epoch #25, batch # 66: loss: 0.36726680 | val_loss: 0.93084871\n",
            "[>] epoch #25, batch # 68: loss: 0.34364939 | val_loss: 0.95800181\n",
            "[>] epoch #25, batch # 70: loss: 0.47173154 | val_loss: 0.93751548\n",
            "[>] epoch #25, batch # 72: loss: 0.36392269 | val_loss: 0.94612000\n",
            "[>] epoch #25, batch # 74: loss: 0.34207866 | val_loss: 0.95505042\n",
            "[>] epoch #25, batch # 76: loss: 0.29139680 | val_loss: 0.93906105\n",
            "[>] epoch #25, batch # 78: loss: 0.46669099 | val_loss: 0.94931598\n",
            "[>] epoch #25, batch # 80: loss: 0.34006608 | val_loss: 0.97959735\n",
            "[>] epoch #25, batch # 82: loss: 0.40438667 | val_loss: 0.93013636\n",
            "[>] epoch #25, batch # 84: loss: 0.34146637 | val_loss: 0.96852932\n",
            "[>] epoch #25, batch # 86: loss: 0.38572273 | val_loss: 0.94750586\n",
            "[>] epoch #25, batch # 88: loss: 0.38384488 | val_loss: 0.94569850\n",
            "[>] epoch #25, batch # 90: loss: 0.37335858 | val_loss: 0.95699395\n",
            "[>] epoch #25, batch # 92: loss: 0.30987865 | val_loss: 0.94643608\n",
            "[>] epoch #25, batch # 94: loss: 0.33034405 | val_loss: 0.95630189\n",
            "[>] epoch #25, batch # 96: loss: 0.41877934 | val_loss: 0.94409556\n",
            "[>] epoch #25, batch # 98: loss: 0.35917515 | val_loss: 0.93761108\n",
            "[>] epoch #25, batch #100: loss: 0.47585735 | val_loss: 0.97072668\n",
            "[>] epoch #25, batch #102: loss: 0.30103365 | val_loss: 0.96542831\n",
            "[>] epoch #25, batch #104: loss: 0.39893305 | val_loss: 0.97173788\n",
            "[>] epoch #25, batch #106: loss: 0.36858037 | val_loss: 0.96566343\n",
            "[>] epoch #25, batch #108: loss: 0.33170065 | val_loss: 0.94991859\n",
            "[>] epoch #25, batch #110: loss: 0.41162860 | val_loss: 0.97000257\n",
            "[>] epoch #25, batch #112: loss: 0.51522547 | val_loss: 0.97053574\n",
            "[>] epoch #25, batch #114: loss: 0.33325514 | val_loss: 0.96327697\n",
            "[>] epoch #25, batch #116: loss: 0.29411957 | val_loss: 0.97572553\n",
            "[>] epoch #25, batch #118: loss: 0.45615718 | val_loss: 0.95487110\n",
            "[>] epoch #25, batch #120: loss: 0.35590303 | val_loss: 0.94753653\n",
            "[>] epoch #25, batch #122: loss: 0.31726077 | val_loss: 0.95383511\n",
            "[>] epoch #25, batch #124: loss: 0.41764945 | val_loss: 0.96632599\n",
            "[>] epoch #25, batch #126: loss: 0.44989476 | val_loss: 0.94987205\n",
            "[>] epoch #25, batch #128: loss: 0.36572924 | val_loss: 0.96759317\n",
            "[>] epoch #25, batch #130: loss: 0.38646072 | val_loss: 0.96800426\n",
            "[>] epoch #25, batch #132: loss: 0.34318501 | val_loss: 0.96162869\n",
            "[>] epoch #25, batch #134: loss: 0.43197671 | val_loss: 0.95398565\n",
            "[>] epoch #25, batch #136: loss: 0.38900745 | val_loss: 0.95998985\n",
            "[>] epoch #25, batch #138: loss: 0.36117807 | val_loss: 0.95024552\n",
            "[>] epoch #25, batch #140: loss: 0.39898789 | val_loss: 0.97308426\n",
            "[>] epoch #26, batch #  1: loss: 0.31516966 | val_loss: 0.94862621\n",
            "[>] epoch #26, batch #  3: loss: 0.36216369 | val_loss: 0.96275632\n",
            "[>] epoch #26, batch #  5: loss: 0.50216585 | val_loss: 0.96283145\n",
            "[>] epoch #26, batch #  7: loss: 0.29220119 | val_loss: 0.94092723\n",
            "[>] epoch #26, batch #  9: loss: 0.27830043 | val_loss: 0.94991809\n",
            "[>] epoch #26, batch # 11: loss: 0.41645816 | val_loss: 0.93973061\n",
            "[>] epoch #26, batch # 13: loss: 0.37018362 | val_loss: 0.97230735\n",
            "[>] epoch #26, batch # 15: loss: 0.30392221 | val_loss: 0.95343756\n",
            "[>] epoch #26, batch # 17: loss: 0.41428316 | val_loss: 0.94244933\n",
            "[>] epoch #26, batch # 19: loss: 0.36797470 | val_loss: 0.96484751\n",
            "[>] epoch #26, batch # 21: loss: 0.31919158 | val_loss: 0.95607503\n",
            "[>] epoch #26, batch # 23: loss: 0.31383988 | val_loss: 0.94386278\n",
            "[>] epoch #26, batch # 25: loss: 0.35923493 | val_loss: 0.92994225\n",
            "[>] epoch #26, batch # 27: loss: 0.40116325 | val_loss: 0.96136681\n",
            "[>] epoch #26, batch # 29: loss: 0.26112726 | val_loss: 0.95074880\n",
            "[>] epoch #26, batch # 31: loss: 0.34434870 | val_loss: 0.93985534\n",
            "[>] epoch #26, batch # 33: loss: 0.32713953 | val_loss: 0.97212349\n",
            "[>] epoch #26, batch # 35: loss: 0.34203109 | val_loss: 0.98724998\n",
            "[>] epoch #26, batch # 37: loss: 0.30949649 | val_loss: 0.94546419\n",
            "[>] epoch #26, batch # 39: loss: 0.45348462 | val_loss: 0.93451981\n",
            "[>] epoch #26, batch # 41: loss: 0.27628100 | val_loss: 0.95165236\n",
            "[>] epoch #26, batch # 43: loss: 0.29488117 | val_loss: 0.95110334\n",
            "[>] epoch #26, batch # 45: loss: 0.43195873 | val_loss: 0.96152446\n",
            "[>] epoch #26, batch # 47: loss: 0.45728621 | val_loss: 0.96645159\n",
            "[>] epoch #26, batch # 49: loss: 0.22411869 | val_loss: 0.97154383\n",
            "[>] epoch #26, batch # 51: loss: 0.24785990 | val_loss: 0.95464555\n",
            "[>] epoch #26, batch # 53: loss: 0.39489159 | val_loss: 0.93766292\n",
            "[>] epoch #26, batch # 55: loss: 0.34465104 | val_loss: 0.93279587\n",
            "[>] epoch #26, batch # 57: loss: 0.25256959 | val_loss: 0.96753281\n",
            "[>] epoch #26, batch # 59: loss: 0.30991530 | val_loss: 0.93064362\n",
            "[>] epoch #26, batch # 61: loss: 0.33442539 | val_loss: 0.93715696\n",
            "[>] epoch #26, batch # 63: loss: 0.45058325 | val_loss: 0.94576073\n",
            "[>] epoch #26, batch # 65: loss: 0.25946051 | val_loss: 0.94120415\n",
            "[>] epoch #26, batch # 67: loss: 0.34454754 | val_loss: 0.94559968\n",
            "[>] epoch #26, batch # 69: loss: 0.37920201 | val_loss: 0.95138442\n",
            "[>] epoch #26, batch # 71: loss: 0.37856761 | val_loss: 0.96997929\n",
            "[>] epoch #26, batch # 73: loss: 0.36292908 | val_loss: 0.97084786\n",
            "[>] epoch #26, batch # 75: loss: 0.34921077 | val_loss: 0.96802788\n",
            "[>] epoch #26, batch # 77: loss: 0.36501116 | val_loss: 0.92636463\n",
            "[>] epoch #26, batch # 79: loss: 0.34005859 | val_loss: 0.94810626\n",
            "[>] epoch #26, batch # 81: loss: 0.34442908 | val_loss: 0.93840169\n",
            "[>] epoch #26, batch # 83: loss: 0.31228501 | val_loss: 0.95641463\n",
            "[>] epoch #26, batch # 85: loss: 0.46028265 | val_loss: 0.89467531\n",
            "[>] epoch #26, batch # 87: loss: 0.41554719 | val_loss: 0.91535139\n",
            "[>] epoch #26, batch # 89: loss: 0.29035467 | val_loss: 0.94655876\n",
            "[>] epoch #26, batch # 91: loss: 0.37865886 | val_loss: 0.95800565\n",
            "[>] epoch #26, batch # 93: loss: 0.48584625 | val_loss: 0.93937979\n",
            "[>] epoch #26, batch # 95: loss: 0.36115789 | val_loss: 0.94437307\n",
            "[>] epoch #26, batch # 97: loss: 0.29693016 | val_loss: 0.92782526\n",
            "[>] epoch #26, batch # 99: loss: 0.37151316 | val_loss: 0.91586339\n",
            "[>] epoch #26, batch #101: loss: 0.29738250 | val_loss: 0.92618924\n",
            "[>] epoch #26, batch #103: loss: 0.33832037 | val_loss: 0.91921270\n",
            "[>] epoch #26, batch #105: loss: 0.37440813 | val_loss: 0.97358684\n",
            "[>] epoch #26, batch #107: loss: 0.27843666 | val_loss: 0.92105707\n",
            "[>] epoch #26, batch #109: loss: 0.35124439 | val_loss: 0.92307457\n",
            "[>] epoch #26, batch #111: loss: 0.29779917 | val_loss: 0.92780412\n",
            "[>] epoch #26, batch #113: loss: 0.30375168 | val_loss: 0.95086621\n",
            "[>] epoch #26, batch #115: loss: 0.30500051 | val_loss: 0.95222215\n",
            "[>] epoch #26, batch #117: loss: 0.32741314 | val_loss: 0.94357886\n",
            "[>] epoch #26, batch #119: loss: 0.37085900 | val_loss: 0.94445386\n",
            "[>] epoch #26, batch #121: loss: 0.40023297 | val_loss: 0.93930930\n",
            "[>] epoch #26, batch #123: loss: 0.40970248 | val_loss: 0.95125645\n",
            "[>] epoch #26, batch #125: loss: 0.37101790 | val_loss: 0.93893304\n",
            "[>] epoch #26, batch #127: loss: 0.43246570 | val_loss: 0.96508063\n",
            "[>] epoch #26, batch #129: loss: 0.41622245 | val_loss: 0.95710279\n",
            "[>] epoch #26, batch #131: loss: 0.35461035 | val_loss: 0.95594437\n",
            "[>] epoch #26, batch #133: loss: 0.32013965 | val_loss: 0.94923954\n",
            "[>] epoch #26, batch #135: loss: 0.28347501 | val_loss: 0.95389964\n",
            "[>] epoch #26, batch #137: loss: 0.34995037 | val_loss: 0.96021873\n",
            "[>] epoch #26, batch #139: loss: 0.48128831 | val_loss: 0.94736913\n",
            "[>] epoch #26, batch #141: loss: 0.25019711 | val_loss: 0.95177539\n",
            "[>] epoch #27, batch #  2: loss: 0.27870920 | val_loss: 0.97907971\n",
            "[>] epoch #27, batch #  4: loss: 0.39903605 | val_loss: 0.95259558\n",
            "[>] epoch #27, batch #  6: loss: 0.27816361 | val_loss: 0.94822424\n",
            "[>] epoch #27, batch #  8: loss: 0.24640246 | val_loss: 0.91821086\n",
            "[>] epoch #27, batch # 10: loss: 0.34716329 | val_loss: 0.93901376\n",
            "[>] epoch #27, batch # 12: loss: 0.30292457 | val_loss: 0.97362496\n",
            "[>] epoch #27, batch # 14: loss: 0.34268335 | val_loss: 0.95575756\n",
            "[>] epoch #27, batch # 16: loss: 0.39122254 | val_loss: 0.93663315\n",
            "[>] epoch #27, batch # 18: loss: 0.22708707 | val_loss: 0.96384573\n",
            "[>] epoch #27, batch # 20: loss: 0.31810921 | val_loss: 0.94797281\n",
            "[>] epoch #27, batch # 22: loss: 0.35338074 | val_loss: 0.95151287\n",
            "[>] epoch #27, batch # 24: loss: 0.30159840 | val_loss: 0.93751768\n",
            "[>] epoch #27, batch # 26: loss: 0.37049246 | val_loss: 0.94188327\n",
            "[>] epoch #27, batch # 28: loss: 0.25235569 | val_loss: 0.94718798\n",
            "[>] epoch #27, batch # 30: loss: 0.26994079 | val_loss: 0.94806756\n",
            "[>] epoch #27, batch # 32: loss: 0.28101662 | val_loss: 0.97251895\n",
            "[>] epoch #27, batch # 34: loss: 0.34685108 | val_loss: 0.94044106\n",
            "[>] epoch #27, batch # 36: loss: 0.29602644 | val_loss: 0.96232387\n",
            "[>] epoch #27, batch # 38: loss: 0.29748809 | val_loss: 0.94008614\n",
            "[>] epoch #27, batch # 40: loss: 0.35663861 | val_loss: 0.93843329\n",
            "[>] epoch #27, batch # 42: loss: 0.25961423 | val_loss: 0.93920563\n",
            "[>] epoch #27, batch # 44: loss: 0.29142967 | val_loss: 0.92491101\n",
            "[>] epoch #27, batch # 46: loss: 0.32528257 | val_loss: 0.92800188\n",
            "[>] epoch #27, batch # 48: loss: 0.27385285 | val_loss: 0.96446044\n",
            "[>] epoch #27, batch # 50: loss: 0.38277093 | val_loss: 0.94534443\n",
            "[>] epoch #27, batch # 52: loss: 0.33942896 | val_loss: 0.94232203\n",
            "[>] epoch #27, batch # 54: loss: 0.34179384 | val_loss: 0.93044764\n",
            "[>] epoch #27, batch # 56: loss: 0.30523100 | val_loss: 0.93495655\n",
            "[>] epoch #27, batch # 58: loss: 0.43639311 | val_loss: 0.95187408\n",
            "[>] epoch #27, batch # 60: loss: 0.33089215 | val_loss: 0.96025390\n",
            "[>] epoch #27, batch # 62: loss: 0.30448300 | val_loss: 0.96897106\n",
            "[>] epoch #27, batch # 64: loss: 0.29953316 | val_loss: 0.95645342\n",
            "[>] epoch #27, batch # 66: loss: 0.34344539 | val_loss: 0.94032937\n",
            "[>] epoch #27, batch # 68: loss: 0.37151888 | val_loss: 0.95214639\n",
            "[>] epoch #27, batch # 70: loss: 0.46908531 | val_loss: 0.91189975\n",
            "[>] epoch #27, batch # 72: loss: 0.25628674 | val_loss: 0.94318506\n",
            "[>] epoch #27, batch # 74: loss: 0.32172906 | val_loss: 0.94496408\n",
            "[>] epoch #27, batch # 76: loss: 0.26886466 | val_loss: 0.98266324\n",
            "[>] epoch #27, batch # 78: loss: 0.36556712 | val_loss: 0.94392097\n",
            "[>] epoch #27, batch # 80: loss: 0.30055228 | val_loss: 0.93851428\n",
            "[>] epoch #27, batch # 82: loss: 0.30154228 | val_loss: 0.95002067\n",
            "[>] epoch #27, batch # 84: loss: 0.26085547 | val_loss: 0.93498604\n",
            "[>] epoch #27, batch # 86: loss: 0.31171924 | val_loss: 0.94982178\n",
            "[>] epoch #27, batch # 88: loss: 0.44892335 | val_loss: 0.95263897\n",
            "[>] epoch #27, batch # 90: loss: 0.25290793 | val_loss: 0.95770559\n",
            "[>] epoch #27, batch # 92: loss: 0.27187657 | val_loss: 0.93824885\n",
            "[>] epoch #27, batch # 94: loss: 0.30647597 | val_loss: 0.92630192\n",
            "[>] epoch #27, batch # 96: loss: 0.26594302 | val_loss: 0.95413432\n",
            "[>] epoch #27, batch # 98: loss: 0.35586902 | val_loss: 0.99091246\n",
            "[>] epoch #27, batch #100: loss: 0.31891045 | val_loss: 0.93191610\n",
            "[>] epoch #27, batch #102: loss: 0.32525459 | val_loss: 0.92414267\n",
            "[>] epoch #27, batch #104: loss: 0.34056938 | val_loss: 0.96057772\n",
            "[>] epoch #27, batch #106: loss: 0.36544716 | val_loss: 0.92882270\n",
            "[>] epoch #27, batch #108: loss: 0.31059635 | val_loss: 0.92394622\n",
            "[>] epoch #27, batch #110: loss: 0.31548342 | val_loss: 0.96222291\n",
            "[>] epoch #27, batch #112: loss: 0.39444828 | val_loss: 0.94101880\n",
            "[>] epoch #27, batch #114: loss: 0.33667627 | val_loss: 0.94690659\n",
            "[>] epoch #27, batch #116: loss: 0.42671141 | val_loss: 0.94736557\n",
            "[>] epoch #27, batch #118: loss: 0.31886736 | val_loss: 0.92233848\n",
            "[>] epoch #27, batch #120: loss: 0.39549890 | val_loss: 0.94107886\n",
            "[>] epoch #27, batch #122: loss: 0.28479895 | val_loss: 0.96071350\n",
            "[>] epoch #27, batch #124: loss: 0.33190253 | val_loss: 0.94988097\n",
            "[>] epoch #27, batch #126: loss: 0.26653770 | val_loss: 0.95059076\n",
            "[>] epoch #27, batch #128: loss: 0.40532732 | val_loss: 0.92280023\n",
            "[>] epoch #27, batch #130: loss: 0.45127699 | val_loss: 0.94893725\n",
            "[>] epoch #27, batch #132: loss: 0.45121965 | val_loss: 0.97187940\n",
            "[>] epoch #27, batch #134: loss: 0.41137886 | val_loss: 0.93417264\n",
            "[>] epoch #27, batch #136: loss: 0.35200077 | val_loss: 0.92118527\n",
            "[>] epoch #27, batch #138: loss: 0.35052744 | val_loss: 0.92307631\n",
            "[>] epoch #27, batch #140: loss: 0.36814198 | val_loss: 0.91984482\n",
            "[>] epoch #28, batch #  1: loss: 0.24120347 | val_loss: 0.94319545\n",
            "[>] epoch #28, batch #  3: loss: 0.33314687 | val_loss: 0.92894080\n",
            "[>] epoch #28, batch #  5: loss: 0.28967503 | val_loss: 0.93886398\n",
            "[>] epoch #28, batch #  7: loss: 0.23151977 | val_loss: 0.93192086\n",
            "[>] epoch #28, batch #  9: loss: 0.24164534 | val_loss: 0.94591114\n",
            "[>] epoch #28, batch # 11: loss: 0.27567768 | val_loss: 0.96800461\n",
            "[>] epoch #28, batch # 13: loss: 0.34298226 | val_loss: 0.93474299\n",
            "[>] epoch #28, batch # 15: loss: 0.32665217 | val_loss: 0.94457894\n",
            "[>] epoch #28, batch # 17: loss: 0.33427745 | val_loss: 0.90981394\n",
            "[>] epoch #28, batch # 19: loss: 0.24356969 | val_loss: 0.92646578\n",
            "[>] epoch #28, batch # 21: loss: 0.28757146 | val_loss: 0.93987779\n",
            "[>] epoch #28, batch # 23: loss: 0.35326862 | val_loss: 0.95223221\n",
            "[>] epoch #28, batch # 25: loss: 0.34341922 | val_loss: 0.92566069\n",
            "[>] epoch #28, batch # 27: loss: 0.28224829 | val_loss: 0.94028092\n",
            "[>] epoch #28, batch # 29: loss: 0.29958296 | val_loss: 0.95556447\n",
            "[>] epoch #28, batch # 31: loss: 0.21635105 | val_loss: 0.93662496\n",
            "[>] epoch #28, batch # 33: loss: 0.32288465 | val_loss: 0.99565665\n",
            "[>] epoch #28, batch # 35: loss: 0.28957248 | val_loss: 0.95668309\n",
            "[>] epoch #28, batch # 37: loss: 0.35172686 | val_loss: 0.94271913\n",
            "[>] epoch #28, batch # 39: loss: 0.31856549 | val_loss: 0.89382179\n",
            "[>] epoch #28, batch # 41: loss: 0.31851196 | val_loss: 0.94129061\n",
            "[>] epoch #28, batch # 43: loss: 0.36624190 | val_loss: 0.91549679\n",
            "[>] epoch #28, batch # 45: loss: 0.31402257 | val_loss: 0.95034481\n",
            "[>] epoch #28, batch # 47: loss: 0.36210388 | val_loss: 0.95962300\n",
            "[>] epoch #28, batch # 49: loss: 0.39277622 | val_loss: 0.93709073\n",
            "[>] epoch #28, batch # 51: loss: 0.37705651 | val_loss: 0.95117622\n",
            "[>] epoch #28, batch # 53: loss: 0.29331011 | val_loss: 0.90777101\n",
            "[>] epoch #28, batch # 55: loss: 0.32170668 | val_loss: 0.91558690\n",
            "[>] epoch #28, batch # 57: loss: 0.30995670 | val_loss: 0.90593632\n",
            "[>] epoch #28, batch # 59: loss: 0.30474663 | val_loss: 0.93076846\n",
            "[>] epoch #28, batch # 61: loss: 0.28481880 | val_loss: 0.93957959\n",
            "[>] epoch #28, batch # 63: loss: 0.34604347 | val_loss: 0.93479465\n",
            "[>] epoch #28, batch # 65: loss: 0.26671556 | val_loss: 0.93292889\n",
            "[>] epoch #28, batch # 67: loss: 0.27572855 | val_loss: 0.93113570\n",
            "[>] epoch #28, batch # 69: loss: 0.30317873 | val_loss: 0.93682336\n",
            "[>] epoch #28, batch # 71: loss: 0.36110538 | val_loss: 0.93049707\n",
            "[>] epoch #28, batch # 73: loss: 0.32606420 | val_loss: 0.90171119\n",
            "[>] epoch #28, batch # 75: loss: 0.32478827 | val_loss: 0.91913394\n",
            "[>] epoch #28, batch # 77: loss: 0.33901027 | val_loss: 0.95075590\n",
            "[>] epoch #28, batch # 79: loss: 0.31061193 | val_loss: 0.96399697\n",
            "[>] epoch #28, batch # 81: loss: 0.28671566 | val_loss: 0.93111480\n",
            "[>] epoch #28, batch # 83: loss: 0.27374306 | val_loss: 0.94492456\n",
            "[>] epoch #28, batch # 85: loss: 0.32265368 | val_loss: 0.90169960\n",
            "[>] epoch #28, batch # 87: loss: 0.30748621 | val_loss: 0.89969962\n",
            "[>] epoch #28, batch # 89: loss: 0.34423426 | val_loss: 0.93572320\n",
            "[>] epoch #28, batch # 91: loss: 0.25241634 | val_loss: 0.95856012\n",
            "[>] epoch #28, batch # 93: loss: 0.30477604 | val_loss: 0.95666833\n",
            "[>] epoch #28, batch # 95: loss: 0.32247061 | val_loss: 0.94451881\n",
            "[>] epoch #28, batch # 97: loss: 0.29793698 | val_loss: 0.93839873\n",
            "[>] epoch #28, batch # 99: loss: 0.25891483 | val_loss: 0.91972978\n",
            "[>] epoch #28, batch #101: loss: 0.38264203 | val_loss: 0.94903056\n",
            "[>] epoch #28, batch #103: loss: 0.29339272 | val_loss: 0.96158403\n",
            "[>] epoch #28, batch #105: loss: 0.30832529 | val_loss: 0.93299418\n",
            "[>] epoch #28, batch #107: loss: 0.37284270 | val_loss: 0.98818471\n",
            "[>] epoch #28, batch #109: loss: 0.35797930 | val_loss: 0.96617715\n",
            "[>] epoch #28, batch #111: loss: 0.34583879 | val_loss: 0.96343907\n",
            "[>] epoch #28, batch #113: loss: 0.28439102 | val_loss: 0.95918292\n",
            "[>] epoch #28, batch #115: loss: 0.31709722 | val_loss: 0.94858630\n",
            "[>] epoch #28, batch #117: loss: 0.33379099 | val_loss: 0.94051543\n",
            "[>] epoch #28, batch #119: loss: 0.33876044 | val_loss: 0.92125982\n",
            "[>] epoch #28, batch #121: loss: 0.32611191 | val_loss: 0.91276317\n",
            "[>] epoch #28, batch #123: loss: 0.26944691 | val_loss: 0.94880015\n",
            "[>] epoch #28, batch #125: loss: 0.36657512 | val_loss: 0.94985257\n",
            "[>] epoch #28, batch #127: loss: 0.33167383 | val_loss: 0.92618304\n",
            "[>] epoch #28, batch #129: loss: 0.38317764 | val_loss: 0.93578287\n",
            "[>] epoch #28, batch #131: loss: 0.51451164 | val_loss: 0.94424698\n",
            "[>] epoch #28, batch #133: loss: 0.30313697 | val_loss: 0.95067095\n",
            "[>] epoch #28, batch #135: loss: 0.30147552 | val_loss: 0.92849753\n",
            "[>] epoch #28, batch #137: loss: 0.28248134 | val_loss: 0.96069556\n",
            "[>] epoch #28, batch #139: loss: 0.28896064 | val_loss: 0.94515378\n",
            "[>] epoch #28, batch #141: loss: 0.43648821 | val_loss: 0.91793319\n",
            "[>] epoch #29, batch #  2: loss: 0.20567228 | val_loss: 0.93959657\n",
            "[>] epoch #29, batch #  4: loss: 0.28186807 | val_loss: 0.96528581\n",
            "[>] epoch #29, batch #  6: loss: 0.22597043 | val_loss: 0.95895474\n",
            "[>] epoch #29, batch #  8: loss: 0.32708523 | val_loss: 0.95718104\n",
            "[>] epoch #29, batch # 10: loss: 0.28409410 | val_loss: 0.95565690\n",
            "[>] epoch #29, batch # 12: loss: 0.33550310 | val_loss: 0.95108353\n",
            "[>] epoch #29, batch # 14: loss: 0.23819368 | val_loss: 0.95438650\n",
            "[>] epoch #29, batch # 16: loss: 0.27167353 | val_loss: 0.94592432\n",
            "[>] epoch #29, batch # 18: loss: 0.38469759 | val_loss: 0.95439727\n",
            "[>] epoch #29, batch # 20: loss: 0.29443842 | val_loss: 0.96829506\n",
            "[>] epoch #29, batch # 22: loss: 0.21630830 | val_loss: 0.93179506\n",
            "[>] epoch #29, batch # 24: loss: 0.24259721 | val_loss: 0.95002788\n",
            "[>] epoch #29, batch # 26: loss: 0.27410546 | val_loss: 0.95838356\n",
            "[>] epoch #29, batch # 28: loss: 0.28777960 | val_loss: 0.94974320\n",
            "[>] epoch #29, batch # 30: loss: 0.26349476 | val_loss: 0.91972100\n",
            "[>] epoch #29, batch # 32: loss: 0.27010754 | val_loss: 0.94582416\n",
            "[>] epoch #29, batch # 34: loss: 0.26589674 | val_loss: 0.98814216\n",
            "[>] epoch #29, batch # 36: loss: 0.27637434 | val_loss: 1.00770974\n",
            "[>] epoch #29, batch # 38: loss: 0.20438844 | val_loss: 0.91413055\n",
            "[>] epoch #29, batch # 40: loss: 0.35623550 | val_loss: 0.93046154\n",
            "[>] epoch #29, batch # 42: loss: 0.38097095 | val_loss: 0.95989299\n",
            "[>] epoch #29, batch # 44: loss: 0.23947653 | val_loss: 0.94773354\n",
            "[>] epoch #29, batch # 46: loss: 0.26090118 | val_loss: 0.91351462\n",
            "[>] epoch #29, batch # 48: loss: 0.38776678 | val_loss: 0.92774675\n",
            "[>] epoch #29, batch # 50: loss: 0.22269416 | val_loss: 0.95302562\n",
            "[>] epoch #29, batch # 52: loss: 0.45016298 | val_loss: 0.94244753\n",
            "[>] epoch #29, batch # 54: loss: 0.23329738 | val_loss: 0.93299755\n",
            "[>] epoch #29, batch # 56: loss: 0.24811620 | val_loss: 0.93724845\n",
            "[>] epoch #29, batch # 58: loss: 0.31533441 | val_loss: 0.91985679\n",
            "[>] epoch #29, batch # 60: loss: 0.33837259 | val_loss: 0.94264226\n",
            "[>] epoch #29, batch # 62: loss: 0.34348467 | val_loss: 0.92605559\n",
            "[>] epoch #29, batch # 64: loss: 0.28348744 | val_loss: 0.93743587\n",
            "[>] epoch #29, batch # 66: loss: 0.21678196 | val_loss: 0.94334433\n",
            "[>] epoch #29, batch # 68: loss: 0.33611730 | val_loss: 0.96642165\n",
            "[>] epoch #29, batch # 70: loss: 0.28296018 | val_loss: 0.97355655\n",
            "[>] epoch #29, batch # 72: loss: 0.37409699 | val_loss: 0.93805332\n",
            "[>] epoch #29, batch # 74: loss: 0.36579263 | val_loss: 0.93026972\n",
            "[>] epoch #29, batch # 76: loss: 0.26602933 | val_loss: 0.89953187\n",
            "[>] epoch #29, batch # 78: loss: 0.29879716 | val_loss: 0.93161223\n",
            "[>] epoch #29, batch # 80: loss: 0.29116783 | val_loss: 0.94658827\n",
            "[>] epoch #29, batch # 82: loss: 0.24137881 | val_loss: 0.97003360\n",
            "[>] epoch #29, batch # 84: loss: 0.23977222 | val_loss: 0.95871384\n",
            "[>] epoch #29, batch # 86: loss: 0.26407602 | val_loss: 0.94197884\n",
            "[>] epoch #29, batch # 88: loss: 0.26942262 | val_loss: 0.96158284\n",
            "[>] epoch #29, batch # 90: loss: 0.30499753 | val_loss: 0.91995263\n",
            "[>] epoch #29, batch # 92: loss: 0.30840099 | val_loss: 0.94592681\n",
            "[>] epoch #29, batch # 94: loss: 0.32958806 | val_loss: 0.93612663\n",
            "[>] epoch #29, batch # 96: loss: 0.31339845 | val_loss: 0.90413547\n",
            "[>] epoch #29, batch # 98: loss: 0.33171892 | val_loss: 0.92904059\n",
            "[>] epoch #29, batch #100: loss: 0.22989444 | val_loss: 0.94036707\n",
            "[>] epoch #29, batch #102: loss: 0.29403707 | val_loss: 0.92223654\n",
            "[>] epoch #29, batch #104: loss: 0.24651548 | val_loss: 0.92142100\n",
            "[>] epoch #29, batch #106: loss: 0.28888059 | val_loss: 0.94632431\n",
            "[>] epoch #29, batch #108: loss: 0.44618177 | val_loss: 0.93560492\n",
            "[>] epoch #29, batch #110: loss: 0.25260946 | val_loss: 0.95628549\n",
            "[>] epoch #29, batch #112: loss: 0.31148601 | val_loss: 0.94316771\n",
            "[>] epoch #29, batch #114: loss: 0.25223431 | val_loss: 0.94246627\n",
            "[>] epoch #29, batch #116: loss: 0.24780339 | val_loss: 0.94758121\n",
            "[>] epoch #29, batch #118: loss: 0.29074499 | val_loss: 0.92726077\n",
            "[>] epoch #29, batch #120: loss: 0.24090011 | val_loss: 0.97443266\n",
            "[>] epoch #29, batch #122: loss: 0.34557280 | val_loss: 0.93641053\n",
            "[>] epoch #29, batch #124: loss: 0.27711749 | val_loss: 0.96341207\n",
            "[>] epoch #29, batch #126: loss: 0.32242623 | val_loss: 0.94005600\n",
            "[>] epoch #29, batch #128: loss: 0.30158964 | val_loss: 0.92432817\n",
            "[>] epoch #29, batch #130: loss: 0.22017455 | val_loss: 0.97188655\n",
            "[>] epoch #29, batch #132: loss: 0.24252361 | val_loss: 0.90058331\n",
            "[>] epoch #29, batch #134: loss: 0.32138318 | val_loss: 0.92503152\n",
            "[>] epoch #29, batch #136: loss: 0.25037003 | val_loss: 0.93944037\n",
            "[>] epoch #29, batch #138: loss: 0.33602858 | val_loss: 0.92963249\n",
            "[>] epoch #29, batch #140: loss: 0.25438133 | val_loss: 0.94510367\n",
            "[>] epoch #30, batch #  1: loss: 0.23008752 | val_loss: 0.96979219\n",
            "[>] epoch #30, batch #  3: loss: 0.21809904 | val_loss: 0.94836481\n",
            "[>] epoch #30, batch #  5: loss: 0.22867744 | val_loss: 0.94794907\n",
            "[>] epoch #30, batch #  7: loss: 0.23235115 | val_loss: 0.94367163\n",
            "[>] epoch #30, batch #  9: loss: 0.27972832 | val_loss: 0.96595167\n",
            "[>] epoch #30, batch # 11: loss: 0.35175526 | val_loss: 0.92285140\n",
            "[>] epoch #30, batch # 13: loss: 0.26224396 | val_loss: 0.96883051\n",
            "[>] epoch #30, batch # 15: loss: 0.27120665 | val_loss: 0.93567095\n",
            "[>] epoch #30, batch # 17: loss: 0.32148981 | val_loss: 0.94829146\n",
            "[>] epoch #30, batch # 19: loss: 0.19380131 | val_loss: 0.93088902\n",
            "[>] epoch #30, batch # 21: loss: 0.30146500 | val_loss: 0.94900320\n",
            "[>] epoch #30, batch # 23: loss: 0.15538466 | val_loss: 0.92934097\n",
            "[>] epoch #30, batch # 25: loss: 0.29252642 | val_loss: 0.94734024\n",
            "[>] epoch #30, batch # 27: loss: 0.23870844 | val_loss: 0.92015897\n",
            "[>] epoch #30, batch # 29: loss: 0.28618464 | val_loss: 0.95282723\n",
            "[>] epoch #30, batch # 31: loss: 0.32272956 | val_loss: 0.92939563\n",
            "[>] epoch #30, batch # 33: loss: 0.24219784 | val_loss: 0.93988853\n",
            "[>] epoch #30, batch # 35: loss: 0.24692154 | val_loss: 0.97132037\n",
            "[>] epoch #30, batch # 37: loss: 0.21642141 | val_loss: 0.95288415\n",
            "[>] epoch #30, batch # 39: loss: 0.31623784 | val_loss: 0.94343711\n",
            "[>] epoch #30, batch # 41: loss: 0.31456721 | val_loss: 0.94474674\n",
            "[>] epoch #30, batch # 43: loss: 0.19581296 | val_loss: 0.91800579\n",
            "[>] epoch #30, batch # 45: loss: 0.25323135 | val_loss: 0.95648980\n",
            "[>] epoch #30, batch # 47: loss: 0.25145566 | val_loss: 0.94463753\n",
            "[>] epoch #30, batch # 49: loss: 0.29887435 | val_loss: 0.92825094\n",
            "[>] epoch #30, batch # 51: loss: 0.20771334 | val_loss: 0.96161209\n",
            "[>] epoch #30, batch # 53: loss: 0.30682120 | val_loss: 0.93957774\n",
            "[>] epoch #30, batch # 55: loss: 0.22313136 | val_loss: 0.93005507\n",
            "[>] epoch #30, batch # 57: loss: 0.38756555 | val_loss: 0.93990024\n",
            "[>] epoch #30, batch # 59: loss: 0.21233672 | val_loss: 0.94344978\n",
            "[>] epoch #30, batch # 61: loss: 0.31369898 | val_loss: 0.95592591\n",
            "[>] epoch #30, batch # 63: loss: 0.20907544 | val_loss: 0.93631558\n",
            "[>] epoch #30, batch # 65: loss: 0.23246552 | val_loss: 0.95202684\n",
            "[>] epoch #30, batch # 67: loss: 0.25755486 | val_loss: 0.93144314\n",
            "[>] epoch #30, batch # 69: loss: 0.21054998 | val_loss: 0.94528398\n",
            "[>] epoch #30, batch # 71: loss: 0.22022071 | val_loss: 0.92205099\n",
            "[>] epoch #30, batch # 73: loss: 0.23504700 | val_loss: 0.94836240\n",
            "[>] epoch #30, batch # 75: loss: 0.34015900 | val_loss: 0.96124766\n",
            "[>] epoch #30, batch # 77: loss: 0.27307943 | val_loss: 0.92232658\n",
            "[>] epoch #30, batch # 79: loss: 0.24421304 | val_loss: 0.91689316\n",
            "[>] epoch #30, batch # 81: loss: 0.28308991 | val_loss: 0.98519779\n",
            "[>] epoch #30, batch # 83: loss: 0.28350985 | val_loss: 0.98627803\n",
            "[>] epoch #30, batch # 85: loss: 0.28932878 | val_loss: 0.92546473\n",
            "[>] epoch #30, batch # 87: loss: 0.28159112 | val_loss: 0.96554271\n",
            "[>] epoch #30, batch # 89: loss: 0.28826281 | val_loss: 0.97025587\n",
            "[>] epoch #30, batch # 91: loss: 0.27545214 | val_loss: 0.94269298\n",
            "[>] epoch #30, batch # 93: loss: 0.30468583 | val_loss: 0.91920954\n",
            "[>] epoch #30, batch # 95: loss: 0.24640526 | val_loss: 0.94031103\n",
            "[>] epoch #30, batch # 97: loss: 0.27726117 | val_loss: 0.94208246\n",
            "[>] epoch #30, batch # 99: loss: 0.20693675 | val_loss: 0.94608959\n",
            "[>] epoch #30, batch #101: loss: 0.26097602 | val_loss: 0.94892910\n",
            "[>] epoch #30, batch #103: loss: 0.24337445 | val_loss: 0.94212912\n",
            "[>] epoch #30, batch #105: loss: 0.28699815 | val_loss: 0.92581221\n",
            "[>] epoch #30, batch #107: loss: 0.23823558 | val_loss: 0.95709892\n",
            "[>] epoch #30, batch #109: loss: 0.21837333 | val_loss: 0.95447323\n",
            "[>] epoch #30, batch #111: loss: 0.30091405 | val_loss: 0.92656205\n",
            "[>] epoch #30, batch #113: loss: 0.23198883 | val_loss: 0.94116796\n",
            "[>] epoch #30, batch #115: loss: 0.25261143 | val_loss: 0.93535869\n",
            "[>] epoch #30, batch #117: loss: 0.27870968 | val_loss: 0.93620831\n",
            "[>] epoch #30, batch #119: loss: 0.27367514 | val_loss: 0.96186774\n",
            "[>] epoch #30, batch #121: loss: 0.30501619 | val_loss: 0.97921593\n",
            "[>] epoch #30, batch #123: loss: 0.23379758 | val_loss: 0.95748538\n",
            "[>] epoch #30, batch #125: loss: 0.31723523 | val_loss: 0.94360157\n",
            "[>] epoch #30, batch #127: loss: 0.33229187 | val_loss: 0.93141453\n",
            "[>] epoch #30, batch #129: loss: 0.27782950 | val_loss: 0.95455497\n",
            "[>] epoch #30, batch #131: loss: 0.33364692 | val_loss: 0.93153928\n",
            "[>] epoch #30, batch #133: loss: 0.31463143 | val_loss: 0.95932632\n",
            "[>] epoch #30, batch #135: loss: 0.32846728 | val_loss: 0.94600902\n",
            "[>] epoch #30, batch #137: loss: 0.27422717 | val_loss: 0.94225923\n",
            "[>] epoch #30, batch #139: loss: 0.32493210 | val_loss: 0.93029584\n",
            "[>] epoch #30, batch #141: loss: 0.20083898 | val_loss: 0.96675033\n",
            "[>] epoch #31, batch #  2: loss: 0.21937595 | val_loss: 0.92667321\n",
            "[>] epoch #31, batch #  4: loss: 0.26543120 | val_loss: 0.93546772\n",
            "[>] epoch #31, batch #  6: loss: 0.22666697 | val_loss: 0.94709755\n",
            "[>] epoch #31, batch #  8: loss: 0.26779780 | val_loss: 0.94010254\n",
            "[>] epoch #31, batch # 10: loss: 0.25515464 | val_loss: 0.94566508\n",
            "[>] epoch #31, batch # 12: loss: 0.24775590 | val_loss: 0.95124735\n",
            "[>] epoch #31, batch # 14: loss: 0.24940890 | val_loss: 0.96512155\n",
            "[>] epoch #31, batch # 16: loss: 0.24471590 | val_loss: 0.95742744\n",
            "[>] epoch #31, batch # 18: loss: 0.24344614 | val_loss: 0.95707198\n",
            "[>] epoch #31, batch # 20: loss: 0.26637173 | val_loss: 0.98908719\n",
            "[>] epoch #31, batch # 22: loss: 0.30695042 | val_loss: 0.94368688\n",
            "[>] epoch #31, batch # 24: loss: 0.19291234 | val_loss: 0.93595810\n",
            "[>] epoch #31, batch # 26: loss: 0.17283067 | val_loss: 0.95654763\n",
            "[>] epoch #31, batch # 28: loss: 0.30620956 | val_loss: 0.96904020\n",
            "[>] epoch #31, batch # 30: loss: 0.25156194 | val_loss: 0.95363148\n",
            "[>] epoch #31, batch # 32: loss: 0.27704638 | val_loss: 0.94138406\n",
            "[>] epoch #31, batch # 34: loss: 0.29459253 | val_loss: 0.95032501\n",
            "[>] epoch #31, batch # 36: loss: 0.21898176 | val_loss: 0.94785134\n",
            "[>] epoch #31, batch # 38: loss: 0.17896408 | val_loss: 0.95424480\n",
            "[>] epoch #31, batch # 40: loss: 0.23846681 | val_loss: 0.93576810\n",
            "[>] epoch #31, batch # 42: loss: 0.21706033 | val_loss: 0.96397075\n",
            "[>] epoch #31, batch # 44: loss: 0.22036028 | val_loss: 0.94163606\n",
            "[>] epoch #31, batch # 46: loss: 0.20435101 | val_loss: 0.92107287\n",
            "[>] epoch #31, batch # 48: loss: 0.22694543 | val_loss: 0.97306391\n",
            "[>] epoch #31, batch # 50: loss: 0.34624043 | val_loss: 0.93756209\n",
            "[>] epoch #31, batch # 52: loss: 0.25224546 | val_loss: 0.96304173\n",
            "[>] epoch #31, batch # 54: loss: 0.22530751 | val_loss: 0.94402910\n",
            "[>] epoch #31, batch # 56: loss: 0.25760394 | val_loss: 0.94818782\n",
            "[>] epoch #31, batch # 58: loss: 0.27937865 | val_loss: 0.93553269\n",
            "[>] epoch #31, batch # 60: loss: 0.30290678 | val_loss: 0.97758118\n",
            "[>] epoch #31, batch # 62: loss: 0.24786878 | val_loss: 0.98034095\n",
            "[>] epoch #31, batch # 64: loss: 0.31621036 | val_loss: 0.91008050\n",
            "[>] epoch #31, batch # 66: loss: 0.27935392 | val_loss: 0.94671286\n",
            "[>] epoch #31, batch # 68: loss: 0.24966468 | val_loss: 0.94503216\n",
            "[>] epoch #31, batch # 70: loss: 0.39131504 | val_loss: 0.93937238\n",
            "[>] epoch #31, batch # 72: loss: 0.29359365 | val_loss: 0.92769064\n",
            "[>] epoch #31, batch # 74: loss: 0.25601462 | val_loss: 0.99999260\n",
            "[>] epoch #31, batch # 76: loss: 0.41228366 | val_loss: 0.93677076\n",
            "[>] epoch #31, batch # 78: loss: 0.25533473 | val_loss: 0.96449028\n",
            "[>] epoch #31, batch # 80: loss: 0.26911139 | val_loss: 0.94627903\n",
            "[>] epoch #31, batch # 82: loss: 0.26513392 | val_loss: 0.94722522\n",
            "[>] epoch #31, batch # 84: loss: 0.20154215 | val_loss: 0.92820502\n",
            "[>] epoch #31, batch # 86: loss: 0.33244941 | val_loss: 0.91296731\n",
            "[>] epoch #31, batch # 88: loss: 0.32563946 | val_loss: 0.93872552\n",
            "[>] epoch #31, batch # 90: loss: 0.28403097 | val_loss: 0.94902432\n",
            "[>] epoch #31, batch # 92: loss: 0.26791397 | val_loss: 0.92484341\n",
            "[>] epoch #31, batch # 94: loss: 0.30153325 | val_loss: 0.94569118\n",
            "[>] epoch #31, batch # 96: loss: 0.28474766 | val_loss: 0.98768314\n",
            "[>] epoch #31, batch # 98: loss: 0.32413250 | val_loss: 0.94136991\n",
            "[>] epoch #31, batch #100: loss: 0.18658911 | val_loss: 0.91517055\n",
            "[>] epoch #31, batch #102: loss: 0.25041837 | val_loss: 0.90748204\n",
            "[>] epoch #31, batch #104: loss: 0.22265270 | val_loss: 0.94644006\n",
            "[>] epoch #31, batch #106: loss: 0.22170600 | val_loss: 0.92388537\n",
            "[>] epoch #31, batch #108: loss: 0.34185392 | val_loss: 0.94945974\n",
            "[>] epoch #31, batch #110: loss: 0.25174603 | val_loss: 0.92003888\n",
            "[>] epoch #31, batch #112: loss: 0.29749137 | val_loss: 0.94659500\n",
            "[>] epoch #31, batch #114: loss: 0.29012468 | val_loss: 0.95528992\n",
            "[>] epoch #31, batch #116: loss: 0.28105971 | val_loss: 0.93305104\n",
            "[>] epoch #31, batch #118: loss: 0.29883677 | val_loss: 0.96824348\n",
            "[>] epoch #31, batch #120: loss: 0.25268644 | val_loss: 0.93652621\n",
            "[>] epoch #31, batch #122: loss: 0.24138476 | val_loss: 0.94135589\n",
            "[>] epoch #31, batch #124: loss: 0.31422389 | val_loss: 0.99447911\n",
            "[>] epoch #31, batch #126: loss: 0.26304334 | val_loss: 0.94882290\n",
            "[>] epoch #31, batch #128: loss: 0.28131500 | val_loss: 0.94314005\n",
            "[>] epoch #31, batch #130: loss: 0.28670269 | val_loss: 0.94504534\n",
            "[>] epoch #31, batch #132: loss: 0.29023480 | val_loss: 0.94643957\n",
            "[>] epoch #31, batch #134: loss: 0.31333765 | val_loss: 0.90843816\n",
            "[>] epoch #31, batch #136: loss: 0.37431505 | val_loss: 0.95541259\n",
            "[>] epoch #31, batch #138: loss: 0.29579440 | val_loss: 0.95096352\n",
            "[>] epoch #31, batch #140: loss: 0.28518805 | val_loss: 0.98044612\n",
            "[>] epoch #32, batch #  1: loss: 0.22774798 | val_loss: 0.90939653\n",
            "[>] epoch #32, batch #  3: loss: 0.19335924 | val_loss: 0.91890752\n",
            "[>] epoch #32, batch #  5: loss: 0.21639171 | val_loss: 0.93946246\n",
            "[>] epoch #32, batch #  7: loss: 0.21685056 | val_loss: 0.93206905\n",
            "[>] epoch #32, batch #  9: loss: 0.22314201 | val_loss: 0.95823753\n",
            "[>] epoch #32, batch # 11: loss: 0.19153117 | val_loss: 0.94690325\n",
            "[>] epoch #32, batch # 13: loss: 0.30383295 | val_loss: 0.94536095\n",
            "[>] epoch #32, batch # 15: loss: 0.27314013 | val_loss: 0.93316162\n",
            "[>] epoch #32, batch # 17: loss: 0.21177946 | val_loss: 0.94733452\n",
            "[>] epoch #32, batch # 19: loss: 0.18407980 | val_loss: 0.94321120\n",
            "[>] epoch #32, batch # 21: loss: 0.19341776 | val_loss: 0.95447126\n",
            "[>] epoch #32, batch # 23: loss: 0.30319026 | val_loss: 0.97517559\n",
            "[>] epoch #32, batch # 25: loss: 0.19360615 | val_loss: 0.97766294\n",
            "[>] epoch #32, batch # 27: loss: 0.28270769 | val_loss: 0.95360380\n",
            "[>] epoch #32, batch # 29: loss: 0.20639417 | val_loss: 0.96293786\n",
            "[>] epoch #32, batch # 31: loss: 0.22708566 | val_loss: 0.96881994\n",
            "[>] epoch #32, batch # 33: loss: 0.23268224 | val_loss: 0.97218461\n",
            "[>] epoch #32, batch # 35: loss: 0.27591553 | val_loss: 0.97688492\n",
            "[>] epoch #32, batch # 37: loss: 0.23510602 | val_loss: 0.95164673\n",
            "[>] epoch #32, batch # 39: loss: 0.33423358 | val_loss: 0.96678230\n",
            "[>] epoch #32, batch # 41: loss: 0.17578003 | val_loss: 0.94959402\n",
            "[>] epoch #32, batch # 43: loss: 0.25356835 | val_loss: 0.96013049\n",
            "[>] epoch #32, batch # 45: loss: 0.27186742 | val_loss: 0.95756660\n",
            "[>] epoch #32, batch # 47: loss: 0.23000079 | val_loss: 0.95763635\n",
            "[>] epoch #32, batch # 49: loss: 0.21259464 | val_loss: 0.94606624\n",
            "[>] epoch #32, batch # 51: loss: 0.20790663 | val_loss: 0.94841270\n",
            "[>] epoch #32, batch # 53: loss: 0.21028878 | val_loss: 0.95554706\n",
            "[>] epoch #32, batch # 55: loss: 0.21463493 | val_loss: 0.95264524\n",
            "[>] epoch #32, batch # 57: loss: 0.32662243 | val_loss: 0.98714718\n",
            "[>] epoch #32, batch # 59: loss: 0.24574320 | val_loss: 0.92410098\n",
            "[>] epoch #32, batch # 61: loss: 0.24120435 | val_loss: 0.95254904\n",
            "[>] epoch #32, batch # 63: loss: 0.18046933 | val_loss: 0.94110976\n",
            "[>] epoch #32, batch # 65: loss: 0.25014663 | val_loss: 0.97935812\n",
            "[>] epoch #32, batch # 67: loss: 0.19673643 | val_loss: 0.94970479\n",
            "[>] epoch #32, batch # 69: loss: 0.21427891 | val_loss: 0.97532795\n",
            "[>] epoch #32, batch # 71: loss: 0.24516971 | val_loss: 0.96054483\n",
            "[>] epoch #32, batch # 73: loss: 0.32260081 | val_loss: 0.98472010\n",
            "[>] epoch #32, batch # 75: loss: 0.26051891 | val_loss: 0.98537079\n",
            "[>] epoch #32, batch # 77: loss: 0.28099036 | val_loss: 0.97735982\n",
            "[>] epoch #32, batch # 79: loss: 0.25427318 | val_loss: 0.97229503\n",
            "[>] epoch #32, batch # 81: loss: 0.32807016 | val_loss: 0.97950316\n",
            "[>] epoch #32, batch # 83: loss: 0.18750720 | val_loss: 1.00646781\n",
            "[>] epoch #32, batch # 85: loss: 0.28997153 | val_loss: 0.94075338\n",
            "[>] epoch #32, batch # 87: loss: 0.31909177 | val_loss: 0.96701507\n",
            "[>] epoch #32, batch # 89: loss: 0.31571603 | val_loss: 0.94778591\n",
            "[>] epoch #32, batch # 91: loss: 0.27774918 | val_loss: 0.96323216\n",
            "[>] epoch #32, batch # 93: loss: 0.22643214 | val_loss: 0.97781845\n",
            "[>] epoch #32, batch # 95: loss: 0.26601291 | val_loss: 0.96159388\n",
            "[>] epoch #32, batch # 97: loss: 0.19234708 | val_loss: 0.96345205\n",
            "[>] epoch #32, batch # 99: loss: 0.19142489 | val_loss: 0.94787243\n",
            "[>] epoch #32, batch #101: loss: 0.30406111 | val_loss: 0.96304729\n",
            "[>] epoch #32, batch #103: loss: 0.28000775 | val_loss: 0.97029805\n",
            "[>] epoch #32, batch #105: loss: 0.27817342 | val_loss: 0.93938737\n",
            "[>] epoch #32, batch #107: loss: 0.19161902 | val_loss: 0.94796713\n",
            "[>] epoch #32, batch #109: loss: 0.27946791 | val_loss: 0.96826449\n",
            "[>] epoch #32, batch #111: loss: 0.24584158 | val_loss: 0.94435517\n",
            "[>] epoch #32, batch #113: loss: 0.20985028 | val_loss: 0.97875205\n",
            "[>] epoch #32, batch #115: loss: 0.29744551 | val_loss: 0.99618770\n",
            "[>] epoch #32, batch #117: loss: 0.27826613 | val_loss: 1.01397339\n",
            "[>] epoch #32, batch #119: loss: 0.20453840 | val_loss: 0.97092404\n",
            "[>] epoch #32, batch #121: loss: 0.22480004 | val_loss: 0.93435436\n",
            "[>] epoch #32, batch #123: loss: 0.20497583 | val_loss: 0.93431546\n",
            "[>] epoch #32, batch #125: loss: 0.22786646 | val_loss: 0.98045661\n",
            "[>] epoch #32, batch #127: loss: 0.42413697 | val_loss: 0.95994625\n",
            "[>] epoch #32, batch #129: loss: 0.27845615 | val_loss: 0.96062115\n",
            "[>] epoch #32, batch #131: loss: 0.28902832 | val_loss: 0.95300978\n",
            "[>] epoch #32, batch #133: loss: 0.25469223 | val_loss: 0.95390979\n",
            "[>] epoch #32, batch #135: loss: 0.25479642 | val_loss: 0.96671572\n",
            "[>] epoch #32, batch #137: loss: 0.28114456 | val_loss: 0.94745787\n",
            "[>] epoch #32, batch #139: loss: 0.30894983 | val_loss: 0.94631982\n",
            "[>] epoch #32, batch #141: loss: 0.10138498 | val_loss: 0.96536308\n",
            "[>] epoch #33, batch #  2: loss: 0.22948557 | val_loss: 0.96913026\n",
            "[>] epoch #33, batch #  4: loss: 0.35014772 | val_loss: 0.97238063\n",
            "[>] epoch #33, batch #  6: loss: 0.21928215 | val_loss: 0.96430663\n",
            "[>] epoch #33, batch #  8: loss: 0.25005209 | val_loss: 0.94769029\n",
            "[>] epoch #33, batch # 10: loss: 0.23869072 | val_loss: 0.98065818\n",
            "[>] epoch #33, batch # 12: loss: 0.18896553 | val_loss: 0.95232310\n",
            "[>] epoch #33, batch # 14: loss: 0.20352209 | val_loss: 0.96467548\n",
            "[>] epoch #33, batch # 16: loss: 0.20473760 | val_loss: 0.96826594\n",
            "[>] epoch #33, batch # 18: loss: 0.24578013 | val_loss: 0.93804449\n",
            "[>] epoch #33, batch # 20: loss: 0.22226846 | val_loss: 0.94985127\n",
            "[>] epoch #33, batch # 22: loss: 0.20611617 | val_loss: 0.93285767\n",
            "[>] epoch #33, batch # 24: loss: 0.23900770 | val_loss: 0.92874095\n",
            "[>] epoch #33, batch # 26: loss: 0.22856753 | val_loss: 0.97591227\n",
            "[>] epoch #33, batch # 28: loss: 0.18068583 | val_loss: 0.99088840\n",
            "[>] epoch #33, batch # 30: loss: 0.22329818 | val_loss: 0.98806506\n",
            "[>] epoch #33, batch # 32: loss: 0.27733371 | val_loss: 0.99741953\n",
            "[>] epoch #33, batch # 34: loss: 0.22204849 | val_loss: 0.92705847\n",
            "[>] epoch #33, batch # 36: loss: 0.18438011 | val_loss: 0.96194502\n",
            "[>] epoch #33, batch # 38: loss: 0.32773650 | val_loss: 0.92217055\n",
            "[>] epoch #33, batch # 40: loss: 0.15970090 | val_loss: 0.95018516\n",
            "[>] epoch #33, batch # 42: loss: 0.25254855 | val_loss: 0.95960329\n",
            "[>] epoch #33, batch # 44: loss: 0.23682889 | val_loss: 0.99206389\n",
            "[>] epoch #33, batch # 46: loss: 0.18857898 | val_loss: 0.96263307\n",
            "[>] epoch #33, batch # 48: loss: 0.27792123 | val_loss: 0.95757178\n",
            "[>] epoch #33, batch # 50: loss: 0.19330828 | val_loss: 0.94883260\n",
            "[>] epoch #33, batch # 52: loss: 0.15535069 | val_loss: 0.96168348\n",
            "[>] epoch #33, batch # 54: loss: 0.28250980 | val_loss: 0.92659144\n",
            "[>] epoch #33, batch # 56: loss: 0.20573001 | val_loss: 0.95615394\n",
            "[>] epoch #33, batch # 58: loss: 0.24541257 | val_loss: 0.96397735\n",
            "[>] epoch #33, batch # 60: loss: 0.16792427 | val_loss: 0.95702746\n",
            "[>] epoch #33, batch # 62: loss: 0.16089191 | val_loss: 0.97087956\n",
            "[>] epoch #33, batch # 64: loss: 0.24070282 | val_loss: 0.92337576\n",
            "[>] epoch #33, batch # 66: loss: 0.26472771 | val_loss: 0.95128019\n",
            "[>] epoch #33, batch # 68: loss: 0.21390545 | val_loss: 0.93104048\n",
            "[>] epoch #33, batch # 70: loss: 0.21791562 | val_loss: 0.96344001\n",
            "[>] epoch #33, batch # 72: loss: 0.27372330 | val_loss: 0.93568129\n",
            "[>] epoch #33, batch # 74: loss: 0.22165865 | val_loss: 0.96101636\n",
            "[>] epoch #33, batch # 76: loss: 0.22339727 | val_loss: 0.98122241\n",
            "[>] epoch #33, batch # 78: loss: 0.26767915 | val_loss: 0.94632360\n",
            "[>] epoch #33, batch # 80: loss: 0.26089728 | val_loss: 0.94308942\n",
            "[>] epoch #33, batch # 82: loss: 0.18735407 | val_loss: 0.93037802\n",
            "[>] epoch #33, batch # 84: loss: 0.24450238 | val_loss: 0.98645755\n",
            "[>] epoch #33, batch # 86: loss: 0.20999938 | val_loss: 0.94016630\n",
            "[>] epoch #33, batch # 88: loss: 0.29182425 | val_loss: 0.97874128\n",
            "[>] epoch #33, batch # 90: loss: 0.22239810 | val_loss: 0.96332736\n",
            "[>] epoch #33, batch # 92: loss: 0.20004356 | val_loss: 0.93024416\n",
            "[>] epoch #33, batch # 94: loss: 0.22344409 | val_loss: 0.96229809\n",
            "[>] epoch #33, batch # 96: loss: 0.26360092 | val_loss: 1.01648557\n",
            "[>] epoch #33, batch # 98: loss: 0.26275858 | val_loss: 0.93587446\n",
            "[>] epoch #33, batch #100: loss: 0.21466748 | val_loss: 0.94671401\n",
            "[>] epoch #33, batch #102: loss: 0.34680450 | val_loss: 0.94422668\n",
            "[>] epoch #33, batch #104: loss: 0.26902655 | val_loss: 0.96011596\n",
            "[>] epoch #33, batch #106: loss: 0.24977340 | val_loss: 0.93872300\n",
            "[>] epoch #33, batch #108: loss: 0.26240081 | val_loss: 0.97243634\n",
            "[>] epoch #33, batch #110: loss: 0.23497082 | val_loss: 0.98141545\n",
            "[>] epoch #33, batch #112: loss: 0.30458856 | val_loss: 0.97666591\n",
            "[>] epoch #33, batch #114: loss: 0.26850769 | val_loss: 1.00946795\n",
            "[>] epoch #33, batch #116: loss: 0.18419382 | val_loss: 0.95328635\n",
            "[>] epoch #33, batch #118: loss: 0.28944263 | val_loss: 1.00366291\n",
            "[>] epoch #33, batch #120: loss: 0.15901697 | val_loss: 0.95626303\n",
            "[>] epoch #33, batch #122: loss: 0.28352782 | val_loss: 0.95608084\n",
            "[>] epoch #33, batch #124: loss: 0.27912062 | val_loss: 0.97516513\n",
            "[>] epoch #33, batch #126: loss: 0.22496949 | val_loss: 0.96221965\n",
            "[>] epoch #33, batch #128: loss: 0.24361373 | val_loss: 0.98469150\n",
            "[>] epoch #33, batch #130: loss: 0.26644012 | val_loss: 0.94395157\n",
            "[>] epoch #33, batch #132: loss: 0.25016585 | val_loss: 0.96179467\n",
            "[>] epoch #33, batch #134: loss: 0.23432633 | val_loss: 0.95773660\n",
            "[>] epoch #33, batch #136: loss: 0.29375368 | val_loss: 0.96012463\n",
            "[>] epoch #33, batch #138: loss: 0.15242831 | val_loss: 0.96193241\n",
            "[>] epoch #33, batch #140: loss: 0.19691826 | val_loss: 0.93213782\n",
            "[>] epoch #34, batch #  1: loss: 0.19566131 | val_loss: 0.93074884\n",
            "[>] epoch #34, batch #  3: loss: 0.20847708 | val_loss: 0.96565406\n",
            "[>] epoch #34, batch #  5: loss: 0.22337365 | val_loss: 0.97480435\n",
            "[>] epoch #34, batch #  7: loss: 0.21787646 | val_loss: 0.97454376\n",
            "[>] epoch #34, batch #  9: loss: 0.25609356 | val_loss: 0.98805227\n",
            "[>] epoch #34, batch # 11: loss: 0.17462349 | val_loss: 0.95717169\n",
            "[>] epoch #34, batch # 13: loss: 0.22793315 | val_loss: 0.97351450\n",
            "[>] epoch #34, batch # 15: loss: 0.16522190 | val_loss: 0.93506150\n",
            "[>] epoch #34, batch # 17: loss: 0.18480083 | val_loss: 0.93675262\n",
            "[>] epoch #34, batch # 19: loss: 0.20155987 | val_loss: 0.96329895\n",
            "[>] epoch #34, batch # 21: loss: 0.22680423 | val_loss: 0.96572747\n",
            "[>] epoch #34, batch # 23: loss: 0.33191773 | val_loss: 0.98146589\n",
            "[>] epoch #34, batch # 25: loss: 0.19635653 | val_loss: 0.96202376\n",
            "[>] epoch #34, batch # 27: loss: 0.28691170 | val_loss: 0.96891914\n",
            "[>] epoch #34, batch # 29: loss: 0.16399205 | val_loss: 0.94665331\n",
            "[>] epoch #34, batch # 31: loss: 0.25658914 | val_loss: 0.90672928\n",
            "[>] epoch #34, batch # 33: loss: 0.15881477 | val_loss: 0.95040892\n",
            "[>] epoch #34, batch # 35: loss: 0.19719820 | val_loss: 0.97927553\n",
            "[>] epoch #34, batch # 37: loss: 0.20376359 | val_loss: 0.96764521\n",
            "[>] epoch #34, batch # 39: loss: 0.26580307 | val_loss: 0.95245714\n",
            "[>] epoch #34, batch # 41: loss: 0.19928634 | val_loss: 0.95899976\n",
            "[>] epoch #34, batch # 43: loss: 0.25358745 | val_loss: 0.94953118\n",
            "[>] epoch #34, batch # 45: loss: 0.28027511 | val_loss: 0.94317624\n",
            "[>] epoch #34, batch # 47: loss: 0.22967526 | val_loss: 0.93312165\n",
            "[>] epoch #34, batch # 49: loss: 0.24396449 | val_loss: 0.96687448\n",
            "[>] epoch #34, batch # 51: loss: 0.26618230 | val_loss: 0.97369375\n",
            "[>] epoch #34, batch # 53: loss: 0.23129971 | val_loss: 0.95903590\n",
            "[>] epoch #34, batch # 55: loss: 0.20681241 | val_loss: 0.97406911\n",
            "[>] epoch #34, batch # 57: loss: 0.22379871 | val_loss: 0.97388980\n",
            "[>] epoch #34, batch # 59: loss: 0.15806103 | val_loss: 0.95132516\n",
            "[>] epoch #34, batch # 61: loss: 0.22027598 | val_loss: 0.96285568\n",
            "[>] epoch #34, batch # 63: loss: 0.20679615 | val_loss: 0.95137117\n",
            "[>] epoch #34, batch # 65: loss: 0.20519605 | val_loss: 0.94401530\n",
            "[>] epoch #34, batch # 67: loss: 0.23920040 | val_loss: 0.93639868\n",
            "[>] epoch #34, batch # 69: loss: 0.24906220 | val_loss: 0.94629274\n",
            "[>] epoch #34, batch # 71: loss: 0.29528725 | val_loss: 0.96212305\n",
            "[>] epoch #34, batch # 73: loss: 0.22681612 | val_loss: 0.97148725\n",
            "[>] epoch #34, batch # 75: loss: 0.26816207 | val_loss: 0.95869742\n",
            "[>] epoch #34, batch # 77: loss: 0.28036383 | val_loss: 0.92877918\n",
            "[>] epoch #34, batch # 79: loss: 0.27704880 | val_loss: 0.93146830\n",
            "[>] epoch #34, batch # 81: loss: 0.19833006 | val_loss: 0.94331382\n",
            "[>] epoch #34, batch # 83: loss: 0.18219724 | val_loss: 0.92566553\n",
            "[>] epoch #34, batch # 85: loss: 0.27810925 | val_loss: 0.96269569\n",
            "[>] epoch #34, batch # 87: loss: 0.23033145 | val_loss: 0.95639268\n",
            "[>] epoch #34, batch # 89: loss: 0.28041136 | val_loss: 0.95114796\n",
            "[>] epoch #34, batch # 91: loss: 0.29575902 | val_loss: 0.95966211\n",
            "[>] epoch #34, batch # 93: loss: 0.21515542 | val_loss: 0.94270682\n",
            "[>] epoch #34, batch # 95: loss: 0.21276656 | val_loss: 0.93970832\n",
            "[>] epoch #34, batch # 97: loss: 0.23684196 | val_loss: 0.95747175\n",
            "[>] epoch #34, batch # 99: loss: 0.22114281 | val_loss: 0.97632352\n",
            "[>] epoch #34, batch #101: loss: 0.30034459 | val_loss: 0.92350879\n",
            "[>] epoch #34, batch #103: loss: 0.20175891 | val_loss: 0.92339584\n",
            "[>] epoch #34, batch #105: loss: 0.19285110 | val_loss: 0.93680984\n",
            "[>] epoch #34, batch #107: loss: 0.26606750 | val_loss: 0.97472149\n",
            "[>] epoch #34, batch #109: loss: 0.22650550 | val_loss: 0.92994845\n",
            "[>] epoch #34, batch #111: loss: 0.21819852 | val_loss: 0.94747404\n",
            "[>] epoch #34, batch #113: loss: 0.19349167 | val_loss: 0.94547374\n",
            "[>] epoch #34, batch #115: loss: 0.26558039 | val_loss: 0.96215822\n",
            "[>] epoch #34, batch #117: loss: 0.25410822 | val_loss: 0.96735599\n",
            "[>] epoch #34, batch #119: loss: 0.20795478 | val_loss: 0.95737504\n",
            "[>] epoch #34, batch #121: loss: 0.26408309 | val_loss: 0.98462492\n",
            "[>] epoch #34, batch #123: loss: 0.28084984 | val_loss: 0.97190563\n",
            "[>] epoch #34, batch #125: loss: 0.24167371 | val_loss: 0.96392219\n",
            "[>] epoch #34, batch #127: loss: 0.28884956 | val_loss: 0.97958039\n",
            "[>] epoch #34, batch #129: loss: 0.18690494 | val_loss: 0.95822442\n",
            "[>] epoch #34, batch #131: loss: 0.19603401 | val_loss: 0.93558886\n",
            "[>] epoch #34, batch #133: loss: 0.21953923 | val_loss: 0.98042158\n",
            "[>] epoch #34, batch #135: loss: 0.19608450 | val_loss: 0.93514839\n",
            "[>] epoch #34, batch #137: loss: 0.19276854 | val_loss: 0.93968064\n",
            "[>] epoch #34, batch #139: loss: 0.25388783 | val_loss: 0.93881681\n",
            "[>] epoch #34, batch #141: loss: 0.20082460 | val_loss: 0.96123147\n",
            "[>] epoch #35, batch #  2: loss: 0.27882901 | val_loss: 0.94772710\n",
            "[>] epoch #35, batch #  4: loss: 0.18367825 | val_loss: 0.94946132\n",
            "[>] epoch #35, batch #  6: loss: 0.19361058 | val_loss: 0.96877281\n",
            "[>] epoch #35, batch #  8: loss: 0.21556436 | val_loss: 0.97140920\n",
            "[>] epoch #35, batch # 10: loss: 0.23062907 | val_loss: 0.96125455\n",
            "[>] epoch #35, batch # 12: loss: 0.17121780 | val_loss: 0.97396033\n",
            "[>] epoch #35, batch # 14: loss: 0.17474458 | val_loss: 0.97952381\n",
            "[>] epoch #35, batch # 16: loss: 0.21607102 | val_loss: 0.94379983\n",
            "[>] epoch #35, batch # 18: loss: 0.21970728 | val_loss: 0.97054980\n",
            "[>] epoch #35, batch # 20: loss: 0.21494262 | val_loss: 0.95515443\n",
            "[>] epoch #35, batch # 22: loss: 0.16140242 | val_loss: 0.94956600\n",
            "[>] epoch #35, batch # 24: loss: 0.20560646 | val_loss: 0.94507604\n",
            "[>] epoch #35, batch # 26: loss: 0.25394154 | val_loss: 0.96914704\n",
            "[>] epoch #35, batch # 28: loss: 0.24555318 | val_loss: 0.96839568\n",
            "[>] epoch #35, batch # 30: loss: 0.20195624 | val_loss: 0.99678369\n",
            "[>] epoch #35, batch # 32: loss: 0.23440130 | val_loss: 0.95432061\n",
            "[>] epoch #35, batch # 34: loss: 0.19124754 | val_loss: 0.94827960\n",
            "[>] epoch #35, batch # 36: loss: 0.14787139 | val_loss: 0.98585631\n",
            "[>] epoch #35, batch # 38: loss: 0.20205164 | val_loss: 0.96866945\n",
            "[>] epoch #35, batch # 40: loss: 0.22013663 | val_loss: 0.94670205\n",
            "[>] epoch #35, batch # 42: loss: 0.20967659 | val_loss: 0.95860590\n",
            "[>] epoch #35, batch # 44: loss: 0.22024396 | val_loss: 0.96485900\n",
            "[>] epoch #35, batch # 46: loss: 0.20452644 | val_loss: 0.96012298\n",
            "[>] epoch #35, batch # 48: loss: 0.18437262 | val_loss: 0.93170974\n",
            "[>] epoch #35, batch # 50: loss: 0.17071326 | val_loss: 0.94270915\n",
            "[>] epoch #35, batch # 52: loss: 0.24687275 | val_loss: 0.94758159\n",
            "[>] epoch #35, batch # 54: loss: 0.20199040 | val_loss: 0.96352615\n",
            "[>] epoch #35, batch # 56: loss: 0.35080016 | val_loss: 0.95042390\n",
            "[>] epoch #35, batch # 58: loss: 0.19730255 | val_loss: 0.95606009\n",
            "[>] epoch #35, batch # 60: loss: 0.21772678 | val_loss: 0.94038808\n",
            "[>] epoch #35, batch # 62: loss: 0.26588392 | val_loss: 0.94824955\n",
            "[>] epoch #35, batch # 64: loss: 0.19254017 | val_loss: 0.96918296\n",
            "[>] epoch #35, batch # 66: loss: 0.15806845 | val_loss: 0.99891888\n",
            "[>] epoch #35, batch # 68: loss: 0.22273803 | val_loss: 0.93907090\n",
            "[>] epoch #35, batch # 70: loss: 0.15544036 | val_loss: 0.97592880\n",
            "[>] epoch #35, batch # 72: loss: 0.25657475 | val_loss: 0.98483806\n",
            "[>] epoch #35, batch # 74: loss: 0.27579683 | val_loss: 0.98990169\n",
            "[>] epoch #35, batch # 76: loss: 0.21646789 | val_loss: 1.02782766\n",
            "[>] epoch #35, batch # 78: loss: 0.25572872 | val_loss: 0.98700774\n",
            "[>] epoch #35, batch # 80: loss: 0.20083047 | val_loss: 0.96955348\n",
            "[>] epoch #35, batch # 82: loss: 0.23303771 | val_loss: 0.98129000\n",
            "[>] epoch #35, batch # 84: loss: 0.22712395 | val_loss: 0.98634906\n",
            "[>] epoch #35, batch # 86: loss: 0.24736714 | val_loss: 0.94617050\n",
            "[>] epoch #35, batch # 88: loss: 0.31416732 | val_loss: 0.96462612\n",
            "[>] epoch #35, batch # 90: loss: 0.27550068 | val_loss: 0.97353828\n",
            "[>] epoch #35, batch # 92: loss: 0.21680580 | val_loss: 0.97495351\n",
            "[>] epoch #35, batch # 94: loss: 0.18665493 | val_loss: 0.95692220\n",
            "[>] epoch #35, batch # 96: loss: 0.21733320 | val_loss: 0.99823927\n",
            "[>] epoch #35, batch # 98: loss: 0.27744770 | val_loss: 0.98747556\n",
            "[>] epoch #35, batch #100: loss: 0.20194826 | val_loss: 0.97233739\n",
            "[>] epoch #35, batch #102: loss: 0.20344418 | val_loss: 0.97632496\n",
            "[>] epoch #35, batch #104: loss: 0.18924251 | val_loss: 0.98949917\n",
            "[>] epoch #35, batch #106: loss: 0.25900292 | val_loss: 0.97798395\n",
            "[>] epoch #35, batch #108: loss: 0.26147586 | val_loss: 0.98688330\n",
            "[>] epoch #35, batch #110: loss: 0.13490330 | val_loss: 0.92956419\n",
            "[>] epoch #35, batch #112: loss: 0.20087931 | val_loss: 0.93370577\n",
            "[>] epoch #35, batch #114: loss: 0.24161309 | val_loss: 0.94641191\n",
            "[>] epoch #35, batch #116: loss: 0.25731361 | val_loss: 0.93748481\n",
            "[>] epoch #35, batch #118: loss: 0.22538832 | val_loss: 0.93098331\n",
            "[>] epoch #35, batch #120: loss: 0.17365691 | val_loss: 0.96698151\n",
            "[>] epoch #35, batch #122: loss: 0.33910224 | val_loss: 0.97362963\n",
            "[>] epoch #35, batch #124: loss: 0.20747292 | val_loss: 0.96633537\n",
            "[>] epoch #35, batch #126: loss: 0.26978114 | val_loss: 0.96675627\n",
            "[>] epoch #35, batch #128: loss: 0.22492135 | val_loss: 0.97423223\n",
            "[>] epoch #35, batch #130: loss: 0.27677858 | val_loss: 0.98032910\n",
            "[>] epoch #35, batch #132: loss: 0.21508478 | val_loss: 0.94342846\n",
            "[>] epoch #35, batch #134: loss: 0.27298275 | val_loss: 0.95532006\n",
            "[>] epoch #35, batch #136: loss: 0.21569531 | val_loss: 0.94843878\n",
            "[>] epoch #35, batch #138: loss: 0.28146908 | val_loss: 0.96212134\n",
            "[>] epoch #35, batch #140: loss: 0.27658832 | val_loss: 0.98006456\n",
            "[>] epoch #36, batch #  1: loss: 0.17656808 | val_loss: 0.99720705\n",
            "[>] epoch #36, batch #  3: loss: 0.16829611 | val_loss: 0.97937718\n",
            "[>] epoch #36, batch #  5: loss: 0.15224029 | val_loss: 0.95761599\n",
            "[>] epoch #36, batch #  7: loss: 0.16500780 | val_loss: 0.97187118\n",
            "[>] epoch #36, batch #  9: loss: 0.20554109 | val_loss: 0.98045819\n",
            "[>] epoch #36, batch # 11: loss: 0.20480353 | val_loss: 0.95942046\n",
            "[>] epoch #36, batch # 13: loss: 0.22317000 | val_loss: 0.95574848\n",
            "[>] epoch #36, batch # 15: loss: 0.12329088 | val_loss: 0.97524185\n",
            "[>] epoch #36, batch # 17: loss: 0.17953005 | val_loss: 0.96640957\n",
            "[>] epoch #36, batch # 19: loss: 0.25253719 | val_loss: 0.99496921\n",
            "[>] epoch #36, batch # 21: loss: 0.26411015 | val_loss: 0.98043814\n",
            "[>] epoch #36, batch # 23: loss: 0.19503239 | val_loss: 0.95227300\n",
            "[>] epoch #36, batch # 25: loss: 0.18373452 | val_loss: 0.97076442\n",
            "[>] epoch #36, batch # 27: loss: 0.13983192 | val_loss: 0.93905958\n",
            "[>] epoch #36, batch # 29: loss: 0.25960872 | val_loss: 0.94160766\n",
            "[>] epoch #36, batch # 31: loss: 0.20275173 | val_loss: 0.96689702\n",
            "[>] epoch #36, batch # 33: loss: 0.19765909 | val_loss: 0.97939328\n",
            "[>] epoch #36, batch # 35: loss: 0.19666266 | val_loss: 0.94255999\n",
            "[>] epoch #36, batch # 37: loss: 0.26275837 | val_loss: 0.98181017\n",
            "[>] epoch #36, batch # 39: loss: 0.20419496 | val_loss: 0.98163986\n",
            "[>] epoch #36, batch # 41: loss: 0.18231797 | val_loss: 0.98718535\n",
            "[>] epoch #36, batch # 43: loss: 0.13684265 | val_loss: 0.99048295\n",
            "[>] epoch #36, batch # 45: loss: 0.18914974 | val_loss: 0.99723483\n",
            "[>] epoch #36, batch # 47: loss: 0.18900363 | val_loss: 0.95841040\n",
            "[>] epoch #36, batch # 49: loss: 0.22517186 | val_loss: 0.96814896\n",
            "[>] epoch #36, batch # 51: loss: 0.22466618 | val_loss: 0.99701187\n",
            "[>] epoch #36, batch # 53: loss: 0.20430124 | val_loss: 0.99165751\n",
            "[>] epoch #36, batch # 55: loss: 0.18979858 | val_loss: 0.96937275\n",
            "[>] epoch #36, batch # 57: loss: 0.16785176 | val_loss: 0.99075464\n",
            "[>] epoch #36, batch # 59: loss: 0.16543697 | val_loss: 0.96056592\n",
            "[>] epoch #36, batch # 61: loss: 0.21672122 | val_loss: 0.96159510\n",
            "[>] epoch #36, batch # 63: loss: 0.20238608 | val_loss: 0.96706232\n",
            "[>] epoch #36, batch # 65: loss: 0.16962638 | val_loss: 0.97396431\n",
            "[>] epoch #36, batch # 67: loss: 0.27484563 | val_loss: 0.99079402\n",
            "[>] epoch #36, batch # 69: loss: 0.25291815 | val_loss: 0.96353263\n",
            "[>] epoch #36, batch # 71: loss: 0.28237286 | val_loss: 1.00086893\n",
            "[>] epoch #36, batch # 73: loss: 0.16970283 | val_loss: 0.99311250\n",
            "[>] epoch #36, batch # 75: loss: 0.26654994 | val_loss: 0.98706025\n",
            "[>] epoch #36, batch # 77: loss: 0.21843109 | val_loss: 0.97252764\n",
            "[>] epoch #36, batch # 79: loss: 0.21199286 | val_loss: 0.96795036\n",
            "[>] epoch #36, batch # 81: loss: 0.18400127 | val_loss: 0.99222622\n",
            "[>] epoch #36, batch # 83: loss: 0.22608694 | val_loss: 0.96038629\n",
            "[>] epoch #36, batch # 85: loss: 0.26068628 | val_loss: 0.98073679\n",
            "[>] epoch #36, batch # 87: loss: 0.24704559 | val_loss: 0.97299461\n",
            "[>] epoch #36, batch # 89: loss: 0.18775161 | val_loss: 0.99586719\n",
            "[>] epoch #36, batch # 91: loss: 0.19562760 | val_loss: 0.97611762\n",
            "[>] epoch #36, batch # 93: loss: 0.17512904 | val_loss: 0.98152058\n",
            "[>] epoch #36, batch # 95: loss: 0.17518064 | val_loss: 0.95173138\n",
            "[>] epoch #36, batch # 97: loss: 0.18959500 | val_loss: 0.95252225\n",
            "[>] epoch #36, batch # 99: loss: 0.24562614 | val_loss: 0.97216259\n",
            "[>] epoch #36, batch #101: loss: 0.17653665 | val_loss: 0.97320939\n",
            "[>] epoch #36, batch #103: loss: 0.24292524 | val_loss: 0.98838797\n",
            "[>] epoch #36, batch #105: loss: 0.19752815 | val_loss: 1.00418911\n",
            "[>] epoch #36, batch #107: loss: 0.25549361 | val_loss: 1.02019995\n",
            "[>] epoch #36, batch #109: loss: 0.14062735 | val_loss: 0.99581872\n",
            "[>] epoch #36, batch #111: loss: 0.15669306 | val_loss: 1.01617588\n",
            "[>] epoch #36, batch #113: loss: 0.27156815 | val_loss: 0.95315144\n",
            "[>] epoch #36, batch #115: loss: 0.24028522 | val_loss: 0.97535726\n",
            "[>] epoch #36, batch #117: loss: 0.24854456 | val_loss: 0.96543650\n",
            "[>] epoch #36, batch #119: loss: 0.27614921 | val_loss: 0.99969674\n",
            "[>] epoch #36, batch #121: loss: 0.20554242 | val_loss: 0.97738012\n",
            "[>] epoch #36, batch #123: loss: 0.20455486 | val_loss: 0.95896262\n",
            "[>] epoch #36, batch #125: loss: 0.28846565 | val_loss: 0.98252733\n",
            "[>] epoch #36, batch #127: loss: 0.36596990 | val_loss: 0.99100134\n",
            "[>] epoch #36, batch #129: loss: 0.20307690 | val_loss: 0.98292821\n",
            "[>] epoch #36, batch #131: loss: 0.22363195 | val_loss: 0.97933709\n",
            "[>] epoch #36, batch #133: loss: 0.23275585 | val_loss: 0.98088778\n",
            "[>] epoch #36, batch #135: loss: 0.16816843 | val_loss: 0.99097052\n",
            "[>] epoch #36, batch #137: loss: 0.24922618 | val_loss: 1.00648227\n",
            "[>] epoch #36, batch #139: loss: 0.29351294 | val_loss: 0.95030380\n",
            "[>] epoch #36, batch #141: loss: 0.18662156 | val_loss: 0.98581663\n",
            "[>] epoch #37, batch #  2: loss: 0.19132648 | val_loss: 0.98943515\n",
            "[>] epoch #37, batch #  4: loss: 0.18674347 | val_loss: 0.97954669\n",
            "[>] epoch #37, batch #  6: loss: 0.23446476 | val_loss: 0.95737364\n",
            "[>] epoch #37, batch #  8: loss: 0.18327099 | val_loss: 0.93787357\n",
            "[>] epoch #37, batch # 10: loss: 0.23144299 | val_loss: 0.96863351\n",
            "[>] epoch #37, batch # 12: loss: 0.13825102 | val_loss: 1.00772542\n",
            "[>] epoch #37, batch # 14: loss: 0.21907336 | val_loss: 0.98947702\n",
            "[>] epoch #37, batch # 16: loss: 0.22132362 | val_loss: 0.95853214\n",
            "[>] epoch #37, batch # 18: loss: 0.17987086 | val_loss: 1.00679415\n",
            "[>] epoch #37, batch # 20: loss: 0.14173682 | val_loss: 0.97576918\n",
            "[>] epoch #37, batch # 22: loss: 0.19535336 | val_loss: 0.97798095\n",
            "[>] epoch #37, batch # 24: loss: 0.18628490 | val_loss: 0.97243174\n",
            "[>] epoch #37, batch # 26: loss: 0.14041387 | val_loss: 0.98050696\n",
            "[>] epoch #37, batch # 28: loss: 0.23796917 | val_loss: 0.97992913\n",
            "[>] epoch #37, batch # 30: loss: 0.24161476 | val_loss: 1.01276361\n",
            "[>] epoch #37, batch # 32: loss: 0.17591122 | val_loss: 0.97387331\n",
            "[>] epoch #37, batch # 34: loss: 0.25374603 | val_loss: 0.99484896\n",
            "[>] epoch #37, batch # 36: loss: 0.18094936 | val_loss: 0.99755249\n",
            "[>] epoch #37, batch # 38: loss: 0.22260170 | val_loss: 0.96068683\n",
            "[>] epoch #37, batch # 40: loss: 0.21200623 | val_loss: 0.98051817\n",
            "[>] epoch #37, batch # 42: loss: 0.20607395 | val_loss: 0.93676486\n",
            "[>] epoch #37, batch # 44: loss: 0.23420033 | val_loss: 0.98060621\n",
            "[>] epoch #37, batch # 46: loss: 0.20577686 | val_loss: 0.98088538\n",
            "[>] epoch #37, batch # 48: loss: 0.15967377 | val_loss: 0.98398131\n",
            "[>] epoch #37, batch # 50: loss: 0.16621476 | val_loss: 0.97444975\n",
            "[>] epoch #37, batch # 52: loss: 0.15390757 | val_loss: 0.94664828\n",
            "[>] epoch #37, batch # 54: loss: 0.19235556 | val_loss: 0.98238629\n",
            "[>] epoch #37, batch # 56: loss: 0.20074353 | val_loss: 0.99115090\n",
            "[>] epoch #37, batch # 58: loss: 0.16044617 | val_loss: 0.99194948\n",
            "[>] epoch #37, batch # 60: loss: 0.27592134 | val_loss: 0.96774537\n",
            "[>] epoch #37, batch # 62: loss: 0.20086171 | val_loss: 0.92964414\n",
            "[>] epoch #37, batch # 64: loss: 0.18459956 | val_loss: 0.97979461\n",
            "[>] epoch #37, batch # 66: loss: 0.15088730 | val_loss: 0.94722983\n",
            "[>] epoch #37, batch # 68: loss: 0.17757837 | val_loss: 0.99592277\n",
            "[>] epoch #37, batch # 70: loss: 0.20789665 | val_loss: 0.94998082\n",
            "[>] epoch #37, batch # 72: loss: 0.17468897 | val_loss: 0.96026869\n",
            "[>] epoch #37, batch # 74: loss: 0.22138977 | val_loss: 0.96091256\n",
            "[>] epoch #37, batch # 76: loss: 0.22972459 | val_loss: 0.95283118\n",
            "[>] epoch #37, batch # 78: loss: 0.13279864 | val_loss: 0.95313870\n",
            "[>] epoch #37, batch # 80: loss: 0.17915668 | val_loss: 0.98065886\n",
            "[>] epoch #37, batch # 82: loss: 0.20182684 | val_loss: 0.95479417\n",
            "[>] epoch #37, batch # 84: loss: 0.19703919 | val_loss: 0.95346626\n",
            "[>] epoch #37, batch # 86: loss: 0.20712660 | val_loss: 0.94160834\n",
            "[>] epoch #37, batch # 88: loss: 0.18586048 | val_loss: 1.00114506\n",
            "[>] epoch #37, batch # 90: loss: 0.21180741 | val_loss: 0.96638608\n",
            "[>] epoch #37, batch # 92: loss: 0.16795239 | val_loss: 0.95175482\n",
            "[>] epoch #37, batch # 94: loss: 0.17863555 | val_loss: 1.01361796\n",
            "[>] epoch #37, batch # 96: loss: 0.21084440 | val_loss: 0.93846702\n",
            "[>] epoch #37, batch # 98: loss: 0.17197974 | val_loss: 0.95790324\n",
            "[>] epoch #37, batch #100: loss: 0.19805951 | val_loss: 0.95920388\n",
            "[>] epoch #37, batch #102: loss: 0.16247135 | val_loss: 0.96626893\n",
            "[>] epoch #37, batch #104: loss: 0.20693439 | val_loss: 0.95447062\n",
            "[>] epoch #37, batch #106: loss: 0.19034924 | val_loss: 0.97130261\n",
            "[>] epoch #37, batch #108: loss: 0.26312703 | val_loss: 0.95984555\n",
            "[>] epoch #37, batch #110: loss: 0.17607950 | val_loss: 0.98684501\n",
            "[>] epoch #37, batch #112: loss: 0.30705559 | val_loss: 0.99031397\n",
            "[>] epoch #37, batch #114: loss: 0.21776941 | val_loss: 0.99037094\n",
            "[>] epoch #37, batch #116: loss: 0.16331236 | val_loss: 0.95723768\n",
            "[>] epoch #37, batch #118: loss: 0.19545220 | val_loss: 0.95610059\n",
            "[>] epoch #37, batch #120: loss: 0.17912038 | val_loss: 0.97405529\n",
            "[>] epoch #37, batch #122: loss: 0.20293404 | val_loss: 0.94907604\n",
            "[>] epoch #37, batch #124: loss: 0.18825907 | val_loss: 0.95471349\n",
            "[>] epoch #37, batch #126: loss: 0.23618287 | val_loss: 0.95037563\n",
            "[>] epoch #37, batch #128: loss: 0.19979857 | val_loss: 0.99489293\n",
            "[>] epoch #37, batch #130: loss: 0.24775051 | val_loss: 0.96512808\n",
            "[>] epoch #37, batch #132: loss: 0.16009331 | val_loss: 0.98437994\n",
            "[>] epoch #37, batch #134: loss: 0.20167269 | val_loss: 1.00873420\n",
            "[>] epoch #37, batch #136: loss: 0.17575416 | val_loss: 0.98531502\n",
            "[>] epoch #37, batch #138: loss: 0.25535437 | val_loss: 0.95636664\n",
            "[>] epoch #37, batch #140: loss: 0.16252233 | val_loss: 0.97907449\n",
            "[>] epoch #38, batch #  1: loss: 0.11694868 | val_loss: 1.01026200\n",
            "[>] epoch #38, batch #  3: loss: 0.21159138 | val_loss: 1.01679002\n",
            "[>] epoch #38, batch #  5: loss: 0.19715124 | val_loss: 1.03312740\n",
            "[>] epoch #38, batch #  7: loss: 0.15131825 | val_loss: 0.98727335\n",
            "[>] epoch #38, batch #  9: loss: 0.12322198 | val_loss: 1.00832252\n",
            "[>] epoch #38, batch # 11: loss: 0.19172060 | val_loss: 0.99548371\n",
            "[>] epoch #38, batch # 13: loss: 0.21711250 | val_loss: 0.97804016\n",
            "[>] epoch #38, batch # 15: loss: 0.20244066 | val_loss: 0.99077613\n",
            "[>] epoch #38, batch # 17: loss: 0.21367614 | val_loss: 0.99861941\n",
            "[>] epoch #38, batch # 19: loss: 0.15702465 | val_loss: 0.97722160\n",
            "[>] epoch #38, batch # 21: loss: 0.14216311 | val_loss: 0.98401748\n",
            "[>] epoch #38, batch # 23: loss: 0.19263025 | val_loss: 0.98486026\n",
            "[>] epoch #38, batch # 25: loss: 0.20747764 | val_loss: 0.99040714\n",
            "[>] epoch #38, batch # 27: loss: 0.15267152 | val_loss: 1.00300532\n",
            "[>] epoch #38, batch # 29: loss: 0.16866718 | val_loss: 0.97533056\n",
            "[>] epoch #38, batch # 31: loss: 0.21175931 | val_loss: 0.99104804\n",
            "[>] epoch #38, batch # 33: loss: 0.30645940 | val_loss: 0.99575366\n",
            "[>] epoch #38, batch # 35: loss: 0.17791797 | val_loss: 1.01310857\n",
            "[>] epoch #38, batch # 37: loss: 0.21139090 | val_loss: 0.97557353\n",
            "[>] epoch #38, batch # 39: loss: 0.12275835 | val_loss: 0.99728249\n",
            "[>] epoch #38, batch # 41: loss: 0.12704208 | val_loss: 0.99845060\n",
            "[>] epoch #38, batch # 43: loss: 0.13604966 | val_loss: 1.01195474\n",
            "[>] epoch #38, batch # 45: loss: 0.15385133 | val_loss: 0.96513823\n",
            "[>] epoch #38, batch # 47: loss: 0.19745637 | val_loss: 0.99191869\n",
            "[>] epoch #38, batch # 49: loss: 0.16754702 | val_loss: 0.95525438\n",
            "[>] epoch #38, batch # 51: loss: 0.17607974 | val_loss: 0.99888545\n",
            "[>] epoch #38, batch # 53: loss: 0.22536708 | val_loss: 1.02348216\n",
            "[>] epoch #38, batch # 55: loss: 0.19240595 | val_loss: 0.99503705\n",
            "[>] epoch #38, batch # 57: loss: 0.16138075 | val_loss: 0.97405339\n",
            "[>] epoch #38, batch # 59: loss: 0.23500757 | val_loss: 0.98847352\n",
            "[>] epoch #38, batch # 61: loss: 0.25962690 | val_loss: 0.99260265\n",
            "[>] epoch #38, batch # 63: loss: 0.14776622 | val_loss: 0.98604282\n",
            "[>] epoch #38, batch # 65: loss: 0.18352152 | val_loss: 0.97475808\n",
            "[>] epoch #38, batch # 67: loss: 0.19540317 | val_loss: 0.96255449\n",
            "[>] epoch #38, batch # 69: loss: 0.22665112 | val_loss: 0.98119379\n",
            "[>] epoch #38, batch # 71: loss: 0.23161927 | val_loss: 0.98263399\n",
            "[>] epoch #38, batch # 73: loss: 0.18241110 | val_loss: 0.95650009\n",
            "[>] epoch #38, batch # 75: loss: 0.18640727 | val_loss: 0.97310894\n",
            "[>] epoch #38, batch # 77: loss: 0.16523598 | val_loss: 1.00062344\n",
            "[>] epoch #38, batch # 79: loss: 0.18441765 | val_loss: 0.97713399\n",
            "[>] epoch #38, batch # 81: loss: 0.19381842 | val_loss: 0.97340638\n",
            "[>] epoch #38, batch # 83: loss: 0.19036989 | val_loss: 0.96867590\n",
            "[>] epoch #38, batch # 85: loss: 0.16852553 | val_loss: 1.01624555\n",
            "[>] epoch #38, batch # 87: loss: 0.21625717 | val_loss: 0.99303972\n",
            "[>] epoch #38, batch # 89: loss: 0.25365847 | val_loss: 0.96789416\n",
            "[>] epoch #38, batch # 91: loss: 0.14030130 | val_loss: 0.98520613\n",
            "[>] epoch #38, batch # 93: loss: 0.20700152 | val_loss: 0.98768779\n",
            "[>] epoch #38, batch # 95: loss: 0.20131786 | val_loss: 0.97843507\n",
            "[>] epoch #38, batch # 97: loss: 0.20776668 | val_loss: 0.96173167\n",
            "[>] epoch #38, batch # 99: loss: 0.22537002 | val_loss: 0.97431100\n",
            "[>] epoch #38, batch #101: loss: 0.17656013 | val_loss: 0.96976538\n",
            "[>] epoch #38, batch #103: loss: 0.21325761 | val_loss: 0.98020800\n",
            "[>] epoch #38, batch #105: loss: 0.24581747 | val_loss: 0.99107296\n",
            "[>] epoch #38, batch #107: loss: 0.17628637 | val_loss: 0.98146118\n",
            "[>] epoch #38, batch #109: loss: 0.23917063 | val_loss: 0.96519351\n",
            "[>] epoch #38, batch #111: loss: 0.15301885 | val_loss: 1.01104448\n",
            "[>] epoch #38, batch #113: loss: 0.20139116 | val_loss: 0.99671791\n",
            "[>] epoch #38, batch #115: loss: 0.15056743 | val_loss: 1.00507592\n",
            "[>] epoch #38, batch #117: loss: 0.16882631 | val_loss: 1.00990847\n",
            "[>] epoch #38, batch #119: loss: 0.26081356 | val_loss: 0.97019361\n",
            "[>] epoch #38, batch #121: loss: 0.18855695 | val_loss: 0.97322085\n",
            "[>] epoch #38, batch #123: loss: 0.24846277 | val_loss: 0.95995395\n",
            "[>] epoch #38, batch #125: loss: 0.16410533 | val_loss: 0.96750960\n",
            "[>] epoch #38, batch #127: loss: 0.20395850 | val_loss: 0.94140018\n",
            "[>] epoch #38, batch #129: loss: 0.16760635 | val_loss: 0.95139933\n",
            "[>] epoch #38, batch #131: loss: 0.17125583 | val_loss: 0.97922882\n",
            "[>] epoch #38, batch #133: loss: 0.18055871 | val_loss: 1.01859587\n",
            "[>] epoch #38, batch #135: loss: 0.31353995 | val_loss: 0.94258046\n",
            "[>] epoch #38, batch #137: loss: 0.22583348 | val_loss: 0.96836341\n",
            "[>] epoch #38, batch #139: loss: 0.15946862 | val_loss: 0.93029731\n",
            "[>] epoch #38, batch #141: loss: 0.33415604 | val_loss: 0.95879573\n",
            "[>] epoch #39, batch #  2: loss: 0.16531698 | val_loss: 0.98018497\n",
            "[>] epoch #39, batch #  4: loss: 0.18144380 | val_loss: 0.98556438\n",
            "[>] epoch #39, batch #  6: loss: 0.17998855 | val_loss: 0.99723897\n",
            "[>] epoch #39, batch #  8: loss: 0.10530296 | val_loss: 0.97144030\n",
            "[>] epoch #39, batch # 10: loss: 0.19446947 | val_loss: 0.98737082\n",
            "[>] epoch #39, batch # 12: loss: 0.14161041 | val_loss: 0.94977090\n",
            "[>] epoch #39, batch # 14: loss: 0.19577236 | val_loss: 0.96178367\n",
            "[>] epoch #39, batch # 16: loss: 0.17993081 | val_loss: 0.94838205\n",
            "[>] epoch #39, batch # 18: loss: 0.15460926 | val_loss: 0.99456363\n",
            "[>] epoch #39, batch # 20: loss: 0.25791514 | val_loss: 0.99911845\n",
            "[>] epoch #39, batch # 22: loss: 0.11546201 | val_loss: 1.00794376\n",
            "[>] epoch #39, batch # 24: loss: 0.12312870 | val_loss: 0.94603771\n",
            "[>] epoch #39, batch # 26: loss: 0.16117045 | val_loss: 0.99025454\n",
            "[>] epoch #39, batch # 28: loss: 0.22543752 | val_loss: 1.00173320\n",
            "[>] epoch #39, batch # 30: loss: 0.12732777 | val_loss: 0.96644461\n",
            "[>] epoch #39, batch # 32: loss: 0.15307079 | val_loss: 0.98281099\n",
            "[>] epoch #39, batch # 34: loss: 0.22715721 | val_loss: 0.98522925\n",
            "[>] epoch #39, batch # 36: loss: 0.08482915 | val_loss: 0.99237595\n",
            "[>] epoch #39, batch # 38: loss: 0.23949690 | val_loss: 1.00801086\n",
            "[>] epoch #39, batch # 40: loss: 0.12889871 | val_loss: 1.02009241\n",
            "[>] epoch #39, batch # 42: loss: 0.15261745 | val_loss: 0.96843539\n",
            "[>] epoch #39, batch # 44: loss: 0.18289754 | val_loss: 0.97829652\n",
            "[>] epoch #39, batch # 46: loss: 0.14062452 | val_loss: 0.96035422\n",
            "[>] epoch #39, batch # 48: loss: 0.18183547 | val_loss: 0.96167517\n",
            "[>] epoch #39, batch # 50: loss: 0.18562357 | val_loss: 0.95809475\n",
            "[>] epoch #39, batch # 52: loss: 0.15974794 | val_loss: 0.99322205\n",
            "[>] epoch #39, batch # 54: loss: 0.20175920 | val_loss: 0.97209877\n",
            "[>] epoch #39, batch # 56: loss: 0.18941337 | val_loss: 0.99742330\n",
            "[>] epoch #39, batch # 58: loss: 0.20086718 | val_loss: 0.98282644\n",
            "[>] epoch #39, batch # 60: loss: 0.17663671 | val_loss: 0.98388572\n",
            "[>] epoch #39, batch # 62: loss: 0.25048104 | val_loss: 1.00589622\n",
            "[>] epoch #39, batch # 64: loss: 0.16196096 | val_loss: 0.96728041\n",
            "[>] epoch #39, batch # 66: loss: 0.16742899 | val_loss: 0.99636059\n",
            "[>] epoch #39, batch # 68: loss: 0.16198699 | val_loss: 0.98382574\n",
            "[>] epoch #39, batch # 70: loss: 0.14325632 | val_loss: 0.98753269\n",
            "[>] epoch #39, batch # 72: loss: 0.17527881 | val_loss: 1.00160494\n",
            "[>] epoch #39, batch # 74: loss: 0.12335207 | val_loss: 1.00768875\n",
            "[>] epoch #39, batch # 76: loss: 0.16560790 | val_loss: 0.99360222\n",
            "[>] epoch #39, batch # 78: loss: 0.19210795 | val_loss: 0.96140280\n",
            "[>] epoch #39, batch # 80: loss: 0.18244874 | val_loss: 0.97959412\n",
            "[>] epoch #39, batch # 82: loss: 0.16146298 | val_loss: 0.97868088\n",
            "[>] epoch #39, batch # 84: loss: 0.15166160 | val_loss: 0.94676633\n",
            "[>] epoch #39, batch # 86: loss: 0.18525814 | val_loss: 0.97162531\n",
            "[>] epoch #39, batch # 88: loss: 0.19907351 | val_loss: 0.98246419\n",
            "[>] epoch #39, batch # 90: loss: 0.18373710 | val_loss: 0.98764108\n",
            "[>] epoch #39, batch # 92: loss: 0.23319595 | val_loss: 1.00264391\n",
            "[>] epoch #39, batch # 94: loss: 0.20498367 | val_loss: 1.01306824\n",
            "[>] epoch #39, batch # 96: loss: 0.22718266 | val_loss: 0.99071771\n",
            "[>] epoch #39, batch # 98: loss: 0.17298208 | val_loss: 0.98692068\n",
            "[>] epoch #39, batch #100: loss: 0.20781453 | val_loss: 0.99724053\n",
            "[>] epoch #39, batch #102: loss: 0.17179778 | val_loss: 0.98007778\n",
            "[>] epoch #39, batch #104: loss: 0.28815234 | val_loss: 0.99791854\n",
            "[>] epoch #39, batch #106: loss: 0.20459829 | val_loss: 1.01082602\n",
            "[>] epoch #39, batch #108: loss: 0.15400225 | val_loss: 0.96298929\n",
            "[>] epoch #39, batch #110: loss: 0.26479915 | val_loss: 1.00255437\n",
            "[>] epoch #39, batch #112: loss: 0.21151915 | val_loss: 0.97672446\n",
            "[>] epoch #39, batch #114: loss: 0.11932569 | val_loss: 0.95296567\n",
            "[>] epoch #39, batch #116: loss: 0.21529721 | val_loss: 0.97457449\n",
            "[>] epoch #39, batch #118: loss: 0.12690973 | val_loss: 0.99886676\n",
            "[>] epoch #39, batch #120: loss: 0.22989421 | val_loss: 0.98642745\n",
            "[>] epoch #39, batch #122: loss: 0.20042874 | val_loss: 0.95131812\n",
            "[>] epoch #39, batch #124: loss: 0.17993324 | val_loss: 0.99261572\n",
            "[>] epoch #39, batch #126: loss: 0.29537702 | val_loss: 1.00689760\n",
            "[>] epoch #39, batch #128: loss: 0.17509191 | val_loss: 1.01111955\n",
            "[>] epoch #39, batch #130: loss: 0.20758532 | val_loss: 0.99674214\n",
            "[>] epoch #39, batch #132: loss: 0.15552810 | val_loss: 0.97781727\n",
            "[>] epoch #39, batch #134: loss: 0.19361456 | val_loss: 0.98123010\n",
            "[>] epoch #39, batch #136: loss: 0.22161324 | val_loss: 0.98027879\n",
            "[>] epoch #39, batch #138: loss: 0.21513523 | val_loss: 0.97120495\n",
            "[>] epoch #39, batch #140: loss: 0.23035824 | val_loss: 0.94121957\n",
            "[>] epoch #40, batch #  1: loss: 0.12797619 | val_loss: 0.98180098\n",
            "[>] epoch #40, batch #  3: loss: 0.17516263 | val_loss: 0.95779068\n",
            "[>] epoch #40, batch #  5: loss: 0.17582981 | val_loss: 1.00096366\n",
            "[>] epoch #40, batch #  7: loss: 0.19882472 | val_loss: 1.00903774\n",
            "[>] epoch #40, batch #  9: loss: 0.16962214 | val_loss: 1.00910756\n",
            "[>] epoch #40, batch # 11: loss: 0.21150659 | val_loss: 0.98789778\n",
            "[>] epoch #40, batch # 13: loss: 0.15560468 | val_loss: 0.94392240\n",
            "[>] epoch #40, batch # 15: loss: 0.25235915 | val_loss: 0.94049907\n",
            "[>] epoch #40, batch # 17: loss: 0.15407218 | val_loss: 0.95093243\n",
            "[>] epoch #40, batch # 19: loss: 0.22258148 | val_loss: 0.97255254\n",
            "[>] epoch #40, batch # 21: loss: 0.23384187 | val_loss: 1.00304981\n",
            "[>] epoch #40, batch # 23: loss: 0.16298752 | val_loss: 0.96751626\n",
            "[>] epoch #40, batch # 25: loss: 0.16277523 | val_loss: 1.02314819\n",
            "[>] epoch #40, batch # 27: loss: 0.17448992 | val_loss: 1.00430850\n",
            "[>] epoch #40, batch # 29: loss: 0.14493604 | val_loss: 0.99376496\n",
            "[>] epoch #40, batch # 31: loss: 0.12146902 | val_loss: 0.97859203\n",
            "[>] epoch #40, batch # 33: loss: 0.14746305 | val_loss: 0.95777751\n",
            "[>] epoch #40, batch # 35: loss: 0.16507927 | val_loss: 0.98561465\n",
            "[>] epoch #40, batch # 37: loss: 0.19953236 | val_loss: 0.99135033\n",
            "[>] epoch #40, batch # 39: loss: 0.17102255 | val_loss: 1.01456554\n",
            "[>] epoch #40, batch # 41: loss: 0.14003827 | val_loss: 0.97852632\n",
            "[>] epoch #40, batch # 43: loss: 0.14498644 | val_loss: 0.98279923\n",
            "[>] epoch #40, batch # 45: loss: 0.22955990 | val_loss: 0.99313150\n",
            "[>] epoch #40, batch # 47: loss: 0.16193166 | val_loss: 0.96317621\n",
            "[>] epoch #40, batch # 49: loss: 0.10150658 | val_loss: 0.98553198\n",
            "[>] epoch #40, batch # 51: loss: 0.20963131 | val_loss: 0.97247775\n",
            "[>] epoch #40, batch # 53: loss: 0.15326358 | val_loss: 0.93699776\n",
            "[>] epoch #40, batch # 55: loss: 0.18837570 | val_loss: 1.00252893\n",
            "[>] epoch #40, batch # 57: loss: 0.18830688 | val_loss: 0.98455386\n",
            "[>] epoch #40, batch # 59: loss: 0.18049511 | val_loss: 1.02109567\n",
            "[>] epoch #40, batch # 61: loss: 0.17320843 | val_loss: 0.98141730\n",
            "[>] epoch #40, batch # 63: loss: 0.15579641 | val_loss: 1.02578361\n",
            "[>] epoch #40, batch # 65: loss: 0.11211786 | val_loss: 0.98626373\n",
            "[>] epoch #40, batch # 67: loss: 0.21928722 | val_loss: 0.97403514\n",
            "[>] epoch #40, batch # 69: loss: 0.21859682 | val_loss: 0.98592348\n",
            "[>] epoch #40, batch # 71: loss: 0.16214190 | val_loss: 0.99051024\n",
            "[>] epoch #40, batch # 73: loss: 0.17509413 | val_loss: 0.95635717\n",
            "[>] epoch #40, batch # 75: loss: 0.13416342 | val_loss: 0.98053300\n",
            "[>] epoch #40, batch # 77: loss: 0.20510487 | val_loss: 0.97021635\n",
            "[>] epoch #40, batch # 79: loss: 0.12245924 | val_loss: 0.97317157\n",
            "[>] epoch #40, batch # 81: loss: 0.11726119 | val_loss: 0.95455174\n",
            "[>] epoch #40, batch # 83: loss: 0.16320999 | val_loss: 1.00570681\n",
            "[>] epoch #40, batch # 85: loss: 0.18944187 | val_loss: 0.97696387\n",
            "[>] epoch #40, batch # 87: loss: 0.18163539 | val_loss: 1.00601340\n",
            "[>] epoch #40, batch # 89: loss: 0.24455269 | val_loss: 0.98897066\n",
            "[>] epoch #40, batch # 91: loss: 0.17556728 | val_loss: 0.98903372\n",
            "[>] epoch #40, batch # 93: loss: 0.19701363 | val_loss: 1.00991839\n",
            "[>] epoch #40, batch # 95: loss: 0.18903621 | val_loss: 0.97378096\n",
            "[>] epoch #40, batch # 97: loss: 0.27541763 | val_loss: 0.97688046\n",
            "[>] epoch #40, batch # 99: loss: 0.18099546 | val_loss: 0.97341048\n",
            "[>] epoch #40, batch #101: loss: 0.25583118 | val_loss: 1.00451899\n",
            "[>] epoch #40, batch #103: loss: 0.15964063 | val_loss: 1.03451421\n",
            "[>] epoch #40, batch #105: loss: 0.15341224 | val_loss: 0.99599172\n",
            "[>] epoch #40, batch #107: loss: 0.18513142 | val_loss: 1.00148846\n",
            "[>] epoch #40, batch #109: loss: 0.19042511 | val_loss: 1.00761738\n",
            "[>] epoch #40, batch #111: loss: 0.19977875 | val_loss: 1.00185710\n",
            "[>] epoch #40, batch #113: loss: 0.18031816 | val_loss: 0.93651829\n",
            "[>] epoch #40, batch #115: loss: 0.22688372 | val_loss: 0.96548990\n",
            "[>] epoch #40, batch #117: loss: 0.22517675 | val_loss: 0.98226596\n",
            "[>] epoch #40, batch #119: loss: 0.18326513 | val_loss: 1.00912068\n",
            "[>] epoch #40, batch #121: loss: 0.20398198 | val_loss: 1.01496573\n",
            "[>] epoch #40, batch #123: loss: 0.16166839 | val_loss: 0.98226264\n",
            "[>] epoch #40, batch #125: loss: 0.16216978 | val_loss: 1.01575783\n",
            "[>] epoch #40, batch #127: loss: 0.18851662 | val_loss: 0.96860080\n",
            "[>] epoch #40, batch #129: loss: 0.20722190 | val_loss: 0.97923005\n",
            "[>] epoch #40, batch #131: loss: 0.20181593 | val_loss: 1.00870295\n",
            "[>] epoch #40, batch #133: loss: 0.15675314 | val_loss: 0.99039781\n",
            "[>] epoch #40, batch #135: loss: 0.18035935 | val_loss: 0.97855692\n",
            "[>] epoch #40, batch #137: loss: 0.19208780 | val_loss: 0.99231214\n",
            "[>] epoch #40, batch #139: loss: 0.18547806 | val_loss: 1.00004084\n",
            "[>] epoch #40, batch #141: loss: 0.11104491 | val_loss: 0.99218471\n",
            "[>] epoch #41, batch #  2: loss: 0.17340854 | val_loss: 0.99368503\n",
            "[>] epoch #41, batch #  4: loss: 0.14111437 | val_loss: 0.92024075\n",
            "[>] epoch #41, batch #  6: loss: 0.16247168 | val_loss: 0.94182044\n",
            "[>] epoch #41, batch #  8: loss: 0.19593659 | val_loss: 0.97284671\n",
            "[>] epoch #41, batch # 10: loss: 0.15991984 | val_loss: 0.97649806\n",
            "[>] epoch #41, batch # 12: loss: 0.16196708 | val_loss: 1.00966497\n",
            "[>] epoch #41, batch # 14: loss: 0.14419879 | val_loss: 1.01654059\n",
            "[>] epoch #41, batch # 16: loss: 0.14452267 | val_loss: 0.97729956\n",
            "[>] epoch #41, batch # 18: loss: 0.15740222 | val_loss: 1.01067461\n",
            "[>] epoch #41, batch # 20: loss: 0.17007534 | val_loss: 1.01234809\n",
            "[>] epoch #41, batch # 22: loss: 0.21579768 | val_loss: 0.98251854\n",
            "[>] epoch #41, batch # 24: loss: 0.16773945 | val_loss: 0.98965343\n",
            "[>] epoch #41, batch # 26: loss: 0.16275394 | val_loss: 0.99758483\n",
            "[>] epoch #41, batch # 28: loss: 0.16704755 | val_loss: 0.99209956\n",
            "[>] epoch #41, batch # 30: loss: 0.13495336 | val_loss: 0.99763144\n",
            "[>] epoch #41, batch # 32: loss: 0.11529072 | val_loss: 0.96834188\n",
            "[>] epoch #41, batch # 34: loss: 0.15377446 | val_loss: 1.01575360\n",
            "[>] epoch #41, batch # 36: loss: 0.16010498 | val_loss: 0.99283773\n",
            "[>] epoch #41, batch # 38: loss: 0.14368384 | val_loss: 0.99795030\n",
            "[>] epoch #41, batch # 40: loss: 0.13407677 | val_loss: 0.99143130\n",
            "[>] epoch #41, batch # 42: loss: 0.12645933 | val_loss: 1.00317587\n",
            "[>] epoch #41, batch # 44: loss: 0.13091309 | val_loss: 0.96429189\n",
            "[>] epoch #41, batch # 46: loss: 0.21519531 | val_loss: 0.98961379\n",
            "[>] epoch #41, batch # 48: loss: 0.14437287 | val_loss: 0.98117082\n",
            "[>] epoch #41, batch # 50: loss: 0.16281165 | val_loss: 1.01449468\n",
            "[>] epoch #41, batch # 52: loss: 0.23043509 | val_loss: 1.02427013\n",
            "[>] epoch #41, batch # 54: loss: 0.17058489 | val_loss: 0.99477096\n",
            "[>] epoch #41, batch # 56: loss: 0.15692304 | val_loss: 0.99355395\n",
            "[>] epoch #41, batch # 58: loss: 0.15177847 | val_loss: 0.98258423\n",
            "[>] epoch #41, batch # 60: loss: 0.20121637 | val_loss: 0.99462489\n",
            "[>] epoch #41, batch # 62: loss: 0.23897968 | val_loss: 0.97290084\n",
            "[>] epoch #41, batch # 64: loss: 0.21153452 | val_loss: 1.02265708\n",
            "[>] epoch #41, batch # 66: loss: 0.13806956 | val_loss: 1.01321923\n",
            "[>] epoch #41, batch # 68: loss: 0.18493289 | val_loss: 0.99004661\n",
            "[>] epoch #41, batch # 70: loss: 0.13905212 | val_loss: 1.00459035\n",
            "[>] epoch #41, batch # 72: loss: 0.17046189 | val_loss: 0.99426023\n",
            "[>] epoch #41, batch # 74: loss: 0.22884366 | val_loss: 0.98117335\n",
            "[>] epoch #41, batch # 76: loss: 0.14266838 | val_loss: 1.02509429\n",
            "[>] epoch #41, batch # 78: loss: 0.11824966 | val_loss: 1.01091152\n",
            "[>] epoch #41, batch # 80: loss: 0.17319250 | val_loss: 0.99319613\n",
            "[>] epoch #41, batch # 82: loss: 0.09667849 | val_loss: 1.02657276\n",
            "[>] epoch #41, batch # 84: loss: 0.16697246 | val_loss: 1.00941575\n",
            "[>] epoch #41, batch # 86: loss: 0.20232132 | val_loss: 0.98038117\n",
            "[>] epoch #41, batch # 88: loss: 0.19184341 | val_loss: 0.98849666\n",
            "[>] epoch #41, batch # 90: loss: 0.19280910 | val_loss: 1.00727434\n",
            "[>] epoch #41, batch # 92: loss: 0.28500471 | val_loss: 0.97212857\n",
            "[>] epoch #41, batch # 94: loss: 0.11448061 | val_loss: 0.97391150\n",
            "[>] epoch #41, batch # 96: loss: 0.15505227 | val_loss: 0.99830947\n",
            "[>] epoch #41, batch # 98: loss: 0.17987697 | val_loss: 1.01104312\n",
            "[>] epoch #41, batch #100: loss: 0.15228395 | val_loss: 1.02107743\n",
            "[>] epoch #41, batch #102: loss: 0.16524349 | val_loss: 0.98980864\n",
            "[>] epoch #41, batch #104: loss: 0.16884300 | val_loss: 0.99692094\n",
            "[>] epoch #41, batch #106: loss: 0.23825243 | val_loss: 0.97278098\n",
            "[>] epoch #41, batch #108: loss: 0.20341958 | val_loss: 0.96423333\n",
            "[>] epoch #41, batch #110: loss: 0.17357904 | val_loss: 0.99409414\n",
            "[>] epoch #41, batch #112: loss: 0.25086039 | val_loss: 1.02526995\n",
            "[>] epoch #41, batch #114: loss: 0.18801999 | val_loss: 1.04310114\n",
            "[>] epoch #41, batch #116: loss: 0.22323763 | val_loss: 1.02925766\n",
            "[>] epoch #41, batch #118: loss: 0.15779316 | val_loss: 1.03629195\n",
            "[>] epoch #41, batch #120: loss: 0.16607386 | val_loss: 0.99251113\n",
            "[>] epoch #41, batch #122: loss: 0.11926027 | val_loss: 0.96236062\n",
            "[>] epoch #41, batch #124: loss: 0.21202859 | val_loss: 0.99594971\n",
            "[>] epoch #41, batch #126: loss: 0.17951010 | val_loss: 1.01436391\n",
            "[>] epoch #41, batch #128: loss: 0.19367470 | val_loss: 0.99601681\n",
            "[>] epoch #41, batch #130: loss: 0.22652182 | val_loss: 0.99877638\n",
            "[>] epoch #41, batch #132: loss: 0.18888530 | val_loss: 0.99882907\n",
            "[>] epoch #41, batch #134: loss: 0.27697471 | val_loss: 1.01463918\n",
            "[>] epoch #41, batch #136: loss: 0.12700835 | val_loss: 0.99827858\n",
            "[>] epoch #41, batch #138: loss: 0.21967773 | val_loss: 1.00217815\n",
            "[>] epoch #41, batch #140: loss: 0.23886134 | val_loss: 1.01859813\n",
            "[>] epoch #42, batch #  1: loss: 0.13207594 | val_loss: 1.03530006\n",
            "[>] epoch #42, batch #  3: loss: 0.19566813 | val_loss: 0.99251801\n",
            "[>] epoch #42, batch #  5: loss: 0.11966346 | val_loss: 0.99169291\n",
            "[>] epoch #42, batch #  7: loss: 0.15812156 | val_loss: 0.98865957\n",
            "[>] epoch #42, batch #  9: loss: 0.16667885 | val_loss: 1.02651386\n",
            "[>] epoch #42, batch # 11: loss: 0.13031845 | val_loss: 1.03587066\n",
            "[>] epoch #42, batch # 13: loss: 0.15454994 | val_loss: 1.01400125\n",
            "[>] epoch #42, batch # 15: loss: 0.14306536 | val_loss: 0.99347999\n",
            "[>] epoch #42, batch # 17: loss: 0.11787096 | val_loss: 1.02720055\n",
            "[>] epoch #42, batch # 19: loss: 0.13001746 | val_loss: 1.00174532\n",
            "[>] epoch #42, batch # 21: loss: 0.13159774 | val_loss: 0.97666185\n",
            "[>] epoch #42, batch # 23: loss: 0.12592231 | val_loss: 0.98555823\n",
            "[>] epoch #42, batch # 25: loss: 0.11124723 | val_loss: 1.00071047\n",
            "[>] epoch #42, batch # 27: loss: 0.11898106 | val_loss: 1.03659217\n",
            "[>] epoch #42, batch # 29: loss: 0.15419760 | val_loss: 0.99468306\n",
            "[>] epoch #42, batch # 31: loss: 0.12023766 | val_loss: 1.03159495\n",
            "[>] epoch #42, batch # 33: loss: 0.18152612 | val_loss: 1.00115927\n",
            "[>] epoch #42, batch # 35: loss: 0.19933778 | val_loss: 0.99493715\n",
            "[>] epoch #42, batch # 37: loss: 0.20053570 | val_loss: 0.98988602\n",
            "[>] epoch #42, batch # 39: loss: 0.15904106 | val_loss: 1.01763445\n",
            "[>] epoch #42, batch # 41: loss: 0.15577705 | val_loss: 0.99796698\n",
            "[>] epoch #42, batch # 43: loss: 0.14796542 | val_loss: 1.02692070\n",
            "[>] epoch #42, batch # 45: loss: 0.12449794 | val_loss: 1.00989158\n",
            "[>] epoch #42, batch # 47: loss: 0.17489955 | val_loss: 1.02285913\n",
            "[>] epoch #42, batch # 49: loss: 0.17129070 | val_loss: 1.03478141\n",
            "[>] epoch #42, batch # 51: loss: 0.14769183 | val_loss: 1.01695566\n",
            "[>] epoch #42, batch # 53: loss: 0.21712281 | val_loss: 1.01721229\n",
            "[>] epoch #42, batch # 55: loss: 0.14737889 | val_loss: 0.99690877\n",
            "[>] epoch #42, batch # 57: loss: 0.18474910 | val_loss: 1.00402572\n",
            "[>] epoch #42, batch # 59: loss: 0.13627256 | val_loss: 0.99710960\n",
            "[>] epoch #42, batch # 61: loss: 0.22554171 | val_loss: 1.00366975\n",
            "[>] epoch #42, batch # 63: loss: 0.14544345 | val_loss: 0.99778692\n",
            "[>] epoch #42, batch # 65: loss: 0.14026183 | val_loss: 1.00429907\n",
            "[>] epoch #42, batch # 67: loss: 0.11755272 | val_loss: 1.02090463\n",
            "[>] epoch #42, batch # 69: loss: 0.17205065 | val_loss: 0.98860847\n",
            "[>] epoch #42, batch # 71: loss: 0.25728628 | val_loss: 1.00276339\n",
            "[>] epoch #42, batch # 73: loss: 0.10705385 | val_loss: 0.98961107\n",
            "[>] epoch #42, batch # 75: loss: 0.12125434 | val_loss: 0.99281695\n",
            "[>] epoch #42, batch # 77: loss: 0.19638617 | val_loss: 0.96149179\n",
            "[>] epoch #42, batch # 79: loss: 0.17742519 | val_loss: 0.98322649\n",
            "[>] epoch #42, batch # 81: loss: 0.19896075 | val_loss: 0.98853470\n",
            "[>] epoch #42, batch # 83: loss: 0.10283037 | val_loss: 1.00152781\n",
            "[>] epoch #42, batch # 85: loss: 0.18238628 | val_loss: 0.99129390\n",
            "[>] epoch #42, batch # 87: loss: 0.23986849 | val_loss: 1.01144046\n",
            "[>] epoch #42, batch # 89: loss: 0.16276698 | val_loss: 1.00523227\n",
            "[>] epoch #42, batch # 91: loss: 0.16804987 | val_loss: 0.96100005\n",
            "[>] epoch #42, batch # 93: loss: 0.19595972 | val_loss: 0.96735857\n",
            "[>] epoch #42, batch # 95: loss: 0.16530435 | val_loss: 1.01524353\n",
            "[>] epoch #42, batch # 97: loss: 0.13215081 | val_loss: 0.97509182\n",
            "[>] epoch #42, batch # 99: loss: 0.18878201 | val_loss: 1.03080207\n",
            "[>] epoch #42, batch #101: loss: 0.21104375 | val_loss: 1.01579226\n",
            "[>] epoch #42, batch #103: loss: 0.20364107 | val_loss: 0.98979757\n",
            "[>] epoch #42, batch #105: loss: 0.18833736 | val_loss: 1.04351235\n",
            "[>] epoch #42, batch #107: loss: 0.20240016 | val_loss: 1.00701125\n",
            "[>] epoch #42, batch #109: loss: 0.17301770 | val_loss: 0.99731320\n",
            "[>] epoch #42, batch #111: loss: 0.18124521 | val_loss: 1.04653955\n",
            "[>] epoch #42, batch #113: loss: 0.20242810 | val_loss: 1.03818948\n",
            "[>] epoch #42, batch #115: loss: 0.21757534 | val_loss: 0.99061113\n",
            "[>] epoch #42, batch #117: loss: 0.17451242 | val_loss: 1.02219646\n",
            "[>] epoch #42, batch #119: loss: 0.16768721 | val_loss: 0.99012392\n",
            "[>] epoch #42, batch #121: loss: 0.20037529 | val_loss: 0.98525298\n",
            "[>] epoch #42, batch #123: loss: 0.23565243 | val_loss: 0.99189706\n",
            "[>] epoch #42, batch #125: loss: 0.23391101 | val_loss: 1.01231379\n",
            "[>] epoch #42, batch #127: loss: 0.22908840 | val_loss: 1.00371473\n",
            "[>] epoch #42, batch #129: loss: 0.26376936 | val_loss: 0.99268461\n",
            "[>] epoch #42, batch #131: loss: 0.19025642 | val_loss: 1.01374512\n",
            "[>] epoch #42, batch #133: loss: 0.17054425 | val_loss: 1.04573069\n",
            "[>] epoch #42, batch #135: loss: 0.13974732 | val_loss: 1.04793273\n",
            "[>] epoch #42, batch #137: loss: 0.20071138 | val_loss: 1.01862564\n",
            "[>] epoch #42, batch #139: loss: 0.19333565 | val_loss: 1.00462527\n",
            "[>] epoch #42, batch #141: loss: 0.07585949 | val_loss: 1.02141451\n",
            "[>] epoch #43, batch #  2: loss: 0.10419998 | val_loss: 1.02983815\n",
            "[>] epoch #43, batch #  4: loss: 0.16876744 | val_loss: 0.98649438\n",
            "[>] epoch #43, batch #  6: loss: 0.16456550 | val_loss: 1.02466213\n",
            "[>] epoch #43, batch #  8: loss: 0.12614207 | val_loss: 0.99839117\n",
            "[>] epoch #43, batch # 10: loss: 0.16271290 | val_loss: 1.01228898\n",
            "[>] epoch #43, batch # 12: loss: 0.18371829 | val_loss: 1.01976556\n",
            "[>] epoch #43, batch # 14: loss: 0.13771668 | val_loss: 1.05711647\n",
            "[>] epoch #43, batch # 16: loss: 0.15363728 | val_loss: 1.04884604\n",
            "[>] epoch #43, batch # 18: loss: 0.21583784 | val_loss: 0.98866392\n",
            "[>] epoch #43, batch # 20: loss: 0.18084513 | val_loss: 0.99483860\n",
            "[>] epoch #43, batch # 22: loss: 0.15747358 | val_loss: 1.01214523\n",
            "[>] epoch #43, batch # 24: loss: 0.13702729 | val_loss: 1.01681144\n",
            "[>] epoch #43, batch # 26: loss: 0.12124791 | val_loss: 0.99265864\n",
            "[>] epoch #43, batch # 28: loss: 0.16633537 | val_loss: 0.98033441\n",
            "[>] epoch #43, batch # 30: loss: 0.16751949 | val_loss: 1.00505993\n",
            "[>] epoch #43, batch # 32: loss: 0.11988361 | val_loss: 1.04681593\n",
            "[>] epoch #43, batch # 34: loss: 0.14499415 | val_loss: 1.00824581\n",
            "[>] epoch #43, batch # 36: loss: 0.13937300 | val_loss: 0.99797608\n",
            "[>] epoch #43, batch # 38: loss: 0.15634513 | val_loss: 1.03089449\n",
            "[>] epoch #43, batch # 40: loss: 0.16246180 | val_loss: 1.00392477\n",
            "[>] epoch #43, batch # 42: loss: 0.14479446 | val_loss: 0.99650798\n",
            "[>] epoch #43, batch # 44: loss: 0.17173547 | val_loss: 0.98916325\n",
            "[>] epoch #43, batch # 46: loss: 0.13269359 | val_loss: 0.99752557\n",
            "[>] epoch #43, batch # 48: loss: 0.16412120 | val_loss: 0.98489577\n",
            "[>] epoch #43, batch # 50: loss: 0.15305687 | val_loss: 1.03743736\n",
            "[>] epoch #43, batch # 52: loss: 0.21389663 | val_loss: 1.03543524\n",
            "[>] epoch #43, batch # 54: loss: 0.14964831 | val_loss: 1.01999824\n",
            "[>] epoch #43, batch # 56: loss: 0.18661824 | val_loss: 1.00391113\n",
            "[>] epoch #43, batch # 58: loss: 0.14764600 | val_loss: 0.96712774\n",
            "[>] epoch #43, batch # 60: loss: 0.20550780 | val_loss: 1.03608046\n",
            "[>] epoch #43, batch # 62: loss: 0.18941258 | val_loss: 0.98625066\n",
            "[>] epoch #43, batch # 64: loss: 0.19768956 | val_loss: 0.98309269\n",
            "[>] epoch #43, batch # 66: loss: 0.12968899 | val_loss: 1.00822249\n",
            "[>] epoch #43, batch # 68: loss: 0.16289918 | val_loss: 1.03107690\n",
            "[>] epoch #43, batch # 70: loss: 0.16481958 | val_loss: 1.04618215\n",
            "[>] epoch #43, batch # 72: loss: 0.11987535 | val_loss: 1.01512933\n",
            "[>] epoch #43, batch # 74: loss: 0.17989241 | val_loss: 0.99626847\n",
            "[>] epoch #43, batch # 76: loss: 0.17038514 | val_loss: 0.99161899\n",
            "[>] epoch #43, batch # 78: loss: 0.16400839 | val_loss: 1.04534985\n",
            "[>] epoch #43, batch # 80: loss: 0.21047547 | val_loss: 1.01171605\n",
            "[>] epoch #43, batch # 82: loss: 0.16407697 | val_loss: 0.99409385\n",
            "[>] epoch #43, batch # 84: loss: 0.18327054 | val_loss: 1.03352113\n",
            "[>] epoch #43, batch # 86: loss: 0.16042657 | val_loss: 1.02039542\n",
            "[>] epoch #43, batch # 88: loss: 0.21688144 | val_loss: 1.01128349\n",
            "[>] epoch #43, batch # 90: loss: 0.14193586 | val_loss: 1.00396518\n",
            "[>] epoch #43, batch # 92: loss: 0.19202280 | val_loss: 0.98360446\n",
            "[>] epoch #43, batch # 94: loss: 0.17709823 | val_loss: 0.98789705\n",
            "[>] epoch #43, batch # 96: loss: 0.21555902 | val_loss: 1.01908880\n",
            "[>] epoch #43, batch # 98: loss: 0.15680949 | val_loss: 1.04165966\n",
            "[>] epoch #43, batch #100: loss: 0.12884089 | val_loss: 1.00274022\n",
            "[>] epoch #43, batch #102: loss: 0.19288322 | val_loss: 1.03137721\n",
            "[>] epoch #43, batch #104: loss: 0.19308004 | val_loss: 1.01687170\n",
            "[>] epoch #43, batch #106: loss: 0.14695381 | val_loss: 1.04330485\n",
            "[>] epoch #43, batch #108: loss: 0.28732207 | val_loss: 0.97541305\n",
            "[>] epoch #43, batch #110: loss: 0.16270176 | val_loss: 0.98616206\n",
            "[>] epoch #43, batch #112: loss: 0.11389616 | val_loss: 0.99935114\n",
            "[>] epoch #43, batch #114: loss: 0.18532379 | val_loss: 0.98272573\n",
            "[>] epoch #43, batch #116: loss: 0.18915091 | val_loss: 1.05523258\n",
            "[>] epoch #43, batch #118: loss: 0.24272475 | val_loss: 1.01438696\n",
            "[>] epoch #43, batch #120: loss: 0.18451913 | val_loss: 1.00547834\n",
            "[>] epoch #43, batch #122: loss: 0.18925090 | val_loss: 1.00659143\n",
            "[>] epoch #43, batch #124: loss: 0.15436554 | val_loss: 1.02430234\n",
            "[>] epoch #43, batch #126: loss: 0.22554873 | val_loss: 1.02299095\n",
            "[>] epoch #43, batch #128: loss: 0.21588305 | val_loss: 1.00266123\n",
            "[>] epoch #43, batch #130: loss: 0.16707587 | val_loss: 1.05141545\n",
            "[>] epoch #43, batch #132: loss: 0.13011871 | val_loss: 1.01567088\n",
            "[>] epoch #43, batch #134: loss: 0.20114559 | val_loss: 1.02999783\n",
            "[>] epoch #43, batch #136: loss: 0.18328741 | val_loss: 1.02512191\n",
            "[>] epoch #43, batch #138: loss: 0.15624963 | val_loss: 1.05252575\n",
            "[>] epoch #43, batch #140: loss: 0.14208207 | val_loss: 1.01697146\n",
            "[>] epoch #44, batch #  1: loss: 0.23457395 | val_loss: 0.99439086\n",
            "[>] epoch #44, batch #  3: loss: 0.12952232 | val_loss: 1.02377502\n",
            "[>] epoch #44, batch #  5: loss: 0.13995108 | val_loss: 1.03689222\n",
            "[>] epoch #44, batch #  7: loss: 0.17928553 | val_loss: 1.05058040\n",
            "[>] epoch #44, batch #  9: loss: 0.12557709 | val_loss: 0.98738479\n",
            "[>] epoch #44, batch # 11: loss: 0.08849665 | val_loss: 1.00939235\n",
            "[>] epoch #44, batch # 13: loss: 0.13887197 | val_loss: 1.03087444\n",
            "[>] epoch #44, batch # 15: loss: 0.16180599 | val_loss: 1.04710191\n",
            "[>] epoch #44, batch # 17: loss: 0.17461552 | val_loss: 0.99340505\n",
            "[>] epoch #44, batch # 19: loss: 0.16594507 | val_loss: 1.03953736\n",
            "[>] epoch #44, batch # 21: loss: 0.13943195 | val_loss: 1.00758398\n",
            "[>] epoch #44, batch # 23: loss: 0.10900751 | val_loss: 0.99509212\n",
            "[>] epoch #44, batch # 25: loss: 0.18492942 | val_loss: 0.98369590\n",
            "[>] epoch #44, batch # 27: loss: 0.13339610 | val_loss: 1.02619613\n",
            "[>] epoch #44, batch # 29: loss: 0.10959862 | val_loss: 1.00927431\n",
            "[>] epoch #44, batch # 31: loss: 0.16305427 | val_loss: 1.03890946\n",
            "[>] epoch #44, batch # 33: loss: 0.15500535 | val_loss: 0.96957406\n",
            "[>] epoch #44, batch # 35: loss: 0.10862117 | val_loss: 1.00160783\n",
            "[>] epoch #44, batch # 37: loss: 0.17162637 | val_loss: 1.00282926\n",
            "[>] epoch #44, batch # 39: loss: 0.16964187 | val_loss: 1.06080323\n",
            "[>] epoch #44, batch # 41: loss: 0.21918565 | val_loss: 1.02566425\n",
            "[>] epoch #44, batch # 43: loss: 0.13641781 | val_loss: 1.01722185\n",
            "[>] epoch #44, batch # 45: loss: 0.14137301 | val_loss: 0.99540112\n",
            "[>] epoch #44, batch # 47: loss: 0.14488693 | val_loss: 1.04234827\n",
            "[>] epoch #44, batch # 49: loss: 0.17871903 | val_loss: 1.06903428\n",
            "[>] epoch #44, batch # 51: loss: 0.12283704 | val_loss: 1.01474659\n",
            "[>] epoch #44, batch # 53: loss: 0.20433925 | val_loss: 0.97577684\n",
            "[>] epoch #44, batch # 55: loss: 0.14594969 | val_loss: 1.01043801\n",
            "[>] epoch #44, batch # 57: loss: 0.21445633 | val_loss: 1.04849724\n",
            "[>] epoch #44, batch # 59: loss: 0.20038806 | val_loss: 1.01702850\n",
            "[>] epoch #44, batch # 61: loss: 0.16170622 | val_loss: 1.04117876\n",
            "[>] epoch #44, batch # 63: loss: 0.13534506 | val_loss: 1.01678889\n",
            "[>] epoch #44, batch # 65: loss: 0.23367514 | val_loss: 1.05911658\n",
            "[>] epoch #44, batch # 67: loss: 0.13998061 | val_loss: 1.03637027\n",
            "[>] epoch #44, batch # 69: loss: 0.16351408 | val_loss: 1.01715643\n",
            "[>] epoch #44, batch # 71: loss: 0.17466314 | val_loss: 1.02560277\n",
            "[>] epoch #44, batch # 73: loss: 0.15370233 | val_loss: 1.03528147\n",
            "[>] epoch #44, batch # 75: loss: 0.10824597 | val_loss: 1.03788214\n",
            "[>] epoch #44, batch # 77: loss: 0.12404184 | val_loss: 1.01725663\n",
            "[>] epoch #44, batch # 79: loss: 0.11123901 | val_loss: 1.00325444\n",
            "[>] epoch #44, batch # 81: loss: 0.13424161 | val_loss: 1.04443046\n",
            "[>] epoch #44, batch # 83: loss: 0.14715935 | val_loss: 1.00743989\n",
            "[>] epoch #44, batch # 85: loss: 0.12022027 | val_loss: 1.02044466\n",
            "[>] epoch #44, batch # 87: loss: 0.21738274 | val_loss: 1.04459188\n",
            "[>] epoch #44, batch # 89: loss: 0.18752211 | val_loss: 1.00065851\n",
            "[>] epoch #44, batch # 91: loss: 0.14903076 | val_loss: 1.06278046\n",
            "[>] epoch #44, batch # 93: loss: 0.18652669 | val_loss: 0.98678199\n",
            "[>] epoch #44, batch # 95: loss: 0.20115867 | val_loss: 0.96725930\n",
            "[>] epoch #44, batch # 97: loss: 0.12411893 | val_loss: 1.02285439\n",
            "[>] epoch #44, batch # 99: loss: 0.17051029 | val_loss: 1.03316398\n",
            "[>] epoch #44, batch #101: loss: 0.15297639 | val_loss: 1.00207578\n",
            "[>] epoch #44, batch #103: loss: 0.17282321 | val_loss: 1.02102411\n",
            "[>] epoch #44, batch #105: loss: 0.20690978 | val_loss: 1.07020955\n",
            "[>] epoch #44, batch #107: loss: 0.18930921 | val_loss: 0.99968714\n",
            "[>] epoch #44, batch #109: loss: 0.16396879 | val_loss: 1.02002953\n",
            "[>] epoch #44, batch #111: loss: 0.16072603 | val_loss: 1.00601485\n",
            "[>] epoch #44, batch #113: loss: 0.14756696 | val_loss: 1.00838454\n",
            "[>] epoch #44, batch #115: loss: 0.25347236 | val_loss: 1.04582218\n",
            "[>] epoch #44, batch #117: loss: 0.23663439 | val_loss: 1.00819437\n",
            "[>] epoch #44, batch #119: loss: 0.14367042 | val_loss: 1.07586697\n",
            "[>] epoch #44, batch #121: loss: 0.15804085 | val_loss: 1.05702833\n",
            "[>] epoch #44, batch #123: loss: 0.20205280 | val_loss: 1.01107890\n",
            "[>] epoch #44, batch #125: loss: 0.16187070 | val_loss: 1.02557041\n",
            "[>] epoch #44, batch #127: loss: 0.12946337 | val_loss: 1.02806492\n",
            "[>] epoch #44, batch #129: loss: 0.22664109 | val_loss: 0.99904417\n",
            "[>] epoch #44, batch #131: loss: 0.13107112 | val_loss: 1.00220806\n",
            "[>] epoch #44, batch #133: loss: 0.17714500 | val_loss: 1.01839094\n",
            "[>] epoch #44, batch #135: loss: 0.10299566 | val_loss: 1.01501470\n",
            "[>] epoch #44, batch #137: loss: 0.15631060 | val_loss: 1.01161278\n",
            "[>] epoch #44, batch #139: loss: 0.14214684 | val_loss: 1.01150477\n",
            "[>] epoch #44, batch #141: loss: 0.26055762 | val_loss: 1.02464318\n",
            "[>] epoch #45, batch #  2: loss: 0.09564301 | val_loss: 1.05890399\n",
            "[>] epoch #45, batch #  4: loss: 0.14941816 | val_loss: 1.03713356\n",
            "[>] epoch #45, batch #  6: loss: 0.15895039 | val_loss: 1.00301816\n",
            "[>] epoch #45, batch #  8: loss: 0.12373489 | val_loss: 0.97331885\n",
            "[>] epoch #45, batch # 10: loss: 0.10694484 | val_loss: 1.04059230\n",
            "[>] epoch #45, batch # 12: loss: 0.13810955 | val_loss: 1.02158091\n",
            "[>] epoch #45, batch # 14: loss: 0.18243717 | val_loss: 1.03795106\n",
            "[>] epoch #45, batch # 16: loss: 0.11392420 | val_loss: 1.01016765\n",
            "[>] epoch #45, batch # 18: loss: 0.16087280 | val_loss: 1.01121680\n",
            "[>] epoch #45, batch # 20: loss: 0.19646670 | val_loss: 1.02291877\n",
            "[>] epoch #45, batch # 22: loss: 0.11933745 | val_loss: 1.01950710\n",
            "[>] epoch #45, batch # 24: loss: 0.12382303 | val_loss: 0.98133730\n",
            "[>] epoch #45, batch # 26: loss: 0.12998123 | val_loss: 1.00811995\n",
            "[>] epoch #45, batch # 28: loss: 0.16769925 | val_loss: 1.02255466\n",
            "[>] epoch #45, batch # 30: loss: 0.21877752 | val_loss: 1.01569325\n",
            "[>] epoch #45, batch # 32: loss: 0.17206903 | val_loss: 1.00762650\n",
            "[>] epoch #45, batch # 34: loss: 0.13015321 | val_loss: 1.02193254\n",
            "[>] epoch #45, batch # 36: loss: 0.13866730 | val_loss: 1.02296241\n",
            "[>] epoch #45, batch # 38: loss: 0.10919476 | val_loss: 1.03737080\n",
            "[>] epoch #45, batch # 40: loss: 0.15353455 | val_loss: 1.02859330\n",
            "[>] epoch #45, batch # 42: loss: 0.10654260 | val_loss: 1.03074617\n",
            "[>] epoch #45, batch # 44: loss: 0.10627411 | val_loss: 1.03638399\n",
            "[>] epoch #45, batch # 46: loss: 0.20505719 | val_loss: 1.00903781\n",
            "[>] epoch #45, batch # 48: loss: 0.18733619 | val_loss: 0.98516445\n",
            "[>] epoch #45, batch # 50: loss: 0.16688618 | val_loss: 1.03119385\n",
            "[>] epoch #45, batch # 52: loss: 0.15667753 | val_loss: 1.02494911\n",
            "[>] epoch #45, batch # 54: loss: 0.11511330 | val_loss: 1.03483120\n",
            "[>] epoch #45, batch # 56: loss: 0.12608172 | val_loss: 1.03410020\n",
            "[>] epoch #45, batch # 58: loss: 0.11868320 | val_loss: 1.05834795\n",
            "[>] epoch #45, batch # 60: loss: 0.17989451 | val_loss: 1.03089337\n",
            "[>] epoch #45, batch # 62: loss: 0.16710204 | val_loss: 1.02900173\n",
            "[>] epoch #45, batch # 64: loss: 0.13793340 | val_loss: 1.02002214\n",
            "[>] epoch #45, batch # 66: loss: 0.15824820 | val_loss: 1.00934880\n",
            "[>] epoch #45, batch # 68: loss: 0.20159790 | val_loss: 1.02471083\n",
            "[>] epoch #45, batch # 70: loss: 0.11752875 | val_loss: 1.03341643\n",
            "[>] epoch #45, batch # 72: loss: 0.13619363 | val_loss: 1.02737186\n",
            "[>] epoch #45, batch # 74: loss: 0.15043615 | val_loss: 1.03915144\n",
            "[>] epoch #45, batch # 76: loss: 0.12101357 | val_loss: 1.03940764\n",
            "[>] epoch #45, batch # 78: loss: 0.14470911 | val_loss: 1.01189657\n",
            "[>] epoch #45, batch # 80: loss: 0.16013283 | val_loss: 1.03307896\n",
            "[>] epoch #45, batch # 82: loss: 0.07420242 | val_loss: 0.99042369\n",
            "[>] epoch #45, batch # 84: loss: 0.20638673 | val_loss: 1.01314442\n",
            "[>] epoch #45, batch # 86: loss: 0.15615541 | val_loss: 0.99523650\n",
            "[>] epoch #45, batch # 88: loss: 0.14014481 | val_loss: 1.01922999\n",
            "[>] epoch #45, batch # 90: loss: 0.24249062 | val_loss: 1.00116305\n",
            "[>] epoch #45, batch # 92: loss: 0.09529500 | val_loss: 1.04067682\n",
            "[>] epoch #45, batch # 94: loss: 0.14052859 | val_loss: 0.99151462\n",
            "[>] epoch #45, batch # 96: loss: 0.14819463 | val_loss: 0.95726055\n",
            "[>] epoch #45, batch # 98: loss: 0.13963443 | val_loss: 1.02790752\n",
            "[>] epoch #45, batch #100: loss: 0.09994426 | val_loss: 0.98896566\n",
            "[>] epoch #45, batch #102: loss: 0.15121138 | val_loss: 0.99889630\n",
            "[>] epoch #45, batch #104: loss: 0.13881746 | val_loss: 1.01371287\n",
            "[>] epoch #45, batch #106: loss: 0.18042257 | val_loss: 0.97889679\n",
            "[>] epoch #45, batch #108: loss: 0.20865224 | val_loss: 0.98408607\n",
            "[>] epoch #45, batch #110: loss: 0.17004973 | val_loss: 1.01734840\n",
            "[>] epoch #45, batch #112: loss: 0.20005138 | val_loss: 0.96412960\n",
            "[>] epoch #45, batch #114: loss: 0.14292535 | val_loss: 0.99419031\n",
            "[>] epoch #45, batch #116: loss: 0.15723133 | val_loss: 1.00115611\n",
            "[>] epoch #45, batch #118: loss: 0.13213079 | val_loss: 1.00672886\n",
            "[>] epoch #45, batch #120: loss: 0.19982390 | val_loss: 1.02546735\n",
            "[>] epoch #45, batch #122: loss: 0.16503188 | val_loss: 1.02706402\n",
            "[>] epoch #45, batch #124: loss: 0.19140789 | val_loss: 1.02805938\n",
            "[>] epoch #45, batch #126: loss: 0.16813226 | val_loss: 0.97455035\n",
            "[>] epoch #45, batch #128: loss: 0.20459770 | val_loss: 0.99792049\n",
            "[>] epoch #45, batch #130: loss: 0.19510047 | val_loss: 1.03619473\n",
            "[>] epoch #45, batch #132: loss: 0.20677319 | val_loss: 0.99728349\n",
            "[>] epoch #45, batch #134: loss: 0.19121084 | val_loss: 0.99415049\n",
            "[>] epoch #45, batch #136: loss: 0.16744782 | val_loss: 1.05956427\n",
            "[>] epoch #45, batch #138: loss: 0.19772722 | val_loss: 1.02898728\n",
            "[>] epoch #45, batch #140: loss: 0.14993921 | val_loss: 1.03444099\n",
            "[>] epoch #46, batch #  1: loss: 0.13723485 | val_loss: 0.99072084\n",
            "[>] epoch #46, batch #  3: loss: 0.19521098 | val_loss: 0.98297321\n",
            "[>] epoch #46, batch #  5: loss: 0.08663405 | val_loss: 1.02200359\n",
            "[>] epoch #46, batch #  7: loss: 0.09879187 | val_loss: 1.00417759\n",
            "[>] epoch #46, batch #  9: loss: 0.14565633 | val_loss: 1.03034177\n",
            "[>] epoch #46, batch # 11: loss: 0.18498050 | val_loss: 1.01200852\n",
            "[>] epoch #46, batch # 13: loss: 0.09445341 | val_loss: 0.99823260\n",
            "[>] epoch #46, batch # 15: loss: 0.14115590 | val_loss: 1.01583659\n",
            "[>] epoch #46, batch # 17: loss: 0.17218572 | val_loss: 1.05365189\n",
            "[>] epoch #46, batch # 19: loss: 0.16752923 | val_loss: 1.00872537\n",
            "[>] epoch #46, batch # 21: loss: 0.15905675 | val_loss: 1.00666421\n",
            "[>] epoch #46, batch # 23: loss: 0.19612889 | val_loss: 1.00615486\n",
            "[>] epoch #46, batch # 25: loss: 0.15100646 | val_loss: 0.99857897\n",
            "[>] epoch #46, batch # 27: loss: 0.18457557 | val_loss: 0.98329925\n",
            "[>] epoch #46, batch # 29: loss: 0.15223292 | val_loss: 0.99353845\n",
            "[>] epoch #46, batch # 31: loss: 0.11235085 | val_loss: 1.00742291\n",
            "[>] epoch #46, batch # 33: loss: 0.13916060 | val_loss: 1.03755499\n",
            "[>] epoch #46, batch # 35: loss: 0.16059569 | val_loss: 0.99244887\n",
            "[>] epoch #46, batch # 37: loss: 0.15383431 | val_loss: 1.02890005\n",
            "[>] epoch #46, batch # 39: loss: 0.16063051 | val_loss: 0.97538852\n",
            "[>] epoch #46, batch # 41: loss: 0.20407166 | val_loss: 0.96635186\n",
            "[>] epoch #46, batch # 43: loss: 0.16742887 | val_loss: 1.01815365\n",
            "[>] epoch #46, batch # 45: loss: 0.23893116 | val_loss: 1.03049645\n",
            "[>] epoch #46, batch # 47: loss: 0.20548172 | val_loss: 0.99545090\n",
            "[>] epoch #46, batch # 49: loss: 0.16181520 | val_loss: 1.00733118\n",
            "[>] epoch #46, batch # 51: loss: 0.21548787 | val_loss: 1.02892475\n",
            "[>] epoch #46, batch # 53: loss: 0.15650302 | val_loss: 1.02268335\n",
            "[>] epoch #46, batch # 55: loss: 0.16300462 | val_loss: 1.00167106\n",
            "[>] epoch #46, batch # 57: loss: 0.10060869 | val_loss: 1.02102866\n",
            "[>] epoch #46, batch # 59: loss: 0.14888029 | val_loss: 1.00016266\n",
            "[>] epoch #46, batch # 61: loss: 0.16512920 | val_loss: 1.00115593\n",
            "[>] epoch #46, batch # 63: loss: 0.14343759 | val_loss: 1.00787686\n",
            "[>] epoch #46, batch # 65: loss: 0.13218783 | val_loss: 1.02292253\n",
            "[>] epoch #46, batch # 67: loss: 0.12668788 | val_loss: 1.02673079\n",
            "[>] epoch #46, batch # 69: loss: 0.13704897 | val_loss: 1.02533113\n",
            "[>] epoch #46, batch # 71: loss: 0.15872724 | val_loss: 1.03102186\n",
            "[>] epoch #46, batch # 73: loss: 0.16119243 | val_loss: 1.01803757\n",
            "[>] epoch #46, batch # 75: loss: 0.12225189 | val_loss: 1.01292153\n",
            "[>] epoch #46, batch # 77: loss: 0.14844413 | val_loss: 0.99244808\n",
            "[>] epoch #46, batch # 79: loss: 0.11099661 | val_loss: 1.03900014\n",
            "[>] epoch #46, batch # 81: loss: 0.13466182 | val_loss: 0.99615503\n",
            "[>] epoch #46, batch # 83: loss: 0.15424970 | val_loss: 1.01818866\n",
            "[>] epoch #46, batch # 85: loss: 0.17361909 | val_loss: 1.01299640\n",
            "[>] epoch #46, batch # 87: loss: 0.13687608 | val_loss: 1.02053288\n",
            "[>] epoch #46, batch # 89: loss: 0.13674323 | val_loss: 1.00303348\n",
            "[>] epoch #46, batch # 91: loss: 0.13346891 | val_loss: 1.01177095\n",
            "[>] epoch #46, batch # 93: loss: 0.14403591 | val_loss: 1.01625235\n",
            "[>] epoch #46, batch # 95: loss: 0.22499934 | val_loss: 1.02160882\n",
            "[>] epoch #46, batch # 97: loss: 0.13158695 | val_loss: 1.01400518\n",
            "[>] epoch #46, batch # 99: loss: 0.13185373 | val_loss: 1.04112118\n",
            "[>] epoch #46, batch #101: loss: 0.10708961 | val_loss: 1.01425252\n",
            "[>] epoch #46, batch #103: loss: 0.13707778 | val_loss: 1.00894557\n",
            "[>] epoch #46, batch #105: loss: 0.11826400 | val_loss: 0.99758441\n",
            "[>] epoch #46, batch #107: loss: 0.16714408 | val_loss: 1.02283002\n",
            "[>] epoch #46, batch #109: loss: 0.15275802 | val_loss: 1.02274535\n",
            "[>] epoch #46, batch #111: loss: 0.13248664 | val_loss: 1.03013348\n",
            "[>] epoch #46, batch #113: loss: 0.11294288 | val_loss: 1.01703287\n",
            "[>] epoch #46, batch #115: loss: 0.14843617 | val_loss: 1.00057212\n",
            "[>] epoch #46, batch #117: loss: 0.12493344 | val_loss: 1.02617323\n",
            "[>] epoch #46, batch #119: loss: 0.13009076 | val_loss: 1.02860958\n",
            "[>] epoch #46, batch #121: loss: 0.17608817 | val_loss: 1.01461306\n",
            "[>] epoch #46, batch #123: loss: 0.14459406 | val_loss: 1.00906561\n",
            "[>] epoch #46, batch #125: loss: 0.13797851 | val_loss: 1.03919876\n",
            "[>] epoch #46, batch #127: loss: 0.17398006 | val_loss: 1.00654554\n",
            "[>] epoch #46, batch #129: loss: 0.16173843 | val_loss: 1.05503165\n",
            "[>] epoch #46, batch #131: loss: 0.20652981 | val_loss: 1.04316841\n",
            "[>] epoch #46, batch #133: loss: 0.18501143 | val_loss: 1.01628261\n",
            "[>] epoch #46, batch #135: loss: 0.14669974 | val_loss: 1.00498654\n",
            "[>] epoch #46, batch #137: loss: 0.17254294 | val_loss: 1.03268352\n",
            "[>] epoch #46, batch #139: loss: 0.13399534 | val_loss: 1.02107617\n",
            "[>] epoch #46, batch #141: loss: 0.12689003 | val_loss: 1.05761714\n",
            "[>] epoch #47, batch #  2: loss: 0.14336394 | val_loss: 1.05099843\n",
            "[>] epoch #47, batch #  4: loss: 0.18280195 | val_loss: 1.03065646\n",
            "[>] epoch #47, batch #  6: loss: 0.15215799 | val_loss: 1.02477941\n",
            "[>] epoch #47, batch #  8: loss: 0.16834487 | val_loss: 1.02410316\n",
            "[>] epoch #47, batch # 10: loss: 0.13759124 | val_loss: 1.02519210\n",
            "[>] epoch #47, batch # 12: loss: 0.12236684 | val_loss: 1.05173950\n",
            "[>] epoch #47, batch # 14: loss: 0.16903116 | val_loss: 1.06049704\n",
            "[>] epoch #47, batch # 16: loss: 0.18759808 | val_loss: 1.06510835\n",
            "[>] epoch #47, batch # 18: loss: 0.18792762 | val_loss: 1.01626502\n",
            "[>] epoch #47, batch # 20: loss: 0.15113498 | val_loss: 1.01979147\n",
            "[>] epoch #47, batch # 22: loss: 0.12325620 | val_loss: 1.00384245\n",
            "[>] epoch #47, batch # 24: loss: 0.12499151 | val_loss: 1.00875500\n",
            "[>] epoch #47, batch # 26: loss: 0.13292062 | val_loss: 1.02381063\n",
            "[>] epoch #47, batch # 28: loss: 0.14124255 | val_loss: 1.01564703\n",
            "[>] epoch #47, batch # 30: loss: 0.16540742 | val_loss: 0.98047765\n",
            "[>] epoch #47, batch # 32: loss: 0.16436863 | val_loss: 1.03328179\n",
            "[>] epoch #47, batch # 34: loss: 0.07879884 | val_loss: 1.00801649\n",
            "[>] epoch #47, batch # 36: loss: 0.10213716 | val_loss: 1.04482925\n",
            "[>] epoch #47, batch # 38: loss: 0.09056488 | val_loss: 1.00937774\n",
            "[>] epoch #47, batch # 40: loss: 0.11758863 | val_loss: 0.99505045\n",
            "[>] epoch #47, batch # 42: loss: 0.15656978 | val_loss: 0.97373298\n",
            "[>] epoch #47, batch # 44: loss: 0.11452097 | val_loss: 0.98899057\n",
            "[>] epoch #47, batch # 46: loss: 0.12170716 | val_loss: 1.03891908\n",
            "[>] epoch #47, batch # 48: loss: 0.14831871 | val_loss: 1.02021030\n",
            "[>] epoch #47, batch # 50: loss: 0.16431944 | val_loss: 1.01966835\n",
            "[>] epoch #47, batch # 52: loss: 0.10464731 | val_loss: 0.99975963\n",
            "[>] epoch #47, batch # 54: loss: 0.06675924 | val_loss: 1.01498927\n",
            "[>] epoch #47, batch # 56: loss: 0.13859616 | val_loss: 1.04171666\n",
            "[>] epoch #47, batch # 58: loss: 0.14847006 | val_loss: 1.05014725\n",
            "[>] epoch #47, batch # 60: loss: 0.14758821 | val_loss: 1.01513722\n",
            "[>] epoch #47, batch # 62: loss: 0.12457078 | val_loss: 0.97846987\n",
            "[>] epoch #47, batch # 64: loss: 0.16412838 | val_loss: 1.01115082\n",
            "[>] epoch #47, batch # 66: loss: 0.17630447 | val_loss: 1.00215213\n",
            "[>] epoch #47, batch # 68: loss: 0.14998649 | val_loss: 1.04002969\n",
            "[>] epoch #47, batch # 70: loss: 0.11477713 | val_loss: 1.01817612\n",
            "[>] epoch #47, batch # 72: loss: 0.09069794 | val_loss: 1.04290529\n",
            "[>] epoch #47, batch # 74: loss: 0.13853590 | val_loss: 1.04833768\n",
            "[>] epoch #47, batch # 76: loss: 0.12849797 | val_loss: 1.02116162\n",
            "[>] epoch #47, batch # 78: loss: 0.11289947 | val_loss: 1.01199867\n",
            "[>] epoch #47, batch # 80: loss: 0.11077746 | val_loss: 1.05899011\n",
            "[>] epoch #47, batch # 82: loss: 0.15858833 | val_loss: 1.02604696\n",
            "[>] epoch #47, batch # 84: loss: 0.11984044 | val_loss: 0.99895322\n",
            "[>] epoch #47, batch # 86: loss: 0.13083704 | val_loss: 1.03309220\n",
            "[>] epoch #47, batch # 88: loss: 0.13178456 | val_loss: 1.04483500\n",
            "[>] epoch #47, batch # 90: loss: 0.17843628 | val_loss: 1.03851831\n",
            "[>] epoch #47, batch # 92: loss: 0.19429296 | val_loss: 0.98843841\n",
            "[>] epoch #47, batch # 94: loss: 0.14795174 | val_loss: 1.01144446\n",
            "[>] epoch #47, batch # 96: loss: 0.17621389 | val_loss: 1.02904078\n",
            "[>] epoch #47, batch # 98: loss: 0.14907876 | val_loss: 1.03977606\n",
            "[>] epoch #47, batch #100: loss: 0.17014495 | val_loss: 1.01499360\n",
            "[>] epoch #47, batch #102: loss: 0.15287337 | val_loss: 1.00802369\n",
            "[>] epoch #47, batch #104: loss: 0.17005715 | val_loss: 1.03959587\n",
            "[>] epoch #47, batch #106: loss: 0.13094996 | val_loss: 1.01252615\n",
            "[>] epoch #47, batch #108: loss: 0.16920957 | val_loss: 1.06221012\n",
            "[>] epoch #47, batch #110: loss: 0.16878733 | val_loss: 1.02044658\n",
            "[>] epoch #47, batch #112: loss: 0.21952799 | val_loss: 1.07701424\n",
            "[>] epoch #47, batch #114: loss: 0.18054026 | val_loss: 1.04129401\n",
            "[>] epoch #47, batch #116: loss: 0.16160671 | val_loss: 1.04814994\n",
            "[>] epoch #47, batch #118: loss: 0.12877834 | val_loss: 1.02896837\n",
            "[>] epoch #47, batch #120: loss: 0.12821017 | val_loss: 1.03601979\n",
            "[>] epoch #47, batch #122: loss: 0.16427048 | val_loss: 1.04130233\n",
            "[>] epoch #47, batch #124: loss: 0.12945995 | val_loss: 1.04175964\n",
            "[>] epoch #47, batch #126: loss: 0.10458021 | val_loss: 1.01337848\n",
            "[>] epoch #47, batch #128: loss: 0.11675683 | val_loss: 1.03915004\n",
            "[>] epoch #47, batch #130: loss: 0.16159312 | val_loss: 1.05398207\n",
            "[>] epoch #47, batch #132: loss: 0.23081559 | val_loss: 1.03137657\n",
            "[>] epoch #47, batch #134: loss: 0.10034629 | val_loss: 1.02411215\n",
            "[>] epoch #47, batch #136: loss: 0.16875921 | val_loss: 1.01956897\n",
            "[>] epoch #47, batch #138: loss: 0.12419322 | val_loss: 1.00034649\n",
            "[>] epoch #47, batch #140: loss: 0.11580434 | val_loss: 0.98812071\n",
            "[>] epoch #48, batch #  1: loss: 0.13267188 | val_loss: 0.99191223\n",
            "[>] epoch #48, batch #  3: loss: 0.17203458 | val_loss: 1.03577352\n",
            "[>] epoch #48, batch #  5: loss: 0.17382631 | val_loss: 1.03726608\n",
            "[>] epoch #48, batch #  7: loss: 0.19730316 | val_loss: 1.01399259\n",
            "[>] epoch #48, batch #  9: loss: 0.15314038 | val_loss: 1.05460961\n",
            "[>] epoch #48, batch # 11: loss: 0.11907347 | val_loss: 0.99760157\n",
            "[>] epoch #48, batch # 13: loss: 0.08226299 | val_loss: 1.00655652\n",
            "[>] epoch #48, batch # 15: loss: 0.09789943 | val_loss: 1.02897310\n",
            "[>] epoch #48, batch # 17: loss: 0.16382845 | val_loss: 1.05563022\n",
            "[>] epoch #48, batch # 19: loss: 0.11355989 | val_loss: 1.04064576\n",
            "[>] epoch #48, batch # 21: loss: 0.14202268 | val_loss: 1.00541930\n",
            "[>] epoch #48, batch # 23: loss: 0.12861900 | val_loss: 1.02582676\n",
            "[>] epoch #48, batch # 25: loss: 0.13351715 | val_loss: 1.02910305\n",
            "[>] epoch #48, batch # 27: loss: 0.11241435 | val_loss: 1.04910580\n",
            "[>] epoch #48, batch # 29: loss: 0.12463770 | val_loss: 1.01796715\n",
            "[>] epoch #48, batch # 31: loss: 0.12407570 | val_loss: 1.03404328\n",
            "[>] epoch #48, batch # 33: loss: 0.13747117 | val_loss: 1.02442301\n",
            "[>] epoch #48, batch # 35: loss: 0.12423849 | val_loss: 1.03539874\n",
            "[>] epoch #48, batch # 37: loss: 0.18381363 | val_loss: 1.02920792\n",
            "[>] epoch #48, batch # 39: loss: 0.16468284 | val_loss: 0.99669166\n",
            "[>] epoch #48, batch # 41: loss: 0.11974005 | val_loss: 1.06148368\n",
            "[>] epoch #48, batch # 43: loss: 0.11616901 | val_loss: 1.01898269\n",
            "[>] epoch #48, batch # 45: loss: 0.09677024 | val_loss: 1.05152506\n",
            "[>] epoch #48, batch # 47: loss: 0.20760892 | val_loss: 1.01167030\n",
            "[>] epoch #48, batch # 49: loss: 0.14670478 | val_loss: 1.01741736\n",
            "[>] epoch #48, batch # 51: loss: 0.13419443 | val_loss: 1.06278231\n",
            "[>] epoch #48, batch # 53: loss: 0.13529962 | val_loss: 1.04654413\n",
            "[>] epoch #48, batch # 55: loss: 0.14026020 | val_loss: 1.02143360\n",
            "[>] epoch #48, batch # 57: loss: 0.08747005 | val_loss: 0.99490020\n",
            "[>] epoch #48, batch # 59: loss: 0.12605280 | val_loss: 1.00102007\n",
            "[>] epoch #48, batch # 61: loss: 0.11424234 | val_loss: 0.98821454\n",
            "[>] epoch #48, batch # 63: loss: 0.11105937 | val_loss: 1.02447697\n",
            "[>] epoch #48, batch # 65: loss: 0.20466150 | val_loss: 1.02228355\n",
            "[>] epoch #48, batch # 67: loss: 0.09699790 | val_loss: 1.03731475\n",
            "[>] epoch #48, batch # 69: loss: 0.15306874 | val_loss: 1.01627268\n",
            "[>] epoch #48, batch # 71: loss: 0.19557446 | val_loss: 1.04930281\n",
            "[>] epoch #48, batch # 73: loss: 0.18024257 | val_loss: 1.00945613\n",
            "[>] epoch #48, batch # 75: loss: 0.18251157 | val_loss: 1.01019073\n",
            "[>] epoch #48, batch # 77: loss: 0.09686182 | val_loss: 1.01728755\n",
            "[>] epoch #48, batch # 79: loss: 0.14984092 | val_loss: 1.03606412\n",
            "[>] epoch #48, batch # 81: loss: 0.14464657 | val_loss: 1.07337510\n",
            "[>] epoch #48, batch # 83: loss: 0.19069847 | val_loss: 1.05289397\n",
            "[>] epoch #48, batch # 85: loss: 0.18214987 | val_loss: 1.07338454\n",
            "[>] epoch #48, batch # 87: loss: 0.13146453 | val_loss: 1.06191357\n",
            "[>] epoch #48, batch # 89: loss: 0.10507044 | val_loss: 1.02032656\n",
            "[>] epoch #48, batch # 91: loss: 0.13128243 | val_loss: 1.03584907\n",
            "[>] epoch #48, batch # 93: loss: 0.22654018 | val_loss: 1.02604367\n",
            "[>] epoch #48, batch # 95: loss: 0.14750865 | val_loss: 1.05614018\n",
            "[>] epoch #48, batch # 97: loss: 0.24150926 | val_loss: 1.06633608\n",
            "[>] epoch #48, batch # 99: loss: 0.13310480 | val_loss: 1.01624429\n",
            "[>] epoch #48, batch #101: loss: 0.17059804 | val_loss: 1.04169044\n",
            "[>] epoch #48, batch #103: loss: 0.22293283 | val_loss: 1.00733599\n",
            "[>] epoch #48, batch #105: loss: 0.15450694 | val_loss: 1.03727159\n",
            "[>] epoch #48, batch #107: loss: 0.15435134 | val_loss: 1.04820074\n",
            "[>] epoch #48, batch #109: loss: 0.17115024 | val_loss: 0.97281264\n",
            "[>] epoch #48, batch #111: loss: 0.14031506 | val_loss: 0.99197216\n",
            "[>] epoch #48, batch #113: loss: 0.12058405 | val_loss: 1.05584147\n",
            "[>] epoch #48, batch #115: loss: 0.15157849 | val_loss: 1.05623198\n",
            "[>] epoch #48, batch #117: loss: 0.21400501 | val_loss: 1.06370744\n",
            "[>] epoch #48, batch #119: loss: 0.18355593 | val_loss: 1.06217883\n",
            "[>] epoch #48, batch #121: loss: 0.13929565 | val_loss: 1.00563411\n",
            "[>] epoch #48, batch #123: loss: 0.15786801 | val_loss: 1.06725088\n",
            "[>] epoch #48, batch #125: loss: 0.13829528 | val_loss: 0.99422459\n",
            "[>] epoch #48, batch #127: loss: 0.15508616 | val_loss: 0.98234357\n",
            "[>] epoch #48, batch #129: loss: 0.12981658 | val_loss: 1.05369672\n",
            "[>] epoch #48, batch #131: loss: 0.16068913 | val_loss: 1.04323403\n",
            "[>] epoch #48, batch #133: loss: 0.13953251 | val_loss: 1.05930632\n",
            "[>] epoch #48, batch #135: loss: 0.18019730 | val_loss: 1.04267793\n",
            "[>] epoch #48, batch #137: loss: 0.17793171 | val_loss: 1.05155722\n",
            "[>] epoch #48, batch #139: loss: 0.10795096 | val_loss: 1.07612548\n",
            "[>] epoch #48, batch #141: loss: 0.26419145 | val_loss: 1.03207269\n",
            "[>] epoch #49, batch #  2: loss: 0.10280810 | val_loss: 1.04691635\n",
            "[>] epoch #49, batch #  4: loss: 0.12903538 | val_loss: 0.99533945\n",
            "[>] epoch #49, batch #  6: loss: 0.11917081 | val_loss: 1.03188489\n",
            "[>] epoch #49, batch #  8: loss: 0.12984666 | val_loss: 1.00466805\n",
            "[>] epoch #49, batch # 10: loss: 0.10764301 | val_loss: 1.04654343\n",
            "[>] epoch #49, batch # 12: loss: 0.13966335 | val_loss: 1.06485549\n",
            "[>] epoch #49, batch # 14: loss: 0.13838285 | val_loss: 1.04145449\n",
            "[>] epoch #49, batch # 16: loss: 0.20490488 | val_loss: 1.08216101\n",
            "[>] epoch #49, batch # 18: loss: 0.10362459 | val_loss: 1.10173418\n",
            "[>] epoch #49, batch # 20: loss: 0.16586570 | val_loss: 1.02388273\n",
            "[>] epoch #49, batch # 22: loss: 0.18656297 | val_loss: 1.00322108\n",
            "[>] epoch #49, batch # 24: loss: 0.09903105 | val_loss: 1.07682671\n",
            "[>] epoch #49, batch # 26: loss: 0.13831636 | val_loss: 1.00598602\n",
            "[>] epoch #49, batch # 28: loss: 0.07630356 | val_loss: 1.03408303\n",
            "[>] epoch #49, batch # 30: loss: 0.13377073 | val_loss: 1.02072786\n",
            "[>] epoch #49, batch # 32: loss: 0.19773629 | val_loss: 1.03387266\n",
            "[>] epoch #49, batch # 34: loss: 0.14447810 | val_loss: 1.03828785\n",
            "[>] epoch #49, batch # 36: loss: 0.12754567 | val_loss: 1.04115707\n",
            "[>] epoch #49, batch # 38: loss: 0.16659558 | val_loss: 1.03101058\n",
            "[>] epoch #49, batch # 40: loss: 0.11188535 | val_loss: 1.03064922\n",
            "[>] epoch #49, batch # 42: loss: 0.13527776 | val_loss: 1.01803833\n",
            "[>] epoch #49, batch # 44: loss: 0.15244724 | val_loss: 1.03360553\n",
            "[>] epoch #49, batch # 46: loss: 0.10153350 | val_loss: 1.03834894\n",
            "[>] epoch #49, batch # 48: loss: 0.08910826 | val_loss: 1.02618061\n",
            "[>] epoch #49, batch # 50: loss: 0.12035025 | val_loss: 1.04468166\n",
            "[>] epoch #49, batch # 52: loss: 0.15176797 | val_loss: 1.02347545\n",
            "[>] epoch #49, batch # 54: loss: 0.14155208 | val_loss: 1.04237467\n",
            "[>] epoch #49, batch # 56: loss: 0.15892169 | val_loss: 1.00247875\n",
            "[>] epoch #49, batch # 58: loss: 0.14353913 | val_loss: 1.00544589\n",
            "[>] epoch #49, batch # 60: loss: 0.08709726 | val_loss: 1.01111222\n",
            "[>] epoch #49, batch # 62: loss: 0.12115080 | val_loss: 0.99004544\n",
            "[>] epoch #49, batch # 64: loss: 0.10882119 | val_loss: 0.98156262\n",
            "[>] epoch #49, batch # 66: loss: 0.17763732 | val_loss: 1.04027619\n",
            "[>] epoch #49, batch # 68: loss: 0.12466828 | val_loss: 1.02586613\n",
            "[>] epoch #49, batch # 70: loss: 0.14808840 | val_loss: 1.05187019\n",
            "[>] epoch #49, batch # 72: loss: 0.15742546 | val_loss: 1.06628694\n",
            "[>] epoch #49, batch # 74: loss: 0.22679842 | val_loss: 1.01879449\n",
            "[>] epoch #49, batch # 76: loss: 0.20054571 | val_loss: 1.05828850\n",
            "[>] epoch #49, batch # 78: loss: 0.12304796 | val_loss: 1.01776289\n",
            "[>] epoch #49, batch # 80: loss: 0.16215369 | val_loss: 1.00088207\n",
            "[>] epoch #49, batch # 82: loss: 0.11400715 | val_loss: 1.05441586\n",
            "[>] epoch #49, batch # 84: loss: 0.08525433 | val_loss: 1.02921139\n",
            "[>] epoch #49, batch # 86: loss: 0.18749160 | val_loss: 1.03413978\n",
            "[>] epoch #49, batch # 88: loss: 0.16704242 | val_loss: 1.03138425\n",
            "[>] epoch #49, batch # 90: loss: 0.20448247 | val_loss: 1.01720408\n",
            "[>] epoch #49, batch # 92: loss: 0.08112151 | val_loss: 1.01504078\n",
            "[>] epoch #49, batch # 94: loss: 0.19482398 | val_loss: 1.01927633\n",
            "[>] epoch #49, batch # 96: loss: 0.16763720 | val_loss: 1.03350946\n",
            "[>] epoch #49, batch # 98: loss: 0.09389184 | val_loss: 1.03650508\n",
            "[>] epoch #49, batch #100: loss: 0.15047868 | val_loss: 1.04677085\n",
            "[>] epoch #49, batch #102: loss: 0.13997424 | val_loss: 1.02498513\n",
            "[>] epoch #49, batch #104: loss: 0.12552747 | val_loss: 1.04501585\n",
            "[>] epoch #49, batch #106: loss: 0.13742715 | val_loss: 1.02584817\n",
            "[>] epoch #49, batch #108: loss: 0.09584847 | val_loss: 1.05315551\n",
            "[>] epoch #49, batch #110: loss: 0.13324164 | val_loss: 1.00355461\n",
            "[>] epoch #49, batch #112: loss: 0.22133568 | val_loss: 1.05032397\n",
            "[>] epoch #49, batch #114: loss: 0.14476593 | val_loss: 1.01808828\n",
            "[>] epoch #49, batch #116: loss: 0.20375155 | val_loss: 1.06862856\n",
            "[>] epoch #49, batch #118: loss: 0.11948698 | val_loss: 1.07641788\n",
            "[>] epoch #49, batch #120: loss: 0.19615349 | val_loss: 1.06947221\n",
            "[>] epoch #49, batch #122: loss: 0.22535719 | val_loss: 1.04972135\n",
            "[>] epoch #49, batch #124: loss: 0.14012852 | val_loss: 1.02839712\n",
            "[>] epoch #49, batch #126: loss: 0.11485118 | val_loss: 1.02830973\n",
            "[>] epoch #49, batch #128: loss: 0.17864224 | val_loss: 1.00163272\n",
            "[>] epoch #49, batch #130: loss: 0.12386105 | val_loss: 1.04529092\n",
            "[>] epoch #49, batch #132: loss: 0.19252652 | val_loss: 1.01837461\n",
            "[>] epoch #49, batch #134: loss: 0.15395395 | val_loss: 1.01542360\n",
            "[>] epoch #49, batch #136: loss: 0.14566545 | val_loss: 1.01507967\n",
            "[>] epoch #49, batch #138: loss: 0.15289092 | val_loss: 1.04459201\n",
            "[>] epoch #49, batch #140: loss: 0.13516217 | val_loss: 1.03282054\n",
            "[>] epoch #50, batch #  1: loss: 0.18344523 | val_loss: 1.03397981\n",
            "[>] epoch #50, batch #  3: loss: 0.14365023 | val_loss: 1.06740773\n",
            "[>] epoch #50, batch #  5: loss: 0.12637246 | val_loss: 1.01963206\n",
            "[>] epoch #50, batch #  7: loss: 0.11412944 | val_loss: 1.06563178\n",
            "[>] epoch #50, batch #  9: loss: 0.15939957 | val_loss: 1.07060299\n",
            "[>] epoch #50, batch # 11: loss: 0.10990236 | val_loss: 1.03781914\n",
            "[>] epoch #50, batch # 13: loss: 0.16457671 | val_loss: 1.05787563\n",
            "[>] epoch #50, batch # 15: loss: 0.14258879 | val_loss: 1.06371367\n",
            "[>] epoch #50, batch # 17: loss: 0.12851520 | val_loss: 1.05384765\n",
            "[>] epoch #50, batch # 19: loss: 0.08025610 | val_loss: 1.07239635\n",
            "[>] epoch #50, batch # 21: loss: 0.14680120 | val_loss: 1.05869419\n",
            "[>] epoch #50, batch # 23: loss: 0.17702518 | val_loss: 1.02641480\n",
            "[>] epoch #50, batch # 25: loss: 0.17618813 | val_loss: 1.05029232\n",
            "[>] epoch #50, batch # 27: loss: 0.10757986 | val_loss: 1.04447464\n",
            "[>] epoch #50, batch # 29: loss: 0.10815537 | val_loss: 1.04909657\n",
            "[>] epoch #50, batch # 31: loss: 0.16284776 | val_loss: 1.03169417\n",
            "[>] epoch #50, batch # 33: loss: 0.13132893 | val_loss: 1.05708250\n",
            "[>] epoch #50, batch # 35: loss: 0.13419937 | val_loss: 1.07667434\n",
            "[>] epoch #50, batch # 37: loss: 0.22072949 | val_loss: 1.06388044\n",
            "[>] epoch #50, batch # 39: loss: 0.14952679 | val_loss: 1.02360383\n",
            "[>] epoch #50, batch # 41: loss: 0.07501986 | val_loss: 1.05186773\n",
            "[>] epoch #50, batch # 43: loss: 0.06639907 | val_loss: 1.01528396\n",
            "[>] epoch #50, batch # 45: loss: 0.11988378 | val_loss: 1.01205625\n",
            "[>] epoch #50, batch # 47: loss: 0.12601458 | val_loss: 1.04745990\n",
            "[>] epoch #50, batch # 49: loss: 0.14208426 | val_loss: 1.07288630\n",
            "[>] epoch #50, batch # 51: loss: 0.14113867 | val_loss: 1.03663482\n",
            "[>] epoch #50, batch # 53: loss: 0.13908809 | val_loss: 1.03854282\n",
            "[>] epoch #50, batch # 55: loss: 0.12001335 | val_loss: 1.05759265\n",
            "[>] epoch #50, batch # 57: loss: 0.12647972 | val_loss: 1.01572685\n",
            "[>] epoch #50, batch # 59: loss: 0.12038634 | val_loss: 1.02236927\n",
            "[>] epoch #50, batch # 61: loss: 0.14359301 | val_loss: 1.00906726\n",
            "[>] epoch #50, batch # 63: loss: 0.18335089 | val_loss: 1.01474349\n",
            "[>] epoch #50, batch # 65: loss: 0.17647724 | val_loss: 1.00564807\n",
            "[>] epoch #50, batch # 67: loss: 0.12978202 | val_loss: 1.05862742\n",
            "[>] epoch #50, batch # 69: loss: 0.11895508 | val_loss: 1.05426361\n",
            "[>] epoch #50, batch # 71: loss: 0.10718689 | val_loss: 1.05007955\n",
            "[>] epoch #50, batch # 73: loss: 0.15530434 | val_loss: 1.05002402\n",
            "[>] epoch #50, batch # 75: loss: 0.11719408 | val_loss: 1.04430764\n",
            "[>] epoch #50, batch # 77: loss: 0.14321941 | val_loss: 1.02785259\n",
            "[>] epoch #50, batch # 79: loss: 0.17611165 | val_loss: 1.01691235\n",
            "[>] epoch #50, batch # 81: loss: 0.14030564 | val_loss: 1.04218145\n",
            "[>] epoch #50, batch # 83: loss: 0.11966328 | val_loss: 1.00093515\n",
            "[>] epoch #50, batch # 85: loss: 0.13996495 | val_loss: 1.02460401\n",
            "[>] epoch #50, batch # 87: loss: 0.14356914 | val_loss: 1.01080971\n",
            "[>] epoch #50, batch # 89: loss: 0.13143893 | val_loss: 1.02518265\n",
            "[>] epoch #50, batch # 91: loss: 0.10295387 | val_loss: 1.00456473\n",
            "[>] epoch #50, batch # 93: loss: 0.10080725 | val_loss: 1.02022844\n",
            "[>] epoch #50, batch # 95: loss: 0.16907343 | val_loss: 1.02739187\n",
            "[>] epoch #50, batch # 97: loss: 0.14982563 | val_loss: 1.04089354\n",
            "[>] epoch #50, batch # 99: loss: 0.17656097 | val_loss: 1.04253674\n",
            "[>] epoch #50, batch #101: loss: 0.21184380 | val_loss: 1.01768895\n",
            "[>] epoch #50, batch #103: loss: 0.20175646 | val_loss: 1.03851157\n",
            "[>] epoch #50, batch #105: loss: 0.13974261 | val_loss: 1.00587374\n",
            "[>] epoch #50, batch #107: loss: 0.11457001 | val_loss: 1.01232771\n",
            "[>] epoch #50, batch #109: loss: 0.08300289 | val_loss: 1.05300859\n",
            "[>] epoch #50, batch #111: loss: 0.10232111 | val_loss: 1.02209921\n",
            "[>] epoch #50, batch #113: loss: 0.13580336 | val_loss: 1.06403236\n",
            "[>] epoch #50, batch #115: loss: 0.14646235 | val_loss: 1.03831205\n",
            "[>] epoch #50, batch #117: loss: 0.14703543 | val_loss: 1.03323636\n",
            "[>] epoch #50, batch #119: loss: 0.15349142 | val_loss: 1.01607770\n",
            "[>] epoch #50, batch #121: loss: 0.19758774 | val_loss: 0.99656785\n",
            "[>] epoch #50, batch #123: loss: 0.14026454 | val_loss: 1.03508665\n",
            "[>] epoch #50, batch #125: loss: 0.21634154 | val_loss: 1.04678095\n",
            "[>] epoch #50, batch #127: loss: 0.08644223 | val_loss: 1.06392609\n",
            "[>] epoch #50, batch #129: loss: 0.13502650 | val_loss: 1.04447793\n",
            "[>] epoch #50, batch #131: loss: 0.16530070 | val_loss: 0.99088411\n",
            "[>] epoch #50, batch #133: loss: 0.14101635 | val_loss: 1.04804853\n",
            "[>] epoch #50, batch #135: loss: 0.12121058 | val_loss: 1.04149011\n",
            "[>] epoch #50, batch #137: loss: 0.12473509 | val_loss: 1.01003674\n",
            "[>] epoch #50, batch #139: loss: 0.15567057 | val_loss: 1.03612825\n",
            "[>] epoch #50, batch #141: loss: 0.21089929 | val_loss: 1.05003190\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "train_dataset      = TokenizerDataset(train_data     , src_tokenizer, tgt_tokenizer, **rnn_enc_dec_data_params)\n",
        "validation_dataset = TokenizerDataset(validation_data, src_tokenizer, tgt_tokenizer, **rnn_enc_dec_data_params)\n",
        "\n",
        "rnn_enc_dec_train_data = dict(\n",
        "    train_dataset=train_dataset,\n",
        "    validation_dataset=validation_dataset,\n",
        "    collate_fn=train_dataset.collate\n",
        ")\n",
        "\n",
        "# Resume training from the last checkpoint, if interrupted midway, else begins training from scratch.\n",
        "trainer.resume()\n",
        "\n",
        "# Train as per specified training parameters.\n",
        "trainer.train(**rnn_enc_dec_train_data, **rnn_enc_dec_training_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GtcTRlgs7S1M"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "# Save the final model, with additional metadata.\n",
        "trainer.save(metadata={\n",
        "    'model'   : rnn_enc_dec_params,\n",
        "    'data'    : rnn_enc_dec_data_params,\n",
        "    'training': rnn_enc_dec_training_params\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUshh3Uv7S1N"
      },
      "source": [
        "To validate training, look at sample translations for different examples, and probabilities assigned to different outputs.\n",
        "\n",
        "Extensive evaluation and comparison against other approaches will be carried out later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "USVATGO17S1O"
      },
      "outputs": [],
      "source": [
        "def rnn_greedy_generate(model, seq_x, src_tokenizer, tgt_tokenizer, max_length):\n",
        "    \"\"\" Given a source string, translate it to the target language using the trained model.\n",
        "        This function should perform greedy sampling to generate the results.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): RNN Type Encoder-Decoder Model\n",
        "        seq_x (str): Input string to translate.\n",
        "        src_tokenizer (Tokenizer): Source language tokenizer.\n",
        "        tgt_tokenizer (Tokenizer): Target language tokenizer.\n",
        "        max_length (int): Maximum length of the target sequence to decode.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated string for the given input in the target language.\n",
        "    \"\"\"\n",
        "\n",
        "    # BEGIN CODE : enc-dec-rnn.greedy_generate\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    seq_x = src_tokenizer.encode(seq_x)\n",
        "    seq_x = torch.tensor(seq_x).unsqueeze(0).to(device)  \n",
        "    \n",
        "    decoder_input = torch.tensor([[tgt_tokenizer.get_special_tokens()['[BOS]']]]).to(device)\n",
        "    \n",
        "    output_tokens = [tgt_tokenizer.get_special_tokens()['[BOS]']]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        log_probs, decoder_hidden = model(seq_x, decoder_input)\n",
        "        \n",
        "        for _ in range(max_length - 1):\n",
        "            top_token = log_probs.argmax(dim=-1).squeeze()\n",
        "            \n",
        "            output_tokens.append(top_token.item())\n",
        "            \n",
        "            if top_token.item() == tgt_tokenizer.get_special_tokens()['[EOS]']:\n",
        "                break\n",
        "            \n",
        "            decoder_input = top_token.unsqueeze(0).unsqueeze(0).to(device)\n",
        "            \n",
        "            log_probs, decoder_hidden = model(seq_x, decoder_input, decoder_hidden)\n",
        "    \n",
        "    return tgt_tokenizer.decode(output_tokens)\n",
        "\n",
        "    # END CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tz4Ud06A7S1O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name                      : वरिषा\n",
            "Translation (Expected)    : varisha\n",
            "Translation (Model)       : vishshakha\n",
            "\n",
            "Name                      : उजामा\n",
            "Translation (Expected)    : ujama\n",
            "Translation (Model)       : ujamauje\n",
            "\n",
            "Name                      : बिपिन\n",
            "Translation (Expected)    : bipin\n",
            "Translation (Model)       : bipinandtan\n",
            "\n",
            "Name                      : मित्ठू\n",
            "Translation (Expected)    : mitthu\n",
            "Translation (Model)       : mitthun\n",
            "\n",
            "Name                      : रामैया\n",
            "Translation (Expected)    : ramaiya\n",
            "Translation (Model)       : raamihibai\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "for _, row in train_data.sample(n=5, random_state=42).iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
        "    )\n",
        "\n",
        "    print(\"Name                      :\", row['Name'])\n",
        "    print(\"Translation (Expected)    :\", row['Translation'])\n",
        "    print(\"Translation (Model)       :\", y_pred)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FvEL5aYR7S1O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name                      : दीन\n",
            "Translation (Expected)    : deen\n",
            "Translation (Model)       : dinderinderandir\n",
            "\n",
            "Name                      : मुर्शिदा\n",
            "Translation (Expected)    : murshida\n",
            "Translation (Model)       : mursidash\n",
            "\n",
            "Name                      : शबरा\n",
            "Translation (Expected)    : shabra\n",
            "Translation (Model)       : shaarashbarabana\n",
            "\n",
            "Name                      : श्रीकांत\n",
            "Translation (Expected)    : shrikant\n",
            "Translation (Model)       : srikantth\n",
            "\n",
            "Name                      : कौशल\n",
            "Translation (Expected)    : kaushal\n",
            "Translation (Model)       : kululul\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "for _, row in validation_data.sample(n=5, random_state=42).iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
        "    )\n",
        "\n",
        "    print(\"Name                      :\", row['Name'])\n",
        "    print(\"Translation (Expected)    :\", row['Translation'])\n",
        "    print(\"Translation (Model)       :\", y_pred)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GaoL9IUrsaFU"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "output_data = []\n",
        "for _, row in validation_data.iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
        "    )\n",
        "    output_data.append({ 'Name': row['Name'], 'Translation': y_pred })\n",
        "\n",
        "pd.DataFrame.from_records(output_data).to_csv(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\", \"outputs.csv\"), index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-XRDFy1U7S1P"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "# Release resources\n",
        "if 'trainer' in globals():\n",
        "    del trainer\n",
        "\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "\n",
        "sync_vram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkAB3ivx7S1Q"
      },
      "source": [
        "## Seq-2-Seq Modeling with RNN + Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpFw0Fn7S1Q"
      },
      "source": [
        "In this module, you'll augment the Encoder-Decoder architecture to utilize attention, by implementing an Attention module that attends over the representations / inputs from the encoder.\n",
        "\n",
        "Many approaches have been proposed in literature towards implementing attention. You are free to explore and use any implementation of your choice.\n",
        "\n",
        "Some popular approaches are desribed in the original [paper by Bahdanau et al., 2014 on NMT](https://arxiv.org/abs/1409.0473) and an [exploratory paper by Luong et al, 2015](https://arxiv.org/abs/1508.04025) which explores different effective approaches to attention, including global and local attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2wmAoC77S1R"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class AttentionModule(torch.nn.Module):\n",
        "    \"\"\" Implements an attention module \"\"\"\n",
        "\n",
        "    # Feel free to add additional parameters to __init__\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\" Initializes the attention module.\n",
        "            Feel free to declare any parameters as required. \"\"\"\n",
        "\n",
        "        super(AttentionModule, self).__init__()\n",
        "\n",
        "        # BEGIN CODE : attn.init\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden_state):\n",
        "        \"\"\" Performs a forward pass over the module, computing attention scores for inputs.\n",
        "\n",
        "        Args:\n",
        "            encoder_outputs (torch.Tensor): Output representations from the encoder, of shape [batch_size?, src_seq_len, output_dim].\n",
        "            decoder_hidden_state (torch.Tensor): Hidden state from the decoder at current time step, of appropriate shape as per RNN unit (with optional batch dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attentions scores for given inputs, of shape [batch_size?, 1, src_seq_len]\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : attn.forward\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q73WPlp7S1R"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class RNNEncoderDecoderLMWithAttention(torch.nn.Module):\n",
        "    \"\"\" Implements an Encoder-Decoder network, using RNN units, augmented with attention. \"\"\"\n",
        "\n",
        "    # Feel free to add additional parameters to __init__\n",
        "    def __init__(self,src_vocab_size, tgt_vocab_size, embd_dims, hidden_size, num_layers=1, dropout=0.1):\n",
        "        \"\"\" Initializes the encoder-decoder network, implemented via RNNs.\n",
        "\n",
        "        Args:\n",
        "            src_vocab_size (int): Source vocabulary size.\n",
        "            tgt_vocab_size (int): Target vocabulary size.\n",
        "            embd_dims (int): Embedding dimensions.\n",
        "            hidden_size (int): Size/Dimensions for the hidden states.\n",
        "        \"\"\"\n",
        "\n",
        "        super(RNNEncoderDecoderLMWithAttention, self).__init__()\n",
        "\n",
        "        # Dummy parameter to track the model device. Do not modify.\n",
        "        self._dummy_param = torch.nn.Parameter(torch.Tensor(0), requires_grad=False)\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn-attn.init\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self._dummy_param.device\n",
        "\n",
        "    def log_probability(self, seq_x, seq_y):\n",
        "        \"\"\" Compute the conditional log probability of seq_y given seq_x, i.e., log P(seq_y | seq_x).\n",
        "\n",
        "        Args:\n",
        "            seq_x (torch.tensor): Input sequence of tokens, of shape [src_seq_len] (no batch dim)\n",
        "            seq_y (torch.tensor): Output sequence of tokens, of shape [tgt_seq_len] (no batch dim)\n",
        "\n",
        "        Returns:\n",
        "            float: Log probability of generating sequence y, given sequence x.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn-attn.probability\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def attentions(self, seq_x, terminate_token, max_length):\n",
        "        \"\"\" Obtain attention over a sequence for decoding to the target language.\n",
        "\n",
        "        Args:\n",
        "            seq_x (torch.tensor): Tensor representing the source sequence, of shape [src_seq_len] (no batch dim)\n",
        "            terminate_token (int): Token to use as EOS, to stop generating outputs.\n",
        "            max_length (int): Maximum length to use to terminate the sampling.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.tensor, torch.tensor]:\n",
        "                A tuple of two tensors: the attentions over individual output tokens ([tgt_seq_len, src_seq_len])\n",
        "                and the best output tokens ([tgt_seq_len]) per sequence step, based on greedy sampling.\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : rnn-enc-dec-attn.attentions\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def forward(self, inputs, decoder_inputs=None, decoder_hidden_state=None, output_attention=False):\n",
        "        \"\"\" Performs a forward pass over the encoder-decoder network.\n",
        "\n",
        "            Accepts inputs for the encoder, inputs for the decoder, and hidden state for\n",
        "                the decoder to continue generation after the given input.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): tensor of shape [batch_size?, src_seq_len]\n",
        "            decoder_inputs (torch.Tensor): Decoder inputs, as tensor of shape [batch_size?, 1]\n",
        "            decoder_hidden_state (any): tensor to represent decoder hidden state from time step T-1.\n",
        "            output_attention (bool): If true, this function should also return the\n",
        "                associated attention weights for the time step, of shape [batch_size?, 1, src_seq_len].\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, any]: output from the decoder, and associated hidden state for the next step.\n",
        "\n",
        "            Decoder outputs should be log probabilities over the target vocabulary.\n",
        "\n",
        "        Example:\n",
        "        >>> model = RNNEncoderDecoderWithAttention(*args, **kwargs)\n",
        "        >>> output, hidden = model(..., output_attention=False)\n",
        "        >>> output, hidden, attn_weights = model(..., output_attention=True)\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : enc-dec-rnn-attn.forward\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuLUZV167S1S"
      },
      "outputs": [],
      "source": [
        "## == BEGIN EVALUATION PORTION\n",
        "\n",
        "# Edit the hyperparameters below to your desired values.\n",
        "\n",
        "# BEGIN CODE : rnn-enc-dec-attn.params\n",
        "\n",
        "# Add parameters related to the model here.\n",
        "rnn_enc_dec_attn_params = {\n",
        "    'src_vocab_size': None,\n",
        "    'tgt_vocab_size': None,\n",
        "    'embd_dims'     : None,\n",
        "    'hidden_size'   : None,\n",
        "    'dropout'       : None,\n",
        "    'num_layers'    : None\n",
        "}\n",
        "\n",
        "# Add parameters related to the dataset processing here.\n",
        "rnn_enc_dec_attn_data_params = dict(\n",
        "    src_padding=None,\n",
        "    tgt_padding=None,\n",
        ")\n",
        "\n",
        "# Add parameters related to training here.\n",
        "rnn_enc_dec_attn_training_params = dict(\n",
        "    num_epochs=None,\n",
        "    batch_size=None,\n",
        "    shuffle=None,\n",
        "    save_steps=None,\n",
        "    eval_steps=None\n",
        ")\n",
        "\n",
        "# END CODE\n",
        "\n",
        "# Do not forget to set a deterministic seed.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = RNNEncoderDecoderLMWithAttention(**rnn_enc_dec_attn_params)\n",
        "\n",
        "# BEGIN CODE : rnn-enc-dec-attn.train\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "optimizer = None\n",
        "criterion = None\n",
        "\n",
        "# END CODE\n",
        "\n",
        "trainer = RNNEncoderDecoderTrainer(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\"),\n",
        "    model, criterion, optimizer\n",
        ")\n",
        "\n",
        "## == END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_oypGhq7S1S"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "train_dataset      = TokenizerDataset(train_data     , src_tokenizer, tgt_tokenizer, **rnn_enc_dec_attn_data_params)\n",
        "validation_dataset = TokenizerDataset(validation_data, src_tokenizer, tgt_tokenizer, **rnn_enc_dec_attn_data_params)\n",
        "\n",
        "rnn_enc_dec_attn_train_data = dict(\n",
        "    train_dataset=train_dataset,\n",
        "    validation_dataset=validation_dataset,\n",
        "    collate_fn=train_dataset.collate\n",
        ")\n",
        "\n",
        "# Resume training from the last checkpoint, if interrupted midway, otherwise starts from scratch.\n",
        "trainer.resume()\n",
        "\n",
        "# Train as per specified training parameters.\n",
        "trainer.train(**rnn_enc_dec_attn_train_data, **rnn_enc_dec_attn_training_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAYwB5i47S1T"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "# Save the final model, with additional metadata.\n",
        "trainer.save(metadata={\n",
        "    'model'   : rnn_enc_dec_attn_params,\n",
        "    'data'    : rnn_enc_dec_attn_data_params,\n",
        "    'training': rnn_enc_dec_attn_training_params\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIb8Qkg7S1T"
      },
      "source": [
        "We can validate the model using a few simple tests as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1RKp9rM7S1T"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "for _, row in train_data.sample(n=5, random_state=42).iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_attn_data_params['tgt_padding']\n",
        "    )\n",
        "\n",
        "    print(\"Name                      :\", row['Name'])\n",
        "    print(\"Translation (Expected)    :\", row['Translation'])\n",
        "    print(\"Translation (Model)       :\", y_pred)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt6MLKYX7S1U"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "for _, row in validation_data.sample(n=5, random_state=42).iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_attn_data_params['tgt_padding']\n",
        "    )\n",
        "\n",
        "    print(\"Name                      :\", row['Name'])\n",
        "    print(\"Translation (Expected)    :\", row['Translation'])\n",
        "    print(\"Translation (Model)       :\", y_pred)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8BHmGBR7S1U"
      },
      "source": [
        "It may also be useful to look at attention maps for different examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KM7dAFi7S1U"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "def visualize_attention(src_glyphs, tgt_glyphs, attention, axes):\n",
        "    axes.matshow(attention.numpy(), cmap='bone')\n",
        "\n",
        "    axes.set_xticks(numpy.arange(len(src_glyphs)), labels=src_glyphs)\n",
        "    axes.set_yticks(numpy.arange(len(tgt_glyphs)), labels=tgt_glyphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OixmEkuF7S1V"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "pyplot.figure(figsize=(12, 10))\n",
        "\n",
        "src_id_to_token = inverse_vocabulary(src_tokenizer)\n",
        "tgt_id_to_token = inverse_vocabulary(tgt_tokenizer)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, row in train_data.sample(n=4, random_state=42, ignore_index=True).iterrows():\n",
        "        src_tokens = torch.tensor(src_tokenizer.encode(row['Name']))\n",
        "        attentions, tgt_tokens = model.attentions(src_tokens, tgt_tokenizer.get_special_tokens()['[EOS]'], max_length=50)\n",
        "        src_glyphs = apply_inverse_vocab(src_tokens.tolist(), src_id_to_token)\n",
        "        tgt_glyphs = apply_inverse_vocab(tgt_tokens.tolist(), tgt_id_to_token)\n",
        "        axes = pyplot.subplot(2, 2, i+1)\n",
        "        visualize_attention(src_glyphs, tgt_glyphs, attentions, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE1PU0k9s8Ds"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "output_data = []\n",
        "for _, row in validation_data.iterrows():\n",
        "    y_pred = rnn_greedy_generate(\n",
        "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
        "        max_length = rnn_enc_dec_attn_data_params['tgt_padding']\n",
        "    )\n",
        "    output_data.append({ 'Name': row['Name'], 'Translation': y_pred })\n",
        "\n",
        "pd.DataFrame.from_records(output_data).to_csv(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\", \"outputs.csv\"), index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ7Qndm47S1V"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "# Release resources\n",
        "if 'trainer' in globals():\n",
        "    del trainer\n",
        "\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "\n",
        "sync_vram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "998loM887S1W"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tm4zQ637S1W"
      },
      "source": [
        "In the last few modules, you have implemented different approaches towards transliteration of Indian names to English. To assess how well different systems perform, it is useful to compute different metrics, which assess different properties:\n",
        "\n",
        "- **Accuracy**: From a parallel corpus, number of translations the model got exactly right. Higher the better. Note that this makes sense only for this task. and lacks granularity.\n",
        "- **Edit Distance**: Number of edits at the character level (insertions, deletions, substitutions) required to transform your model's outputs to a reference translation. Lower the better.\n",
        "- **Character Error Rate (CER)**: The rate at which your system/model makes mistakes at the character level. Lower the better.\n",
        "- **Token Error Rate (TER)**: The rate at which your system/model makes mistakes at the token level. Lower the better. Depending on your tokenizer implementation, could be the same as CER.\n",
        "- **BiLingual Evaluation Understudy (BLEU)**: Proposed by [Papineni et al., 2002](https://aclanthology.org/P02-1040/), BLEU is a metric that assess the quality of a translation against reference translations through assessing n-gram overlap. Higher the better.\n",
        "\n",
        "Since accents and half-letters exist as separate characters in the Unicode specification, and can change the interpretation of the output, metrics that operate at the character level will treat these separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V2mXdKh7S1W"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\" Class to handle all the logic concerning the evaluation of trained models.  \"\"\"\n",
        "\n",
        "    def __init__(self, src_tokenizer, tgt_tokenizer) -> None:\n",
        "        \"\"\" Initializes the evaluator.\n",
        "\n",
        "        Args:\n",
        "            src_tokenizer (Tokenizer): Tokenizer for input strings in the source language.\n",
        "            tgt_tokenizer (Tokenizer): Tokenizer for output strings in the target language.\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.decoding_method = None\n",
        "\n",
        "    def set_decoding_method(self, decoding_method):\n",
        "        \"\"\" Sets the decoding method to use with models.\n",
        "                The evaluation function will use the set decoding method to generate outputs from the model.\n",
        "\n",
        "        Args:\n",
        "            decoding_method (function): Decoding method.\n",
        "                Must accept the model instance, the input string, and tokenizers as arguments.\n",
        "                Can accept additional arguments if required.\n",
        "        \"\"\"\n",
        "\n",
        "        self.decoding_method = decoding_method\n",
        "\n",
        "    @staticmethod\n",
        "    def decompose(string):\n",
        "        \"\"\" Decomposes a string into a set of tokens.\n",
        "\n",
        "        Args:\n",
        "            string (str): String to decompose.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: List of characters from the string.\n",
        "        \"\"\"\n",
        "        return unicodedata.normalize('NFKD', string).encode('utf-8')\n",
        "\n",
        "    @staticmethod\n",
        "    def levenshtein_distance(string1, string2):\n",
        "        \"\"\" Computes the levensthein distance between two strings.\n",
        "\n",
        "        Args:\n",
        "            string1 (list[any]): Sequence A.\n",
        "            string2 (list[any]): Sequence B.\n",
        "\n",
        "        Returns:\n",
        "            tuple[int, int, int]: Number of insertions + deletions, substitutions and no-ops.\n",
        "        \"\"\"\n",
        "\n",
        "        costs = [\n",
        "            [ 0 for j in range(len(string2)+1) ]\n",
        "            for i in range(len(string1)+1)\n",
        "        ]\n",
        "\n",
        "        # Prepare matrix of costs.\n",
        "        for i in range(len(string1)+1): costs[i][0] = i\n",
        "        for j in range(len(string2)+1): costs[0][j] = j\n",
        "        for i in range(1, len(string1)+1):\n",
        "            for j in range(1, len(string2)+1):\n",
        "                costs[i][j] = min(\n",
        "                    costs[i][j-1] + 1,\n",
        "                    costs[i-1][j] + 1,\n",
        "                    costs[i-1][j-1] + (0 if string1[i-1] == string2[j-1] else 1)\n",
        "                )\n",
        "\n",
        "        # Decode matrix in backward manner for actual operation counts.\n",
        "        c_ins_del, c_sub, c_noop = 0, 0, 0\n",
        "\n",
        "        i, j = len(string1), len(string2)\n",
        "        while i > 0 or j > 0:\n",
        "            if i > 0 and costs[i][j] == costs[i-1][j] + 1:\n",
        "                c_ins_del += 1\n",
        "                i -= 1\n",
        "            elif j > 0 and costs[i][j] == costs[i][j-1] + 1:\n",
        "                c_ins_del += 1\n",
        "                j -= 1\n",
        "            elif i > 0 and j > 0:\n",
        "                if string1[i-1] == string2[j-1]:\n",
        "                    c_noop += 1\n",
        "                else:\n",
        "                    c_sub += 1\n",
        "                i, j = i-1, j-1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return c_ins_del, c_sub, c_noop\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(y_true, y_pred):\n",
        "        \"\"\" Computes the accuracy of the predictions, against a reference set of predictions.\n",
        "\n",
        "        Args:\n",
        "            y_true (list[str]): Actual translations.\n",
        "            y_pred (list[str]): Generated translations.\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy score, between 0 and 1.\n",
        "        \"\"\"\n",
        "        return sum(yi_true == yi_pred for yi_true, yi_pred in zip(y_true, y_pred)) / len(y_pred)\n",
        "\n",
        "    @classmethod\n",
        "    def char_error_rate(cls, y_true, y_pred):\n",
        "        \"\"\" Computes the character level error rate (CER) of the set of\n",
        "            predictions against the reference translations.\n",
        "\n",
        "        Args:\n",
        "            y_true (list[str]): Actual translations.\n",
        "            y_pred (list[str]): Generated translations.\n",
        "\n",
        "        Returns:\n",
        "            float: CER score, between 0 and 1. Lower the better.\n",
        "        \"\"\"\n",
        "\n",
        "        cer_score = 0\n",
        "\n",
        "        for yi_true, yi_pred in zip(y_true, y_pred):\n",
        "            yi_true, yi_pred = cls.decompose(yi_true), cls.decompose(yi_pred)\n",
        "            c_ins_del, c_sub, c_noop = cls.levenshtein_distance(yi_true, yi_pred)\n",
        "            cer_score += (c_ins_del + c_sub) / (c_ins_del + c_sub + c_noop)\n",
        "\n",
        "        return cer_score / len(y_true)\n",
        "\n",
        "    def token_error_rate(self, y_true, y_pred):\n",
        "        \"\"\" Computes the token level error rate (TER) of the set of\n",
        "            predictions against the reference translations.\n",
        "\n",
        "        Args:\n",
        "            y_true (list[str]): Actual translations.\n",
        "            y_pred (list[str]): Generated translations.\n",
        "\n",
        "        Returns:\n",
        "            float: TER score, between 0 and 1. Lower the better.\n",
        "        \"\"\"\n",
        "\n",
        "        ter_score = 0\n",
        "\n",
        "        for yi_true, yi_pred in zip(y_true, y_pred):\n",
        "            yi_true = self.tgt_tokenizer.encode(yi_true, add_start=False, add_end=False)\n",
        "            yi_pred = self.tgt_tokenizer.encode(yi_pred, add_start=False, add_end=False)\n",
        "            t_ins_del, t_sub, t_noop = self.levenshtein_distance(yi_true, yi_pred)\n",
        "            ter_score += (t_ins_del + t_sub) / (t_ins_del + t_sub + t_noop)\n",
        "\n",
        "        return ter_score / len(y_true)\n",
        "\n",
        "    @classmethod\n",
        "    def bleu_score(cls, y_true, y_pred):\n",
        "        \"\"\" Computes the average BLEU score of the set of predictions against the reference translations.\n",
        "\n",
        "            Uses default parameters and equal weights for all n-grams, with max N = 4. (Thus computes BLEU-4).\n",
        "            Uses a smoothing method for the case of missing n-grams.\n",
        "\n",
        "        Args:\n",
        "            y_true (list[str]): Actual translations.\n",
        "            y_pred (list[str]): Generated translations.\n",
        "\n",
        "        Returns:\n",
        "            float: BLEU-4 score, the higher the better.\n",
        "        \"\"\"\n",
        "\n",
        "        y_true = [ [ cls.decompose(yi) ] for yi in y_true ]\n",
        "        y_pred = [ cls.decompose(yi) for yi in y_pred ]\n",
        "\n",
        "        smoothing = bleu_score.SmoothingFunction()\n",
        "\n",
        "        return bleu_score.corpus_bleu(\n",
        "            y_true, y_pred,\n",
        "            smoothing_function=smoothing.method1\n",
        "        )\n",
        "\n",
        "    def evaluate(self, model_path, data, reference_outputs, **decoding_kwargs):\n",
        "        \"\"\" Performs the evaluation of a specified model over given data.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to load the model from. Must have a model.pt file.\n",
        "            data (list[str]): List of input strings to translate.\n",
        "            reference_outputs (list[str]): List of output strings to use as reference.\n",
        "            decoding_kwargs (dict[str, any]): Additional arguments to forward to the decoding method.\n",
        "                This could be for instance, max_length for a greedy decoding method.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the decoding method is not set apriori.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.decoding_method is None:\n",
        "            raise ValueError(f\"{self.evaluate.__name__}: no decoding method is set, assign before use.\")\n",
        "\n",
        "        # Load the model to the active device.\n",
        "        model = torch.load(os.path.join(model_path, 'model.pt'), map_location=self.device, weights_only=False)\n",
        "\n",
        "        # Set model use parameters.\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        # Generate outputs.\n",
        "        generated_outputs = []\n",
        "        with torch.no_grad():\n",
        "            for seq_x in data:\n",
        "                generated_outputs.append(self.decoding_method(\n",
        "                    model, seq_x, self.src_tokenizer,\n",
        "                    self.tgt_tokenizer, **decoding_kwargs\n",
        "                ))\n",
        "\n",
        "        accuracy_score = self.accuracy(reference_outputs, generated_outputs)\n",
        "        cer_score      = self.char_error_rate(reference_outputs, generated_outputs)\n",
        "        ter_score      = self.token_error_rate(reference_outputs, generated_outputs)\n",
        "        blue_score     = self.bleu_score(reference_outputs, generated_outputs)\n",
        "\n",
        "        print(\"EVALUATION:\", \">\", \"accuracy:\", f\"{accuracy_score:.2%}\")\n",
        "        print(\"EVALUATION:\", \">\", \"CER     :\", f\"{cer_score:.2%}\")\n",
        "        print(\"EVALUATION:\", \">\", \"TER     :\", f\"{ter_score:.2%}\")\n",
        "        print(\"EVALUATION:\", \">\", \"BLEU    :\", f\"{blue_score:.4f}\")\n",
        "        print()\n",
        "\n",
        "        # Free resources once evaluation is complete.\n",
        "        del model\n",
        "        sync_vram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3DxXlLm7S1X"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "evaluator = Evaluator(src_tokenizer, tgt_tokenizer)\n",
        "\n",
        "# Use greedy decoding for producing outputs.\n",
        "evaluator.set_decoding_method(rnn_greedy_generate)\n",
        "\n",
        "# Evaluate enc-dec-rnn\n",
        "print(\"EVALUATION:\", \"enc-dec-rnn\")\n",
        "evaluator.evaluate(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\"),\n",
        "    validation_data['Name'], validation_data['Translation'],\n",
        "    max_length = rnn_enc_dec_data_params['tgt_padding']\n",
        ")\n",
        "\n",
        "# Evaluate enc-dec-rnn-attn\n",
        "print(\"EVALUATION:\", \"enc-dec-rnn-attn\")\n",
        "evaluator.evaluate(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\"),\n",
        "    validation_data['Name'], validation_data['Translation'],\n",
        "    max_length = rnn_enc_dec_attn_data_params['tgt_padding']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfI3NL8w7S1Y"
      },
      "source": [
        "## Decoding Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scHoXxoi7S1Y"
      },
      "source": [
        "A conditional language model aims to learn $P_\\theta(y | x)$, that is, the probability of the target sequence being $y$ when the input sequence is $x$. This is modeled as $P_{\\theta}(y | x) = \\prod_{i=1}^{|y|} {P_\\theta(y_i | x, y_{1:i-1})}$.\n",
        "\n",
        "For translation, our goal is to find the sequence that maximizes this conditional probability, i.e. $y^* = \\arg \\max_{y} P_\\theta(y | x)$. $y^*$ is then the 'best' translation for the input sequence $x$. However, computing probabilities for all possible $y$ to find the maximizer is intractable. As a result, decoding strategies are employed to produce reasonable approximations of $y^*$.\n",
        "\n",
        "In the last module, you evaluated your models through different metrics, but the approach for generating outputs from the model was fixed to greedy decoding, where at each time step, the token to be produced is determined by $y_{i,greedy} := \\arg \\max_{y_i} P(y_i| x, y_{1:i-1})$. While this approach is fast, $P(y_{greedy}|x)$ may be much less than $P(y^*|x)$. Fortunately, better decoding strategies exist to produce better approximations, however at the cost of higher time complexity. One such strategy is:\n",
        "\n",
        "- **Beam-Search Decoding**: At every time step, retains $k$ candidate token generations, which are decoded individually (each path is referred as a beam) to obtain $k$ successors per beam. For the next time step, the best $k$ candidates are retained such that conditional probability of the sequence generated so far is maximized. Has a complexity of $O(kV|y|)$, where $V$ is the size of the target vocabulary, and $|y|$ is the target sequence length. Using $k=1$ makes it equivalent to greedy decoding. Implementations also employ length penalties to not be biased towards larger target sequences.\n",
        "\n",
        "In the next cell, you will implement the above explained Beam-Search Decoding decoding strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4sw-Wrb7S1Z"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "# Feel free to add additional parameters to rnn_better_generate, such as k for Beam Search Decoding.\n",
        "def rnn_better_generate(model, seq_x, src_tokenizer, tgt_tokenizer, max_length, k=5, length_penalty_alpha=0.6):\n",
        "    \"\"\" Given a source string, translate it to the target language using the trained model.\n",
        "        This function should use a better decoding strategy than greedy decoding (see above) to generate the results.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): RNN Type Encoder-Decoder Model\n",
        "        seq_x (str): Input string to translate.\n",
        "        src_tokenizer (Tokenizer): Source language tokenizer.\n",
        "        tgt_tokenizer (Tokenizer): Target language tokenizer.\n",
        "        max_length (int): Maximum length of the target sequence to decode.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated string for the given input in the target language.\n",
        "    \"\"\"\n",
        "\n",
        "    # BEGIN CODE : enc-dec-rnn.better_generate\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhMFQmhY7S1Z"
      },
      "outputs": [],
      "source": [
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "# BEGIN CODE : decoding.init\n",
        "\n",
        "# Add parameter values for your decoding strategy here. Leave empty if unused.\n",
        "\n",
        "decoding_params = dict(\n",
        "    # ADD YOUR CODE HERE\n",
        ")\n",
        "\n",
        "## ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOwxnBIC7S1a"
      },
      "outputs": [],
      "source": [
        "# Please do not change anything in the following cell.\n",
        "\n",
        "evaluator = Evaluator(src_tokenizer, tgt_tokenizer)\n",
        "evaluator.set_decoding_method(rnn_better_generate)\n",
        "\n",
        "# Evaluate enc-dec-rnn\n",
        "print(\"EVALUATION:\", \"enc-dec-rnn\")\n",
        "evaluator.evaluate(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\"),\n",
        "    validation_data['Name'], validation_data['Translation'],\n",
        "    max_length = rnn_enc_dec_data_params['tgt_padding'],\n",
        "    **decoding_params\n",
        ")\n",
        "\n",
        "# Evaluate enc-dec-rnn-attn\n",
        "print(\"EVALUATION:\", \"enc-dec-rnn-attn\")\n",
        "evaluator.evaluate(\n",
        "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\"),\n",
        "    validation_data['Name'], validation_data['Translation'],\n",
        "    max_length = rnn_enc_dec_attn_data_params['tgt_padding'],\n",
        "    **decoding_params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHm5Di5V7S1a"
      },
      "source": [
        "## Congratulations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGerFZH7S1b"
      },
      "source": [
        "You have reached the end of the last assignment! Hope this was a fun exercise!\n",
        "\n",
        "Once you've run the cells above, you should be ready to submit the assignment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
